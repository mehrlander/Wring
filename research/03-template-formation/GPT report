Converting Repeated Spans into Parameterized Templates: Techniques and Considerations

Introduction

Identifying parameterized templates in a document – i.e. finding repeated text patterns with variable “slots” – can greatly compress the content and reveal structure. The challenge lies in choosing the right granularity for these templates (analogous to setting a diff edit-cost): too fine-grained and we miss larger patterns, too coarse and we merge unrelated parts. This report surveys techniques to detect recurring spans and turn them into templates with parameters, drawing on methods from log parsing, grammar compression, wrapper induction, and diff algorithms. We focus on when to merge co-occurring repeated spans into one template versus keep them separate, and how to balance compression vs. interpretability. We also emphasize approaches amenable to in-browser JavaScript/WASM implementation (favoring linear or near-linear complexity). Key aspects include anchor sequence search, offset analysis, multi-occurrence alignment, gap entropy, slot boundary inference, and merging heuristics.

Anchor-Sequence Search Algorithms

A fundamental step is finding recurring ordered sequences of spans (anchors) that signal a template. Unlike straightforward substring search, we allow gaps (wildcards) between anchors. Naïvely checking every pair/triple of repeated tokens is expensive (O(n²)); instead, we seek scalable strategies closer to O(n). Several approaches can help:
	•	Frequent Subsequence Mining: Algorithms from data mining and bioinformatics can discover patterns with wildcards. For example, the Teiresias algorithm finds motifs that occur at least k times by first scanning for short seed patterns and then joining them into longer patterns in a convolution phase ￼ ￼. Teiresias outputs maximal patterns with up to W–L wildcards (gaps) under frequency constraints. While pattern discovery is NP-hard in general ￼, such algorithms prune the search space by requiring that all sub-fragments of a pattern also occur k times. This avoids brute-force enumeration and yields all high-frequency patterns with allowed gap lengths. A similar prefix-growth strategy appears in sequential pattern mining (e.g. PrefixSpan), which extends frequent prefixes rather than joining all pairs, mitigating the combinatorial blowup. These methods are more efficient than naive Apriori joins and can often approach linear-time in practice for reasonably structured data.
	•	Suffix Structures: If gaps are not too large, suffix-based indexes can help. A suffix tree or suffix automaton finds all repeated contiguous substrings in O(n) time. To handle gaps, one approach is to treat certain tokens as anchors and search for patterns like A * B * C by combining occurrences of substrings. For example, one can collect all indices of a frequent token A, then check if token B appears after A consistently, etc., effectively performing an index intersection. This is analogous to vertical format sequence mining: maintain for each token the list of positions it occurs, then intersect these lists with appropriate offsets to find frequent token sequences with gaps. Using such positional intersections and linear scans (instead of nested loops) can achieve near-linear complexity for pattern joins ￼ ￼. This anchor join heuristic scales better than comparing every occurrence pair.
	•	Heuristic Log Parsing (Streaming): Practical log template miners often operate in near O(n) by making one pass and clustering similar entries. For instance, the Drain algorithm uses a fixed-depth parse tree: it routes each log line by token positions (e.g. first token, second token, etc.) to group messages that share those anchors ￼. This effectively finds common token sequences incrementally. Another approach, Logram, identifies frequent n-grams in the text and treats them as candidate static anchors ￼. Frequent word sequences are more likely to be part of a template’s literal text. However, a threshold is needed – Logram’s accuracy is sensitive to the chosen frequency cutoff for n-grams ￼. In a single document scenario, we could similarly count n-gram frequencies (for small n) in one pass; highly frequent phrases become anchor candidates. These can then be extended into longer patterns by checking what surrounds them in each occurrence (similar to how Sequitur grammar compression detects repeated phrases and factors them out).
	•	Suffix-to-Prefix Joins: If certain anchor pairs co-occur adjacently, a linear join is straightforward using a suffix automaton or KMP automation. But for non-adjacent anchors, one trick from grammar compression is to insert a marker to “tie” distant spans. For example, a context-free rule can capture a non-contiguous pattern: one answer shows how a grammar rule S -> <script $1 > $2 </script> captures an HTML <script>…</script> block with placeholders for the inner content ￼. It effectively treats <script and </script> as anchor tokens and the inside as gaps. Detecting such a pattern requires finding matching start/end anchors, which can be done by a stack or regex scan in linear time (for well-nested tags), or via a parser. More generally, grammar-based compression can represent distant anchors with a single nonterminal (like S for the whole structure) as long as we can identify the matching pairs in one pass ￼. This suggests using domain knowledge or delimiters (like XML/HTML tags, section headers, etc.) as anchor cues to achieve linear performance.

Scalability: In practice, combining these ideas yields efficient anchor searches. For example, first identify all repeating tokens or phrases (linear scan with hashing). Then attempt to extend those into longer sequences by checking neighboring tokens or by intersecting position lists. This phased approach (find base anchors → build longer chains) mimics the join step of pattern mining but avoids an explicit O(n²) loop by exploiting sorted positions or known delimiters. The result is a set of candidate templates like “A … B … C” that appear multiple times. We will later decide whether to merge them or not based on additional criteria.

Offset Histograms and Span Positioning

Relative positioning of repeated spans can reveal structural patterns. By computing offset histograms – the distribution of distances between spans – we can spot consistent layouts:
	•	Anchor Distance Consistency: If two spans A and B frequently occur with roughly the same separation, it suggests they are part of one template with a fixed-length gap. For example, suppose phrase A occurs at positions [10, 60, 110] and B at [20, 70, 120] in the text. The offsets from A to B (≈10 characters apart each time) form a tight cluster. This indicates a likely template “A ______ B” where the blank has a consistent length. Plotting a histogram of all distances between A and B occurrences would show a peak at ~10 characters. A narrow peak (low variance) means the span between A and B is consistent, reinforcing that they belong to one structural unit. Conversely, if the distances vary widely, A and B might occur independently or in different contexts, so treating them as one template could be incorrect. Thus, offset consistency can be a clue for merging spans.
	•	Periodic or Sectional Repetition: In some documents, repeated structures occur at regular intervals (e.g. every line, every paragraph, or every N tokens). Computing a histogram of absolute positions of a repeated marker might show a periodic spacing. For instance, if a certain header phrase appears every ~500 characters, it might mark the start of a repeating section. Such periodic spacing can inform template boundaries (each section could be one template instance). An offset histogram of successive occurrences of that marker would peak around 500, confirming a periodic structure. This is analogous to detecting page or record boundaries by distance.
	•	Multi-anchor Alignment via Offsets: Consider an anchor sequence A…B…C identified earlier. We can analyze the relative distances: A→B and B→C. If both are consistent, the triple likely forms a rigid template “A _ B _ C”. If one gap is consistent but another varies, it might indicate one slot is of fixed size (like a fixed-length field) and another is truly variable length. These insights help when aligning multiple occurrences (next section) and deciding slot boundaries. For example, if A→B is always 5 tokens but B→C ranges widely, we might infer a fixed literal between A and B, and a big slot between B and C.
	•	Implementation: Computing these histograms is straightforward once occurrences are known: iterate through each pair/triple of anchor occurrences and record differences. The heavy lifting is in finding the anchors (previous section). The offset analysis itself is O(m) for m occurrences, which is usually manageable. Visualization or clustering algorithms can then identify dominant offsets. In a browser environment, one can compute difference arrays and use simple peak-finding logic. If needed, a WebAssembly numeric library could assist in computing distribution statistics quickly, but plain JS should suffice for moderate sizes.
	•	Interpreting Offsets: A high peak (low dispersion) in the offset histogram means low entropy in that gap’s length – essentially a predictable structure. A wide or flat distribution means the gap length is variable (or the pairing of spans isn’t consistent). This ties into gap entropy discussed later. Overall, offset consistency is a useful heuristic: it can signal which repeated spans likely belong to the same template and even hint at whether a gap is a fixed literal or a parameter field. It complements content-based measures by adding a spatial perspective on repetition.

Aligning Multiple Occurrences (Multi-Occurrence Alignment)

Once we identify that certain spans repeat together, we need to align all occurrences to infer the constant vs. variable parts. This is akin to multiple sequence alignment (MSA) in bioinformatics, where we line up sequences to find common subsequences. Here our “sequences” are the multiple occurrences of a suspected template. The goal is to identify which tokens are the same across all occurrences (static text) and which tokens differ (slots).

Challenges: Optimal multi-string alignment is computationally expensive (MSA is NP-hard for many sequences), so we rely on heuristics:
	•	Center-Star Alignment: One quick heuristic is to choose a representative occurrence (e.g. the longest or “median” example) as the base, and align all others to it. This star alignment is known to be within a factor of 2 of optimal sum-of-pairs score in theory ￼. In practice, you align each other sequence to the center using pairwise alignment (in our case, probably a longest common subsequence or edit-distance alignment). The center’s tokens become the template skeleton. This approach is simple (O(k * L^2) for k occurrences of length ~L each, if using DP for each pair) and tends to preserve a readable template similar to one of the real instances.
	•	Progressive Alignment: Another approach is iterative. Start by aligning two occurrences, producing a preliminary template, then align a third occurrence to that template (treating the template as a sequence with wildcard gaps), and so on. This is essentially how many log parsers work. For example, the Spell algorithm takes the first log message as a seed and computes the longest common subsequence (LCS) with a new message to update the template ￼. Each new message that is similar enough (LCS above a threshold) is merged into the template by aligning it, gradually generalizing the pattern. This greedy clustering is efficient and was shown to parse logs accurately in many cases ￼. It’s analogous to how the ROADRUNNER wrapper induction works for HTML: one page is taken as the initial template (wrapper), then it attempts to parse the next page with it; whenever a mismatch occurs, the template is generalized by inserting a wildcard or optional token ￼ ￼. If all pages can be parsed after successive generalizations, a common wrapper (template) is found. This center-sequential approach ensures that each new occurrence can only broaden the template (never specialize it), which tends to keep it general enough to fit all seen instances.
	•	Pairwise Consensus (Median Alignment): If computational resources allow (perhaps via WASM for speed), one could attempt to find a consensus alignment by more advanced means – e.g. treat it as finding a sequence that minimizes total edit distance to all occurrences (a median string). There are approximation algorithms and tools in multiple sequence alignment literature that could be applied, but these are likely overkill for browser-based template inference. Simpler heuristics usually suffice given the repetitive nature of templates.

Alignment Algorithm Details: In aligning text occurrences, dynamic programming (DP) can be used (like Needleman-Wunsch algorithm for sequence alignment). However, a simpler technique in our context is to use placeholders greedily:
	1.	Identify all tokens that appear in every occurrence (the intersection of token sets, possibly in order). These are definitely static anchors.
	2.	Starting from the first anchor token, align the sequences on that token, designate whatever lies between as a gap (slot).
	3.	Move to the next common token, align on it, etc. This is effectively a common-subsequence alignment approach, treating unmatched segments as gaps. It’s linear in the total length once the common tokens are known. Spell’s LCS method is along these lines – it finds the longest common subsequence (tokens) between two logs as the static template ￼, treating differences as parameters.

For multiple occurrences, one can generalize: use a center star by choosing one occurrence and finding its LCS with each of the others. The tokens that consistently appear in all LCS results are likely the static backbone. Alternatively, perform pairwise LCS for all pairs and use a majority vote which tokens are common. These approaches are more heuristic but can work well when the template is consistent.

Example: Suppose we have 3 occurrences of a pattern:

1. Error at file X, line Y: message Z  
2. Error at file A, line B: message C  
3. Error at file M, line N: message O

All share the literal tokens: "Error at file", ",", "line", ":", "message". We align those and mark X/A/M, Y/B/N, Z/C/O as slots. A center-star method picking occurrence 1 as base would align 2 and 3 to it on those common phrases, which is straightforward here. A progressive method might align 1 & 2 first (finding template "Error at file * , line * : message *"), then align occurrence 3 to that template (no new static text appears, so it stays same). Both yield the same result.

Trade-offs:
	•	Center-star is fast but if the chosen center is an outlier (e.g. missing a token present in others), that token might never appear in the template. Progressive alignment can sometimes suffer if an early pair alignment makes a decision that later data would contradict (though algorithms like ROADRUNNER handle this by backtracking and generalizing as needed ￼ ￼).
	•	A more principled method could use all-vs-all alignment with a scoring scheme, but given our goals, a greedy approach guided by common subsequences and anchor tokens is usually sufficient and far easier to implement in JavaScript.

In summary, multi-occurrence alignment transforms raw repeated strings into a structured template with slots. Techniques inspired by sequence alignment (center-star, progressive LCS) are effective. These can be implemented efficiently – e.g. computing LCS of two strings is O(n·m), which for reasonably sized repeated spans (log lines or paragraphs) is fine, and doing it iteratively or star-wise remains manageable for dozens of occurrences. Careful use of these alignments ensures we correctly identify which parts are invariant anchors across all instances and which parts vary.

Gap Entropy and Variability

Not all gaps (differences between anchors) are created equal. Measuring the entropy or variability of gap content helps decide if a segment should be a slot and whether two templates should merge. Gap entropy refers to how unpredictable or diverse the filler content is across all occurrences of a pattern. We use it in several ways:
	•	Slot Identification: If the content between two anchor tokens varies significantly each time, it should be treated as a slot. A high entropy gap – meaning many different values or a large spread of characters – indicates a free variable. For example, if the gap between “user:” and “logged in” can be any username, its entropy is high (nearly every occurrence is different). By contrast, if the gap only ever takes one of two values (e.g. “ON” or “OFF”), its entropy is low. Low entropy gaps might actually represent a small fixed set of literals rather than true free text. In such cases, one might consider leaving those as separate templates (one for “ON” and one for “OFF”) rather than a single template with a slot, especially if those values carry meaning. The decision depends on whether combining them improves compression and if treating that field uniformly is useful. As a general rule, higher variability in a span’s content favors abstracting it as a slot, whereas consistently repeating content favors keeping it literal.
	•	Template Merging: Suppose we have two candidate templates that are very similar except for one segment. If that segment in template A has only a few possible values and in template B it’s a constant, merging them would create a slot whose values are limited (low entropy). This could reduce interpretability (hiding a meaningful literal difference inside a parameter). In log parsing, it’s noted that a frequently occurring “variable” token can degrade accuracy, because the algorithms might mistakenly treat it as static text ￼. Essentially, if a supposed slot value repeats too often, the parser might think it was a literal. Many log parsers mitigate this by preprocessing known frequent tokens (like common IPs or timestamps) so they don’t confuse the model ￼ ￼. In our context, if a slot’s content has low entropy (say it’s always a digit from 1-5), we might increase the entropy by splitting into separate patterns (one per value) or by including part of the value as literal. On the other hand, if merging two patterns yields a slot with high entropy (many different values), that’s a strong indicator it was a correct merge and that slot is a genuine parameter.
	•	Entropy Calculation: We can quantify gap entropy by treating the set of observed gap values as a distribution. For example, if a gap has values {apple, banana, cherry, …} with roughly equal frequency, its Shannon entropy $H=-\sum p_i \log p_i$ will be high. If it has {foo (90%), bar (10%)}, entropy is lower. Another simpler metric is just count of distinct values or max frequency of any single value. A high distinct count or no dominant value points to a true slot. A single dominant value (or very few distinct values) suggests maybe that field is quasi-static or at least might merit special-casing. In practice, we might set a threshold: e.g., if a slot’s most common value appears >50% of the time, maybe it wasn’t a good slot split (the “slot” might actually be a literal that occasionally varies due to noise). This heuristic could trigger splitting the template.
	•	Guiding Template Extraction: We can use entropy during template inference. For instance, when aligning multiple occurrences, for each gap we record all encountered values. If any gap’s entropy is suspiciously low, a refinement step might check if treating that segment as literal yields a better compression (essentially splitting the template). Conversely, if we have two separate templates that only differ by a low-entropy token, we might consider merging them if compression gain is worthwhile and interpretability isn’t hurt.
	•	Compression Gain & Entropy: There’s a relationship between entropy and compression. High entropy content is incompressible except by abstracting it out (as a parameter). Low entropy content (like a small finite set of options) can sometimes be inlined without much cost. For example, if a slot only ever has 2 values, splitting into two templates (each with that position as a literal) might compress just as well, or even better if it avoids needing a slot placeholder. On the other hand, a slot with a hundred unique values clearly must be a parameter for concise representation. Thus, gap entropy informs the compression gain: higher entropy gaps yield more compression benefit when turned into slots.

In summary, measuring gap variability (entropy or distinct count) is a valuable guide. It helps ensure we place slots where the content truly varies, and it helps us decide whether to consolidate templates. Practically, computing this in JavaScript is easy (just use a frequency map for each gap and compute distinct counts or entropy). We should be cautious not to overfit – even a low-entropy slot might need to remain a slot if those few values are conceptually the same “field”. But when optimizing for interpretability, you might split those cases. This is where domain knowledge can supplement raw entropy (e.g., if the slot is “status = OK/FAIL”, it might be clearer as two separate templates since “OK” vs “FAIL” are meaningful labels, despite being a 1-bit entropy difference).

Slot Boundary Inference

Determining where a slot begins and ends is crucial for template clarity. We generally want slots to align with natural token boundaries or field boundaries, rather than slicing through meaningful literals. Key considerations:
	•	Tokenization Strategies: Proper tokenization of the document is a prerequisite. If we only split text on whitespace, we might end up with tokens that contain both static and dynamic parts. For instance, a log message token might be userID=12345. If taken as one token, a parser would likely treat the whole thing as variable (since the number changes), losing the fact that "userID=" is constant. Best practice is to split on punctuation and letter-digit transitions so that "userID=" and "12345" become separate tokens. Many log parsers do exactly this: after splitting on spaces, they further break tokens containing digits or certain symbols ￼. This yields more granular tokens and improves parsing accuracy. In our case, we should ensure the preprocessing separates alphanumeric boundaries and common delimiters (=, :, ,, etc.) to isolate potential literals. As noted in a survey, the way content is split into tokens has a significant impact on parsing accuracy ￼.
	•	Align to Word Boundaries: As a heuristic, template slots should ideally correspond to whole words or fields, not parts of them. Splitting a word reduces interpretability (the template might show a fragment of a word as static text). However, there are exceptions. In some structured text, part of a “word” is actually constant. For example, consider error codes like ERR42 and ERR53 – the prefix ERR is constant, only the number varies. If our tokenizer kept ERR42 as one token, we’d treat it all as a slot, missing the opportunity to factor out ERR. Allowing token-splitting on pattern boundaries can improve compression. One approach is a secondary alignment at the character level within tokens if needed. For instance, if a slot values all share a common prefix or suffix, we could split that out as literal. Grammar compression techniques sometimes do this by introducing rules that capture common prefixes of symbols. But doing this arbitrarily can lead to bizarre templates (too many tiny pieces).
	•	Best Practice: Use domain knowledge to define splitting rules. For text with identifiers, a good rule is: separate alphabetic prefix from numeric suffix. This way, ERR42 becomes ERR + 42. Similarly, foo_bar could be split into foo_ and bar if foo_ is a recurring literal prefix across many tokens. Many programming-language diff tools and log parsers use regexes to identify numbers, timestamps, hex hashes, etc., and mark them as separate tokens or wildcard tokens upfront ￼. This ensures templates align with meaningful boundaries. In an HTML context, tokens might be extracted from the DOM structure (each attribute value as a token, etc.), which inherently gives boundaries.
	•	No Splitting vs Over-Splitting: There is a balance. If we never split tokens, we may miss smaller literal patterns. If we over-split (e.g. every single character as a token), we’ll find trivial “repeated” characters as anchors but lose all meaning (and possibly make the problem intractable). A balanced approach is to start with a sensible tokenization (whitespace + punctuation) and then apply targeted splits for known patterns (like digits, underscores). For example, one could use a regex to split tokens like ([A-Za-z]+)(\d+) into two. This way, slot boundaries align with the end of a letter sequence or start of a number, which often corresponds to real field boundaries.
	•	Slot Boundary vs Compression: Sometimes, allowing a slot to eat a punctuation can save a few bytes but at cost of readability. For instance, consider templates around punctuation: original logs Foo:123 and Foo:456. If tokenized as ["Foo:123"] vs ["Foo:456"], we might align them as Foo: literal and the number as slot but including the colon (because of tokenization). If we instead had tokens ["Foo", ":", "123"], we’d clearly keep Foo and : as literals and 123 as slot. The latter is preferable – each slot stands alone as the variable part. So, including delimiters with the static tokens is usually better. A simple rule: treat punctuation as separate tokens or attach them to the preceding literal token, not to the variable token.

In implementation, after initial pattern mining, one can refine slot boundaries by examining the content of each slot across occurrences:
	•	If all slot values share a common prefix/suffix string, that prefix/suffix could actually be part of the literal template. We can trim it off the slot and append it to the literal anchors. This improves compression and interpretability.
	•	Ensure that after this adjustment, slot values are what vary completely. For example, if slot values all begin with "ID", and only the number after differs, consider pulling "ID" into the template literal.

Overall, slot boundary inference is about maximizing the template’s static content while keeping slots meaningful. It often comes down to good tokenization and minor post-processing. In a browser setting, this might involve applying regex replacements to the input text to insert spaces around delimiters or camelCase boundaries, thereby guiding the tokenizer. The result is templates that are both compact and human-understandable, with slots neatly covering only the truly variable parts.

Template Merging vs. Separation Heuristics

A core decision is when to merge two repeated spans into one template versus keep them as separate templates. Merging increases compression (one template covers more instances) but can reduce clarity (too many slots or overly general patterns). We use several heuristics and thresholds to guide this decision:
	•	Minimum Literal Length: Require a template to have a certain amount of static text (literal characters) after merging. If merging two patterns would result in a template where the static parts are extremely short (e.g. just a couple of characters), it might be too generic. For example, merging patterns that share only a common punctuation and nothing else would yield a template like “*” (everything is a slot except maybe a comma) which is not useful. Ensuring the template’s literal backbone has a reasonable length (or number of tokens) prevents over-generalization. In practice, one might set a threshold like “at least 3 static tokens or 8 static characters” for a template. This is akin to ensuring a certain compression unit; it’s also motivated by results in grammar induction that avoid creating rules for very short common substrings unless they have high payoff ￼.
	•	Maximum Slots (Slot Count Limit): Imposing an upper bound on how many slots a single template can have helps maintain interpretability. If two templates differ in many places, merging them would produce a template with numerous placeholders. At some point, it’s clearer to treat them as separate templates. For example, merging A X B Y C and A U B V C yields A {slot1} B {slot2} C (2 slots) which might be fine. But merging more divergent ones like A X B C and D Y B Z (sharing only B) would yield something like {slot0} B {slot1} plus perhaps differing prefixes, which might not make sense as a single concept. A typical rule of thumb could be to allow at most, say, 4 slots in one template. If a merge would create more than that, the patterns likely represent different structures and should remain separate. Each slot is a cognitive load for whoever reads the template, so limiting slots keeps the template structure evident.
	•	Frequency and Compression Gain: Perhaps the most important criterion is compression gain. We can formally define the gain of a template as how many characters or tokens it saves relative to raw text. For instance, if a substring of length L appears k times, the raw total length is k*L. Representing it as a template of length L (literal part) + slots might cost roughly L + sum_of(slot_lengths) + overhead. The gain is (k-1)*L minus overhead (since one occurrence’s literal is kept and others are largely replaced by shorter references). We can compute a simple proxy: gain = (k - 1) * (L - avg_slot_length). More directly, think in terms of description length: length(template literals) + length(all slot values) vs length(original occurrences). A merge is justified if it significantly reduces this total. A threshold can be set that a merge must achieve a minimum gain (or minimum k). For example, a pattern should appear at least k_min times or reduce total length by at least X% to warrant templating. If two occurrences of a phrase are the only repetition, merging them yields limited benefit – some systems require >=3 occurrences to form a template, as two might be coincidence. Others allow two if the substring is very long (since even 2 occurrences of a long paragraph is worth it). Thus the threshold might be dynamic: e.g., require at least 50 characters saved or at least 3 occurrences.
	•	Semantic Coherence: Automated metrics aside, consider if merging spans would group semantically different content. We want templates to represent one “type” of structure. If two repeated segments occur in different contexts or have different meanings, forcing them into one template could confuse analysis. This is harder to quantify, but one proxy is anchor correlation: if two spans always appear together (high mutual information), they likely form one context. If not, perhaps they belong separate. In wrapper induction, the idea of union-free regular expressions was to avoid merging patterns that don’t truly share a single structure. If needed, it’s better to output two simpler templates than one complex one that tries to be all-encompassing.
	•	Analogy to Diff Edit Cost: This merge decision parallels the diff algorithm’s granularity setting. A high edit-cost setting makes diff output treat small changes as part of a larger change (merging them), whereas a low edit-cost splits differences into tiny pieces ￼. Likewise, we can tune our “merge aggressiveness.” A higher merge threshold (demanding big savings) leads to more but narrower templates (like a fine-grained diff with many small changes isolated). A lower threshold merges more aggressively, yielding fewer, more generalized templates (like a diff that lumps small changes together into one big block). Finding a balance is key. Often an MDL (Minimum Description Length) principle can be applied: choose the set of templates that minimizes the overall length (templates + encoded data) ￼. That naturally optimizes compression while penalizing overly complex models (since each additional template or slot has a cost). In practice, one might iterate: try merging two templates and compute the new total description length; if it improves beyond a threshold, accept the merge; if not, keep them separate.

To summarize these heuristics, consider the following guiding rules:

<table>
<thead>
<tr><th>Heuristic</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td>**Min Literal Size**</td><td>Ensure merged template has a substantial static core (e.g. ≥ 3 tokens) to avoid overly generic patterns [oai_citation:27‡cs.stackexchange.com](https://cs.stackexchange.com/questions/23534/compression-of-non-adjacent-structure-using-grammar#:~:text=SXBYBZA).</td></tr>
<tr><td>**Max Slot Count**</td><td>Limit slots per template (e.g. ≤ 4) for readability; if merging introduces too many slots, don’t merge.</td></tr>
<tr><td>**Min Occurrences**</td><td>Require a pattern to appear multiple times (often ≥ 3) before forming a template, unless extremely long. This filters out one-off or coincidental repeats.</td></tr>
<tr><td>**Compression Gain**</td><td>Quantify bytes/tokens saved by merging. Merge only if gain exceeds a threshold (e.g. saves ≥ X characters or ≥ Y% of original size).</td></tr>
<tr><td>**Slot Entropy**</td><td>Examine variability of each prospective slot. Very low entropy slots (few distinct values) suggest caution – the “slot” might actually be a small set of literals (consider separate templates), whereas high entropy slots confirm a good generalization.</td></tr>
</tbody>
</table>


Applying these heuristics will yield a set of templates that strike a balance between compactness and clarity. In one case study, a grammar-based compressor only introduced a non-contiguous rule when the pattern was “frequent enough to justify the cost of creating the rule” ￼ – this reflects the compression gain threshold. Similarly, log template algorithms often have tunable parameters like the n-gram frequency or LCS similarity threshold that control how aggressively they merge log lines ￼. By adjusting these dials (and analogously the above heuristics), one can fine-tune the granularity of extracted templates.

Conclusion

Converting repeated spans in a document into parameterized templates involves a pipeline of detection and decision steps. We first find candidate patterns via anchor sequence search, using efficient algorithms to spot recurring token chains with gaps. We then analyze offsets and alignment across occurrences to solidify the pattern structure, determining which parts are fixed vs. variable. Measures like gap entropy guide us in labeling slots and deciding if patterns are similar enough to merge. We enforce sensible slot boundaries (aligned to tokens and meaningful units) to maximize interpretability. Finally, we apply heuristics for merging vs. separating templates to ensure each template provides a good compression benefit without being over-general.

Throughout, we leverage insights from related domains:
	•	From log parsing, we borrowed ideas like LCS-based merging (Spell) ￼, fixed-depth partitioning (Drain) ￼, and token preprocessing for variables ￼.
	•	From grammar compression, we learned how non-adjacent structures can be captured with rules ￼ and the importance of using patterns only when frequency justifies it ￼.
	•	Wrapper induction (RoadRunner) demonstrated progressive alignment of multiple examples by generalizing on mismatches ￼ ￼, a strategy we emulate for multi-occurrence alignment.
	•	Diff algorithms provided an analogy for controlling granularity via an “edit cost” parameter ￼, inspiring our approach to merging thresholds.

In an implementation context (browser-based JS/WASM), the recommended approaches emphasize linear or near-linear passes over text and moderate data structures (maps of positions, frequency counts) rather than heavy global optimizations. Many steps (tokenizing, scanning for repeats, building frequency maps, computing LCS for short segments) are efficient in JavaScript, and more intensive tasks (like suffix automata or complex alignments) could be offloaded to WebAssembly if needed. By judiciously combining these techniques, one can automatically infer templates that compress the document effectively while remaining interpretable. The end result is a set of parameterized templates where co-occurring repeated spans are merged when appropriate – yielding maximal compression – and left separate when their differences are too great or not beneficial, thus maintaining meaningful granularity.

Sources:
	•	He et al., Drain: An Online Log Parsing Approach with Fixed Depth Tree – describes efficient log template mining and the importance of token partitioning ￼.
	•	Zhu et al., Logram: Log Parsing using Frequent n-grams – uses frequent n-gram anchors and highlights threshold sensitivity ￼.
	•	Spell (by Lou et al.) – log parsing via longest common subsequence, requiring majority of message to be variable ￼.
	•	Babou’s answer on StackExchange – explains grammar-based compression of nonadjacent patterns and when a rule is worthwhile ￼ ￼.
	•	Fraser’s Diff Match Patch demo – explains edit cost in diff merging (granularity control) ￼.
	•	Crescenzi et al., ROADRUNNER – outlines wrapper induction by aligning multiple HTML pages and generalizing on mismatches ￼ ￼.
	•	Log parsing background (FSE’21 study) – emphasizes tokenization and notes frequent dynamic values can mislead parsing ￼ ￼.
	•	Teiresias algorithm (Rigoutsos & Floratos) – illustrates pattern mining with gap constraints and two-phase (scan/convolution) approach ￼ ￼.

These sources collectively inform the strategies discussed, bridging theoretical foundations with practical heuristics for template inference.