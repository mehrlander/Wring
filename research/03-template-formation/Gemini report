Advanced Algorithmic Paradigms for Log Template Formation: A Synthesis of Information Theory and Combinatorial Optimization
1. Introduction: The Epistemology of Log Parsing
In the contemporary landscape of distributed computing, the log file remains the immutable system of record. It is the fundamental artifact of execution, capturing the discrete states, transitions, and errors of software systems that have grown too complex for manual comprehension. As architectures evolve from monolithic structures to ephemeral, microservices-based ecosystems, the volume of log data has undergone a hyper-inflationary expansion. This phenomenon necessitates the transition from human-centric debugging to AIOps (Artificial Intelligence for IT Operations), where automated systems ingest, parse, and analyze machine-generated text to detect anomalies, forecast failures, and conduct root cause analysis.
The critical bottleneck in this pipeline is Log Parsing: the transformation of semi-structured, raw text messages into structured events. A raw log message is a hybrid entity, a composite of static code (the "template") and dynamic runtime state (the "variables"). The parsing objective is to disentangle these two components, mapping the infinite space of raw strings to a finite, manageable set of event templates.
This report addresses "Research Question 3: Template Formation" within the Wring project architecture. It explores the algorithmic transition from identifying "Repeated Spans" (frequent substrings) to constructing robust, parameterized templates. The challenge is not merely identifying patterns, but determining the optimal level of granularity. The central research question—When should two co-occurring repeated spans become one template versus remain separate?—strikes at the heart of the "granularity dilemma" in pattern mining. If a parser is too specific, it fails to generalize, resulting in "pattern explosion" where variables (like IDs) are mistaken for template literals. If it is too general, it conflates distinct error modes into generic "noise," destroying the diagnostic value of the data.
We posit that Template Formation is fundamentally a compression problem governed by the principles of Minimum Description Length (MDL) and Information Theory. By viewing the log stream as a transmission channel, we can leverage the statistical properties of "Anchors" (repeated spans) and "Gaps" (variable regions) to mathematically derive the optimal template structure. This report synthesizes techniques from diverse fields—including bioinformatics (Multiple Sequence Alignment, Spaced Motifs), streaming data mining (SWIFT, Sliding Windows), and computational linguistics (Entropy, Subword Tokenization)—to define a rigorous framework for template formation.
1.1 The State of the Art and Its Limitations
Current log parsers typically fall into two categories: syntax-based and semantic-based. Syntax-based parsers, such as Drain and Spell, rely on heuristics like fixed-depth parse trees and longest common subsequence (LCS) algorithms. Drain, for instance, uses a parse tree to guide the search for log groups, employing similarity thresholds to decide whether an incoming log merges with an existing group or forms a new one. While efficient, these methods often require manual tuning of parameters like sim_th (similarity threshold) and struggle with "concept drift" in evolving software.
Semantic-based parsers, including recent LLM-driven approaches like Lemur and LogParser-LLM, attempt to understand the "meaning" of the log to distinguish variables. Lemur, for example, utilizes entropy sampling to cluster logs and Chain-of-Thought (CoT) prompting to merge templates based on semantic equivalence. However, these methods can be computationally prohibitive for high-velocity streams and may hallucinate templates if not grounded in strict statistical evidence.
The Wring project necessitates a hybrid approach: one that possesses the computational efficiency of streaming algorithms (like SWIFT ) but incorporates the nuanced variable detection of information-theoretic models. The "granularity knob" sought by the core question is analogous to the Diff_EditCost in sequence comparison , a parameter that governs the willingness of the algorithm to "stitch" disparate segments together. This report defines that knob not as a heuristic constant, but as a dynamic function of Gap Entropy and Offset Stability.
2. Theoretical Framework: The Anatomy of a Log Template
To construct a robust algorithm, we must first formalize the generative model of a log message and the components of the parsing process.
2.1 The Generative Model
A log message L is generated by a logging statement in the source code, which can be modeled as a function T taking a parameter vector \theta:
Here, \Sigma = \{ \sigma_1, \dots, \sigma_k \} represents the set of static template literals (constants), and \Theta = \{ \theta_1, \dots, \theta_{k-1} \} represents the dynamic variables. The operator \cdot denotes concatenation.
The parsing task is the inverse problem: Given a set of observations \{L_1, \dots, L_N\}, recover the set of template functions \mathcal{T} and the corresponding parameter vectors \Theta_i for each log. The difficulty arises because the boundary between \Sigma and \Theta is ambiguous in the raw text. A frequent error code (e.g., "500") may appear static, while a rare error message may appear dynamic.
2.2 Anchors, Gaps, and Skeletons
In the proposed Wring architecture, the parsing process begins with Anchors—repeated spans of text that have been identified as statistically significant across the corpus.
	•	Anchor (A): A substring with high frequency and low internal entropy.
	•	Gap (G): The interstitial region between two anchors A_i and A_{i+1}.
	•	Skeleton: An ordered sequence of anchors and gaps: A_1 - G_1 - A_2 - G_2 - \dots.
The "Core Question" of this research is determining the nature of the Gap G.
	•	If G is a Literal (missed during anchor detection due to fragmentation), then A_i, G, A_{i+1} should be merged into a single template segment.
	•	If G is a Variable, then A_i and A_{i+1} remain separate, delimiting a parameter slot.
2.3 The Granularity Spectrum
The decision to merge or separate defines the granularity of the resulting schema. This spectrum is critical for downstream utility.
	•	High Specificity (Fine-Grained): The parser separates spans aggressively.
	•	Result: Templates like Failed to connect to node-1, Failed to connect to node-2.
	•	Metric: High pattern count (|P|), low compression ratio.
	•	Risk: Overfitting to variables.
	•	High Applicability (Coarse-Grained): The parser merges aggressive.
	•	Result: Template Failed <*> covering connection errors, disk errors, and auth errors.
	•	Metric: Low pattern count, high distortion (loss of information).
	•	Risk: Underfitting, loss of diagnostic precision.
The optimal point on this spectrum is defined by the Minimum Description Length (MDL) principle. MDL suggests that the best hypothesis (template set) is the one that minimizes the sum of the length of the hypothesis and the length of the data when encoded by that hypothesis.
This principle inherently penalizes both pattern explosion (high L(Model)) and over-generalization (high L(Data|Model) due to poor fit).
3. Anchor-Sequence Search: Constructing the Skeleton
The first operational phase of the pipeline is Anchor-Sequence Search. Given a set of disjoint repeated spans (Anchors), we must discover the ordered chains that recur across the log stream. This process transforms the unstructured text problem into a sequential pattern mining problem.
3.1 Event Stream Emission
We treat the raw log file as a sequence of events. Each log line is scanned to locate instances of known Anchors. We emit a stream of tuples:
This abstraction reduces the high-dimensional raw text into a lower-dimensional alphabet of Anchor IDs. The computational challenge is to find subsequences (A, B, C) that appear frequently together, despite varying distances (gaps) between them.
3.2 Sliding Window Joins
To identify these chains efficiently, we employ a Sliding Window approach, a technique validated in streaming pattern mining literature.
	•	Mechanism: A window of size W (defined by token count or byte length) moves across the anchor stream of a log line.
	•	Candidate Generation: Within the window, we generate all pairs (A, B) such that Position(A) < Position(B).
	•	Support Counting: We maintain a frequency count for each pair A \to B.
This method is superior to naive full-sequence scanning because it operates in O(N \cdot W) time, where N is the log volume and W is the window size (typically small and constant). This satisfies the requirement for an O(\#candidates) strategy rather than quadratic O(N^2). For high-velocity streams, probabilistic data structures like Count-Min Sketch or reservoir sampling can be used to track pair frequencies within fixed memory bounds.
3.3 Graph-Based Skeleton Assembly
The frequent pairs (A, B) form the edges of a Directed Acyclic Graph (DAG) where nodes are Anchors.
	•	Edge Weight: The conditional probability P(B|A) or raw co-occurrence count.
	•	Path Finding: A template skeleton corresponds to a path A \to B \to C in this graph.
	•	Transitive Reduction: If we observe edges A \to B, B \to C, and A \to C (a transitive edge), we must determine if A \to C is a distinct relationship or merely a byproduct of the chain A \to B \to C.
	•	If the occurrence of A \to C almost always includes B, the edge A \to C is redundant and pruned.
	•	This logic effectively reconstructs the linear sequence of the template: [Anchor A] -- gap -- -- gap -- [Anchor C].
The graph-based approach aligns with methods used in genome assembly (Overlap-Layout-Consensus) and process mining , allowing the reconstruction of long-range structures from local adjacencies.
4. Offset Histograms: The Geometry of Variable Gaps
Once candidate pairs (A, B) are identified, the nature of the relationship between them must be characterized. Is the "gap" a rigid delimiter (suggesting a merge) or a flexible variable (suggesting a slot)? Offset Histograms provide the statistical evidence for this classification.
4.1 Computing Offset Distributions
For every occurrence of the pair (A, B) in the log stream, we compute the positional difference:
This \delta represents the number of characters (or tokens) separating the two anchors. We aggregate these values into a frequency histogram H_{AB}(\delta).
4.2 Interpreting Histogram Topology
The shape of the offset histogram serves as a "fingerprint" for the type of variable residing in the gap. Insights from "spaced motif" discovery in bioinformatics and seismic offset analysis suggest the following interpretations:
Histogram Shape
Interpretation
Example Content
Template Decision
Dirac Delta (Single Peak)
Rigid Coupling. The gap is invariant.
A single space, a colon :, or fixed delimiter.
Merge (A and B are one literal).
Multimodal (Few Peaks)
Discrete Variation. The variable has fixed states.
Enumerations like ON/OFF, HTTP verbs GET/POST.
Slot (Categorical/Enum).
Gaussian / Broad
Elastic Coupling. The variable varies in length.
Usernames, File Paths, URLs, Error Messages.
Slot (String/Wildcard).
Uniform / Random
No Coupling. Co-occurrence is coincidental.
Interleaved logs or independent clauses.
Separate (Do not link).
4.3 Statistical Gating and Efficiency
Calculating offsets for all pairs is computationally expensive (O(N^2) in the worst case of dense anchors). To avoid this "quadratic blowup," we implement gating mechanisms :
	1	Frequency Gate: Only compute histograms for pairs with support exceeding a threshold \sigma.
	2	Distance Gate: Only consider pairs within a maximum distance D_{max}.
	3	Difference Histograms: Use efficient voting algorithms (like those used in UAV visual odometry for position deltas ) to quickly identify the mode of the distribution without storing every instance.
The Offset Histogram is a powerful "granularity knob." A tight variance threshold enforces high specificity (splitting templates if variables change length), while a loose threshold allows for broader generalization.
5. Multi-Occurrence Alignment: The Center-Star Approach
Having established the skeletons (A \dots B \dots C), we must now align all raw instances of logs fitting this skeleton to precisely define the boundaries of literals and slots. This is a Multiple Sequence Alignment (MSA) problem, a staple of bioinformatics that is directly transferable to log analysis.
5.1 The Alignment Problem in Logs
Given a cluster of k log messages believed to follow the same skeleton, we aim to insert gaps (padding) such that equivalent tokens align in columns.
	•	Constant Columns: Contain identical tokens across all rows \rightarrow Template Literals.
	•	Variable Columns: Contain diverse tokens across rows \rightarrow Parameter Slots.
Standard dynamic programming for MSA is O(L^k), which is exponential and infeasible for log clusters where k can be thousands.
5.2 The Center-Star Algorithm
The Center-Star algorithm is a polynomial-time approximation algorithm that guarantees a sum-of-pairs score within a factor of 2 of the optimal alignment. It is the preferred baseline for this research due to its balance of speed and accuracy.
Algorithmic Steps:
	1	Center Selection: Identify the "Center Sequence" S_c that minimizes the sum of pairwise distances to all other sequences in the cluster. In the domain of logs, the "median length" string is often a sufficient heuristic for S_c, reducing this step from O(k^2 L^2) to O(k L).
	2	Star Alignment: Iteratively align every other sequence S_i to the center S_c.
	•	This requires only k pairwise alignments, rather than k(k-1)/2.
	•	As S_i is aligned to S_c, gaps inserted into S_c are propagated to all previously aligned sequences to maintain consistency.
	3	Consensus Generation: The final multiple alignment allows for column-wise analysis.
5.3 Comparison with Progressive Alignment
While tools like ClustalW use Progressive Alignment (aligning the most similar pairs first along a guide tree) , Center-Star is often superior for logs because log clusters typically consist of a clear "prototype" (the template) with minor deviations. The "Center" log acts as this prototype. Progressive alignment runs the risk of "once a gap, always a gap" errors early in the tree, whereas Center-Star anchors everything to the most representative instance.
For massive datasets, linear-time variants of Center-Star using suffix trees or bitmap strategies can further accelerate the process , ensuring the parsing pipeline scales linearly with log volume.
6. Information-Theoretic Refinement: Entropy and MDL
Alignment provides the physical correspondence between tokens; Information Theory provides the semantic decision logic. To answer the core question—When to merge?—we quantify the information content of the aligned columns and the gaps.
6.1 Entropy as a Stitching Heuristic
Shannon Entropy (H) measures the uncertainty of a random variable.
We apply this metric to the "Gap" regions identified in the skeleton.
	•	Low-Entropy Gap (H \approx 0): The gap content is highly predictable.
	•	Example: Between anchors User and logged, the gap is always successfully.
	•	Decision: Merge. The gap is actually a literal. Template: User successfully logged.
	•	High-Entropy Gap (H \gg 0): The gap content is unpredictable (high surprise).
	•	Example: Between anchors User and logged, the gap contains admin, guest, root, service_account.
	•	Decision: Separate. The gap is a variable slot. Template: User <UserType> logged.
This aligns with the Lemur framework, which uses entropy sampling to distinguish high-information log clusters from repetitive noise. We can define an "Entropy Threshold" \epsilon. If H(Gap) < \epsilon, the anchors are stitched.
6.2 Minimum Description Length (MDL) for Slot Boundaries
The MDL Principle serves as the global objective function for the parser. It formalizes the trade-off between model complexity and data fit.
Application: Splitting Tokens The prompt asks about splitting tokens (e.g., key=value) to improve MDL. This is analogous to Subword Tokenization (BPE, WordPiece) in LLMs , where the vocabulary is optimized to compress the text.
Consider the token cpu_load=99%.
	•	Scenario A (No Split): Treat cpu_load=99% as a single variable token.
	•	Model Cost: Low (one slot <*>).
	•	Data Cost: High. We must encode the literal prefix cpu_load= for every log line in the variable dictionary.
	•	Scenario B (Split): Split into cpu_load= (literal) and 99% (variable).
	•	Model Cost: Higher (template grows by one token cpu_load= <int>%).
	•	Data Cost: Drastically Lower. We only encode the values 99, 10, 5. The repetitive cpu_load= is stored once in the template.
Decision Logic: Calculate \Delta MDL = Cost(A) - Cost(B). If \Delta MDL > 0 (meaning Split is cheaper), we refine the boundary by splitting the token. This rigorous approach prevents arbitrary tokenization rules and adapts to the specific redundancy patterns of the log stream.
6.3 SWIFT: Streaming MDL
The SWIFT algorithm applies MDL to streaming event patterns, introducing the concept of MDL-based Representative Patterns (MRP). SWIFT demonstrates that calculating MDL updates can be done incrementally. In Wring, we can maintain running MDL scores for candidate templates. When a new log arrives, we check if merging two existing templates or splitting a token reduces the global description length of the window. This allows the parser to dynamically adjust granularity as data characteristics change.
7. Granularity Control: The "Merge" Function
We synthesize the findings into a concrete decision function for merging spans, answering the Core Question. This function acts as the "granularity knob."
Let A and B be two sequential anchors. We define the Merge Cost C_{merge}(A, B):
	•	\alpha, \beta, \gamma: Tunable weights (the knob).
	•	\text{Var}(H_{AB}): Variance of the offset histogram. Low variance \rightarrow Low cost (favors merge).
	•	\text{H}(\text{Gap}_{AB}): Entropy of the text between anchors. Low entropy \rightarrow Low cost (favors merge).
	•	\text{Jaccard}(A, B): Measure of how often A and B appear together versus apart. High Jaccard \rightarrow Low cost (favors merge).
Decision Rule: If C_{merge}(A, B) < \tau (Threshold), Merge A and B into a single template unit. Otherwise, treat the gap as a parameter slot.
This function provides a continuum.
	•	High \tau (Coarse): Merges even if offsets vary slightly or entropy is moderate. Result: Generic templates.
	•	Low \tau (Fine): Merges only if anchors are rigidly locked. Result: Specific templates.
8. Synthesis & Implementation Architecture
To operationalize these insights for the Wring project, we propose the following Wring-MDL Pipeline. This architecture ensures scalability (linear time) while maximizing template quality.
8.1 The Wring-MDL Pipeline
Stage
Component
Algorithm / Technique
Output
1
Ingest & Tokenize
Stream Processing
Stream of (Pos, TokenID)
2
Anchor Detection
Count-Min Sketch / Lossy Counting
Set of Frequent Anchors
3
Skeleton Construction
Sliding Window Join (O(N))
Stream of Candidate Pairs (A, B)
4
Gap Profiling
Offset Histograms & Entropy Accumulators
Profiles: H_{AB}(\delta), H(Gap)
5
Granularity Decision
Merge Function (Cost < \tau)
Rough Templates (Merged Anchors)
6
Fine Alignment
Center-Star MSA (on clusters)
Aligned Token Matrix
7
Boundary Refinement
MDL Optimization (Token Splitting)
Final Parameterized Templates
8.2 Complexity Analysis
	•	Time Complexity: The pipeline is dominated by the sliding window search and histogram updates, which are O(N) (linear with log volume). The expensive Center-Star alignment is applied only to clusters of logs (identified by skeletons), not the entire corpus pairwise. If the number of templates is small relative to N (a valid assumption for logs), the total complexity remains effectively linear.
	•	Space Complexity: Memory usage is bounded by the number of active candidate pairs. Using sketches and LRU eviction policies (similar to Drain's tree cache) ensures the system fits within fixed memory constraints.
9. Conclusion
The conversion of repeated spans into parameterized templates is a structural inference problem that requires balancing rigidity with elasticity. By moving beyond static heuristics and adopting a "physics of data" approach—where Offset Histograms measure the spatial forces between anchors and Entropy measures the informational density of gaps—we can construct a parser that is both robust and adaptive.
The "granularity knob" is not a single magic number but a composite threshold derived from the Minimum Description Length principle. This principle dictates that two spans should merge when their union explains the data more concisely than their separation. This report provides the Wring project with a theoretically sound and algorithmically viable path to implement this logic, leveraging the speed of anchor-based search and the precision of multi-sequence alignment to achieve state-of-the-art log parsing performance. The integration of "Split Token" refinement via MDL represents a novel advancement, aligning log analysis with modern subword tokenization trends in Large Language Models.
Works cited
1. (PDF) Using Natural Language Processing for Log Analysis and Automated Alert Prioritization - ResearchGate, https://www.researchgate.net/publication/397738546_Using_Natural_Language_Processing_for_Log_Analysis_and_Automated_Alert_Prioritization 2. Techniques to Improve the Parsing of Unstructured Logs for AIOps, https://spectrum.library.concordia.ca/id/eprint/995238/1/Sedki_PhD_S2025.pdf 3. LogParser-LLM: Advancing Efficient Log Parsing with Large Language Models - arXiv, https://arxiv.org/html/2408.13727v1 4. logpai/Drain3: A robust streaming log template miner based ... - GitHub, https://github.com/logpai/Drain3 5. Drain: An Online Log Parsing Approach with Fixed Depth Tree - Tsinghua NetMan Lab, https://netman.aiops.org/~peidan/ANM2023/6.LogAnomalyDetection/phe_icws2017_drain.pdf 6. LogParser-LLM: Advancing Efficient Log Parsing with Large Language Models - Liner, https://liner.com/review/logparserllm-advancing-efficient-log-parsing-with-large-language-models 7. lemur: log parsing with entropy sampling - arXiv, https://arxiv.org/pdf/2402.18205 8. \emojititleLemur: Log Parsing with Entropy Sampling and Chain-of-Thought Merging - arXiv, https://arxiv.org/html/2402.18205v5 9. SWIFT: Mining Representative Patterns from Large Event Streams, http://www.vldb.org/pvldb/vol12/p265-yan.pdf 10. name/fraser/neil/plaintext/diff_match_patch.java - platform/external/google-diff-match-patch, https://android.googlesource.com/platform/external/google-diff-match-patch/+/ed79165d195c99e5d8e283bb5bbf84c3363ae254/name/fraser/neil/plaintext/diff_match_patch.java 11. diff-match-patch CDN by jsDelivr - A CDN for npm and GitHub, https://www.jsdelivr.com/package/npm/diff-match-patch 12. (Exhaustive) Symbolic Regression and model selection by minimum description length, https://arxiv.org/html/2507.13033v1 13. A Tutorial Introduction to the Minimum Description Length Principle - CWI, https://homepages.cwi.nl/~pdg/ftp/mdlintro.pdf 14. Mining Stream, Time-Series, and Sequence Data - Khoury College of Computer Sciences, https://www.khoury.northeastern.edu/home/vip/teach/DMcourse/6_graph_analysis/notes_slides/chapter_8.pdf 15. Efficient Episode Mining of Dynamic Event Streams - Microsoft, https://www.microsoft.com/en-us/research/wp-content/uploads/2012/12/icdm12-episodes.pdf 16. Cactus: Algorithms for genome multiple sequence alignment - PMC - PubMed Central, https://pmc.ncbi.nlm.nih.gov/articles/PMC3166836/ 17. Unsupervised statistical discovery of spaced motifs in prokaryotic genomes - PMC - NIH, https://pmc.ncbi.nlm.nih.gov/articles/PMC5217627/ 18. High Resolution Genome Wide Binding Event Finding and Motif Discovery Reveals Transcription Factor Spatial Binding Constraints | PLOS Computational Biology - Research journals, https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1002638 19. Characterizing Listener Engagement with Popular Songs Using Large-Scale Music Discovery Data - PMC - NIH, https://pmc.ncbi.nlm.nih.gov/articles/PMC5362644/ 20. Non-stationary inversion of seismic data for fracture compliances in azimuthally anisotropic media | Petroleum Geoscience - Lyell Collection, https://www.lyellcollection.org/doi/10.1144/petgeo2023-151 21. A simple visual navigation system for an UAV - Intelligent and Mobile Robotics, https://imr.ciirc.cvut.cz/uploads/Pdfs/Krajnik_SimpleVisualUAV_SSD_2012.pdf 22. Real-Time Visual Odometry Covariance Estimation for Unmanned Air Vehicle Navigation, https://arc.aiaa.org/doi/10.2514/1.G004000 23. Multiple sequence alignment - Wikipedia, https://en.wikipedia.org/wiki/Multiple_sequence_alignment 24. Recent Evolutions of Multiple Sequence Alignment Algorithms - PMC - NIH, https://pmc.ncbi.nlm.nih.gov/articles/PMC1963500/ 25. Multiple Sequence Alignment - SlideServe, https://www.slideserve.com/moses/multiple-sequence-alignment 26. Multiple Sequence Alignments, https://dmi.unibas.ch/fileadmin/user_upload/dmi/Studium/Computer_Science/Vorlesung_HS21/Bioinformatics_Algorithms/Chapter03.pdf 27. Algorithms in Bioinformatics: Lecture 12-13: Multiple Sequence Alignment, https://www.eecs.uottawa.ca/~lucia/courses/5126-11/lecturenotes/12-13MultipleAlignment.pdf 28. CMSA: a heterogeneous CPU/GPU computing system for multiple similar RNA/DNA sequence alignment, http://cic.tju.edu.cn/faculty/tangshanjiang/papers/CMSA_a_heterogeneous_CPUGPU_computing_system_for_multiple_similar_RNADNA_sequence_alignment.pdf 29. Examining Phylogenetic Reconstruction Algorithms - Computer Science, https://www.cs.carleton.edu/cs_comps/1314/sgoings1/final-results/PhyloCompsPaper.pdf 30. Ultrafast and ultralarge multiple sequence alignments using TWILIGHT - Oxford Academic, https://academic.oup.com/bioinformatics/article/41/Supplement_1/i332/8199405 31. Advances in post-processing methods for multiple sequence alignment - PMC - NIH, https://pmc.ncbi.nlm.nih.gov/articles/PMC12640688/ 32. (PDF) Multiple Sequence Alignment Based on a Suffix Tree and Center-Star Strategy: A Linear Method for Multiple Nucleotide Sequence Alignment on Spark Parallel Framework - ResearchGate, https://www.researchgate.net/publication/320938722_Multiple_Sequence_Alignment_Based_on_a_Suffix_Tree_and_Center-Star_Strategy_A_Linear_Method_for_Multiple_Nucleotide_Sequence_Alignment_on_Spark_Parallel_Framework 33. An algorithm for DNA multiple sequence alignment based on center star method and keyword tree | Request PDF - ResearchGate, https://www.researchgate.net/publication/289270502_An_algorithm_for_DNA_multiple_sequence_alignment_based_on_center_star_method_and_keyword_tree 34. Lemur \emojititle: Log Parsing with Entropy Sampling and Chain-of-Thought Merging - arXiv, https://arxiv.org/html/2402.18205v1 35. Learning with the Minimum Description Length Principle. - Taylor & Francis Online, https://www.tandfonline.com/doi/full/10.1080/01621459.2025.2583392?src= 36. Tokenization Is More Than Compression - arXiv, https://arxiv.org/html/2402.18376v1 37. Effects of sub-word segmentation on performance of transformer language models - ACL Anthology, https://aclanthology.org/2023.emnlp-main.459.pdf 38. (PDF) Mining Representative Patterns from Large Event Streams - ResearchGate, https://www.researchgate.net/publication/330508213_Mining_Representative_Patterns_from_Large_Event_Streams 39. SWIFT: Mining Representative Patterns from Large Event Streams - DSpace@MIT, https://dspace.mit.edu/bitstream/handle/1721.1/137804/p265-yan.pdf?sequence=2&isAllowed=y
