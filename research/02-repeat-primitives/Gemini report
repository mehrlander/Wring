Exhaustive Analysis of Repeat Primitives and Candidate Control Mechanisms for High-Signal Template Extraction
Executive Summary
The central challenge in the "Wring" project—and in the broader field of unsupervised structure discovery—is the distillation of massive, noisy text corpora into actionable, structural templates. Whether applied to petabytes of server logs, extensive source code repositories, or vast collections of legal documents, the fundamental algorithmic problem remains consistent: identifying high-signal repetitive structures while mitigating the combinatorial explosion of redundant candidates. This report provides a comprehensive, expert-level analysis of the primitives, data structures, and control mechanisms required to solve this problem.
Our exhaustive review of the literature indicates that naive repeat extraction, based solely on Maximal Repeats (MR), is theoretically sound but practically intractable for hierarchical data due to the nesting of repetitive substrings. Conversely, Supermaximal Repeats (SMR) offer extreme compaction but destroy the structural hierarchy necessary for understanding nested logic, such as stack traces or code blocks. The optimal primitive, occupying the theoretical "sweet spot" between recall and precision, is the Closed Repeat (CR) (and its variant, the Near-Supermaximal Repeat). Closed Repeats capture the completeness of occurrence sets without the redundancy of substring variations, effectively "nailing down" the template to its maximal extent.
We further demonstrate that the Suffix Array (SA) combined with the Longest Common Prefix (LCP) array constitutes the only viable indexing substrate for exact extraction in high-performance environments, offering linear-time construction and a memory footprint 3–5 times smaller than Suffix Trees. For datasets exceeding main memory, Robust Winnowing with weighted minimizers provides the necessary coarse seeding. Finally, we delineate a rigorous Candidate Control pipeline utilizing dominance pruning, entropy-based filtering (MDL/DUST), and parametric length-frequency curves to refine raw primitives into high-fidelity skeletons.
1. Introduction: The Pattern Explosion Dilemma
The "Wring" project is tasked with a problem that sits at the intersection of stringology, information theory, and data mining: the extraction of recurring structures from massive text streams. This task is isomorphic to the problem of Repeat Extraction, where the objective is to identify substrings S[i..j] that occur with frequency f \ge 2 within a corpus T. While the definition is simple, the execution is fraught with algorithmic peril.
1.1 The Combinatorial Nature of Repetition
In any sufficiently large or structured dataset, repetition is not merely a feature; it is the dominant characteristic. Source code is built on repeated syntax (keywords, braces, boilerplate headers); server logs are generated by deterministic print statements executing in loops; legal documents are constructed from standardized clauses.
A naive approach to mining these repeats faces the "Pattern Explosion" problem. Consider a string composed of a single repeated character, S = a^n. Every possible substring a^k (where 1 \le k < n) is a repeat. The number of distinct repeated substrings can grow quadratically, approaching O(n^2). In real-world data, while not always quadratic, the number of "maximal repeats" can be massive. For example, in a log line Error: Connection Failed, a naive extractor might report:
	1	Error: Connection Failed
	2	rror: Connection Faile
	3	ror: Connection Fail
	4	...and so on.
Most of these are statistically "significant" in terms of frequency, but they represent the same underlying event. Reporting all of them creates a signal-to-noise ratio approaching zero.
1.2 The Wring Research Objective
The primary research question addressed in this report is: Which primitives yield high-signal candidates while avoiding this pattern explosion?
To answer this, we must deconstruct the problem into three layers:
	1	The Indexing Layer: How do we efficiently represent the text to allow for traversal? (Suffix Trees vs. Arrays vs. Automata).
	2	The Primitive Layer: How do we define a "repeat" to mathematically exclude redundancy? (Maximal vs. Supermaximal vs. Closed).
	3	The Control Layer: How do we filter the remaining candidates based on information content? (Entropy, Dominance, Winnowing).
The following sections provide a rigorous analysis of each layer, synthesized from the provided research materials.
2. The Indexing Substrate: Data Structures for Enumeration
The efficiency of any repeat extraction algorithm is bounded by the capabilities of its underlying index. We compare the three primary candidates—Suffix Trees, Suffix Arrays, and Suffix Automata—alongside approximation techniques like Winnowing.
2.1 Suffix Trees: The Theoretical Gold Standard
The Suffix Tree (ST) is a compressed trie containing all suffixes of a given text T. It is the most powerful data structure for string analysis because it exposes the internal structure of repetitions explicitly in its topology.
	•	Mechanism: Each edge in the ST is labeled with a substring. Each internal node represents a point where at least two suffixes diverge, implying that the path from the root to that node occurs at least twice in the text.
	•	Extraction: Finding all maximal repeats is equivalent to a traversal of the internal nodes. By annotating nodes with "string depth" and "leaf counts," one can extract repeats in O(n) time.
	•	Limitations: The ST is notoriously memory-inefficient. A standard implementation using pointers requires between 20 to 40 bytes per character of input text. For a 10GB log corpus, this would require 200-400GB of RAM, rendering it impractical for the Wring project's scale. Furthermore, the pointer-chasing nature of tree traversal exhibits poor locality of reference, causing CPU cache thrashing on modern hardware.
2.2 Suffix Arrays and LCP: The Practical Workhorse
The Suffix Array (SA) was introduced to solve the memory bottleneck of Suffix Trees. It is a lexicographically sorted array of integers representing the starting positions of all suffixes of T.
	•	Space Complexity: The SA requires 4n bytes (for 32-bit integers) or 8n bytes (for 64-bit). This is a 3-5x reduction compared to Suffix Trees.
	•	The LCP Array: The SA alone loses the structural information of the tree. To recover this, we compute the Longest Common Prefix (LCP) array, where LCP[i] stores the length of the shared prefix between suffix SA[i] and suffix SA[i-1].
	•	The Virtual Suffix Tree: The combination of SA and LCP allows for the simulation of a Suffix Tree traversal. An interval [i, j] in the SA where all LCP values are \ge h corresponds to a subtree in the ST. This concept is critical for Wring: it allows us to use the memory-efficient SA to perform the sophisticated traversals previously reserved for STs.
Table 1: Comparative Analysis of Indexing Structures
Feature
Suffix Tree (ST)
Suffix Array (SA) + LCP
Suffix Automaton (DAWG)
Space Complexity
O(n) nodes/edges. High constant factor (20-40x).
O(n) integers. Low constant factor (4-8x).
O(n) states. Most compact in terms of states, but edges can be dense.
Construction Time
O(n) (Ukkonen’s). Complex implementation.
O(n) (SA-IS/DC3). Simpler, highly optimized libraries available.
O(n). Linear, but construction is intricate.
Locality
Poor (pointer chasing).
Excellent (contiguous array scan).
Moderate (graph traversal).
Repeat Finding
Internal node traversal.
Interval traversal (LCP scan).
State traversal.
Recommendation: The Suffix Array + LCP is the optimal choice for exact repeat extraction in Wring. It balances linear-time performance with a manageable memory footprint, enabling the processing of significantly larger blocks of text per pass.
2.3 Burrows-Wheeler Transform (BWT) and r-index
For scenarios requiring even greater compression, particularly with highly repetitive logs, the Burrows-Wheeler Transform (BWT) offers a path to sub-linear space complexity.
	•	Run-Length Encoded BWT (RLBWT): In highly repetitive text, the BWT consists of long runs of identical characters. The number of runs, r, is often much smaller than n (r \ll n).
	•	r-enum Algorithm: Recent research has demonstrated algorithms like r-enum that can enumerate maximal repeats using O(r) working space. This is a breakthrough for log analysis, where the text is fundamentally low-entropy. By using the BWT, Wring can extract repeats from datasets that are larger than available RAM, provided the data is repetitive.
2.4 Winnowing: The Coarse Sieve
When precise indexing (SA/ST) is infeasible due to scale (e.g., petabytes of data), Winnowing provides a probabilistic fallback.
	•	Mechanism: Winnowing fingerprints a document by selecting a subset of hashes from a sliding window of size w. The defining guarantee is that if a substring of length w+k-1 matches between documents, at least one selected hash (fingerprint) will match.
	•	Minimizers: The standard selection strategy is to choose the "minimizer"—the smallest hash value in the window.
	•	The Density Problem: A naive minimizer scheme tends to select hashes from low-complexity regions (e.g., AAAA) because these regions produce consistent low hash values. This "density bias" degrades performance.
	•	Robust Winnowing: Advanced techniques like Winnowmap and Universal Hitting Sets impose a weighting scheme to penalize frequent k-mers. This ensures that the selected fingerprints are distributed uniformly across the text, preventing the "sampling clustering" that occurs in repetitive logs.
3. The Taxonomy of Repeat Primitives
To solve the pattern explosion problem, we must rigorously define the unit of extraction. The literature provides a hierarchy of repeat types, each with distinct properties regarding completeness and compactness.
3.1 Maximal Repeats (MR)
The Maximal Repeat is the foundational primitive in stringology.
	•	Definition: A repeat R occurring at positions P_1, P_2, \dots is maximal if it cannot be extended to the immediate left or right without reducing the number of occurrences.
	•	Left-Maximality: There exist at least two occurrences preceded by different characters.
	•	Right-Maximality: There exist at least two occurrences followed by different characters.
	•	Utility: MRs capture every potentially interesting pattern. If a pattern is not maximal, it is merely a fragment of something more significant.
	•	The flaw: MRs do not handle containment well. In the string A B C D E A B C D F, the substring B C D is maximal. However, A B C D is also maximal. B C D is contained within A B C D. In a log file with a nested structure (e.g., a stack trace inside a request wrapper), the number of maximal repeats can be overwhelming, describing every sub-segment of the stack trace independently.
3.2 Supermaximal Repeats (SMR)
To combat the verbosity of MRs, the Supermaximal Repeat was introduced.
	•	Definition: An SMR is a maximal repeat that is not a substring of any other maximal repeat.
	•	Compaction: This definition provides the most aggressive pruning. It effectively retains only the "longest" versions of any repeated sequence.
	•	The flaw (Loss of Hierarchy): SMRs are destructive to hierarchical data. Consider a source code repository where a function Init() is cloned 50 times. Inside Init(), there is a block Log("Start") that is also used in 500 other functions.
	•	Init() is a maximal repeat.
	•	Log("Start") is a maximal repeat.
	•	Since Log("Start") is a substring of Init(), and Init() is a maximal repeat, Log("Start") is not supermaximal (if it only appears inside Init() in this specific subset).
	•	Correction: If Log("Start") appears 500 times and Init() appears 50 times, Log("Start") would be supermaximal because it is not contained in a longer repeat with the same frequency. However, if Log("Start") only appeared inside Init(), it would be discarded.
	•	Implication: SMRs fail to identify reusable components that are frequently embedded in larger templates. They flatten the hierarchy into a single layer of longest matches.
3.3 Closed Repeats (CR)
The Closed Repeat (often synonymous with Maximal Closed Substring) represents the theoretical optimum for template extraction.
	•	Definition: A repeat W is closed if there exists no superstring W' containing W such that W and W' occur at exactly the same positions in the text (i.e., they have the same Occurrence List).
	•	The "Extension" Property: If a string S always appears as a prefix of S \cdot c, then S provides no more information than S \cdot c. S is "open" (extendable). If S appears sometimes followed by c and sometimes by d, it effectively "closes" the extension.
	•	Superiority: Closed Repeats solve the redundancy problem of MRs without the destructive nature of SMRs.
	•	In the case of User Login Failed (100 times) and User Login (100 times), User Login has the exact same occurrence set as the longer string. It is not closed. We discard it.
	•	In the case of User Login Failed (50 times) and User Login Success (50 times), User Login (100 times) is closed. Its occurrence set is the union of the two longer strings. It represents a valid, independent template.
Research Insight: The number of closed repeats is bounded by O(n \log n) , whereas maximal repeats can reach O(n^2). This logarithmic bound is crucial for the output-sensitive complexity requirements of Wring.
3.4 Near-Supermaximal Repeats (NSMR)
A refinement of the closed repeat concept is the Near-Supermaximal Repeat.
	•	Net Frequency: The net frequency of a repeat R is the number of its occurrences that are not covered by any longer repeat.
	•	Definition: An NSMR is a repeat with Net Frequency > 0.
	•	Application: This is particularly useful for identifying templates that can stand alone. If a specific error line usually appears in a stack trace but occasionally appears in isolation, it is an NSMR. This primitive is essential for disentangling interleaved log events where templates may overlap or nest.
Table 2: Primitive Evaluation Matrix
Primitive
Completeness (Recall)
Compactness (Precision)
Hierarchical Preservation
Output Complexity
Wring Suitability
Maximal Repeats (MR)
High
Low (Redundant)
High
O(n^2)
Low (Too Noisy)
Supermaximal (SMR)
Low
High (Aggressive)
Low (Flattens)
O(n)
Low (Misses Sub-templates)
Closed Repeats (CR)
High
High (Optimal)
High
O(n \log n)
High
Near-Supermaximal
Moderate
Very High
Moderate
O(n)
High (as filter)
4. Enumeration Approaches: Algorithms and Implementation
Having selected Closed Repeats as the target primitive and SA+LCP as the index, we detail the enumeration algorithms.
4.1 Interval Traversal on SA+LCP
The most efficient enumeration method uses the "virtual suffix tree" traversal on the LCP array.
	•	Algorithm: We scan the LCP array linearly. We maintain a stack of "open" intervals, where each interval represents a node in the virtual suffix tree.
	•	Logic:
	1	When LCP[i] > LCP[i-1], we push a new interval onto the stack. This signifies entering a deeper node (longer string).
	2	When LCP[i] < LCP[i-1], it signifies that the current deep node has ended (the suffix SA[i] diverges). We pop the stack.
	3	The popped interval corresponds to a Maximal Repeat.
	4	Filtering for Closed Repeats: To filter for closed repeats during this traversal, we check the occurrence frequency. If the frequency of the popped interval is identical to the frequency of the parent interval (the one below it on the stack), the child is not closed (it's just a suffix extension of the parent). We merge them.
	•	Efficiency: This approach is strictly O(n) in time and uses O(d) stack space (where d is the max LCP depth). It avoids the massive pointer overhead of building an explicit tree.
4.2 The r-enum Algorithm
For highly repetitive data (logs), the r-enum algorithm is superior.
	•	Mechanism: It operates on the Run-Length Encoded BWT. Instead of traversing every character, it jumps between "runs" of identical characters.
	•	Output Sensitivity: The time complexity is proportional to the number of runs (r) rather than the length of the text (n). Since logs are often 90%+ repetitive, r is a fraction of n.
	•	Result: This allows Wring to extract repeats from compressed representations without full decompression.
4.3 Winnowing for Block Identification
For multi-terabyte datasets, we cannot build a single SA.
	•	Strategy: We use Robust Winnowing to map the corpus.
	•	Process:
	1	Compute weighted minimizers for all documents.
	2	Build an inverted index mapping Fingerprint -> List<DocumentID>.
	3	Identify (DocA, DocB) pairs that share a high number of collision fingerprints.
	4	Cluster: Group these documents into "shards."
	5	Refine: Run the exact SA+LCP extraction on each shard independently.
	6	Merge: Combine the local repeats from shards into global templates. This hybrid approach balances the scale of Winnowing with the precision of Suffix Arrays.
5. Candidate Control: From Strings to Templates
A "Closed Repeat" is a string. A "Template" is a semantic concept. The transformation requires filtering based on information content and structure.
5.1 Dominance Pruning and the Pareto Frontier
In many cases, the system produces multiple overlapping candidates for the same template.
	•	Example: Error 500: Database Connection (Freq: 50) and 500: Database Connection (Freq: 52).
	•	Dominance Logic: We have two objectives: Length (maximize) and Frequency (maximize).
	•	Pareto Frontier: A candidate C_1 dominates C_2 if Length(C_1) \ge Length(C_2) and Freq(C_1) \ge Freq(C_2).
	•	Relaxed Dominance: In the example above, the shorter string is slightly more frequent (52 vs 50). Strict dominance doesn't apply. However, the information gain of the extra "Error " prefix is high, while the frequency loss (2 occurrences) is low.
	•	Algorithm: We implement a Dominance Graph. Nodes are Closed Repeats. Edges represent containment. We prune node B if it is contained in A and \frac{Freq(B) - Freq(A)}{Freq(B)} < \epsilon. This \epsilon is the "tolerance" for noise.
5.2 Parametric Length-Frequency Curves
Static thresholds (e.g., "Min Length = 20") are destructive. A short string like SEGFAULT is critical; a long string like a 100-character line of dashes is noise.
	•	The Curve: We define a parametric threshold curve T(L).
	•	For short strings (L < 10), required frequency F is high (e.g., must appear in 10% of docs).
	•	For long strings (L > 100), required frequency F is low (e.g., must appear twice).
	•	Equation: F_{req} = \frac{K}{L^\alpha} where \alpha is a tuning parameter (typically 1.0 - 1.5).
	•	Source Code Clone Detection: This technique is extensively used in tools like SourcererCC and CloneWorks to balance Type-1 (exact) and Type-3 (gap) clones. Large code blocks are flagged as clones even with lower similarity, while small snippets require exact matches.
5.3 Entropy and Complexity Filtering
High-frequency repeats often map to low-information separators.
	•	Shannon Entropy: H(S) = - \sum p_i \log_2 p_i.
	•	Strings like ========== have H \approx 0.
	•	Strings like a.b.c.d.e have high entropy.
	•	Filter: Reject candidates with H < \theta.
	•	Minimum Description Length (MDL):
	•	MDL principle states that the best hypothesis is the one that compresses the data the most.
	•	Test: Does extracting repeat R into a dictionary and replacing its occurrences with a pointer reduce the total bit-size of the corpus?
	•	For a repeat of length 5 occurring 2 times, the overhead of the dictionary entry + pointers exceeds the savings. MDL naturally prunes insignificant repeats.
6. Domain-Specific Implementations
The tuning of these primitives varies by the nature of the text.
6.1 Server Logs: The Variable Identification Problem
	•	Structure: Line-based, high repetition, embedded variables (IPs, Timestamps).
	•	Challenge: "Closed Repeats" will fragment if variables are distinct. User 123 and User 456 are separate closed repeats.
	•	Solution: Frequent Closed Itemsets. We view the log lines not just as strings but as itemsets of tokens. By mining Closed Frequent Itemsets , we can identify that {User, Login} co-occur frequently.
	•	The Skeleton: The "Skeleton" of a log is the intersection of the closed repeats. If we have User A Login and User B Login, the LCP is User . The Longest Common Suffix is Login. The skeleton is User * Login.
	•	Algorithm: Extract Closed Repeats. Cluster them by edit distance. The "center" of the cluster is the template; differences are variables.
6.2 Source Code: Clone Detection
	•	Structure: Hierarchical (AST), syntax-heavy.
	•	Clones:
	•	Type-1: Exact matches. (Solved by SA).
	•	Type-2: Renamed identifiers.
	•	Type-3: Gapped/Modified.
	•	Tokenization: To find Type-2 clones using exact repeat primitives, Wring must perform a Tokenization Pass. Replace all identifiers with ID, all strings with STR. The code if (x > 0) becomes IF (ID > LIT).
	•	SA Application: Running SA+LCP on the token stream finds Type-2 clones as exact Type-1 repeats in the token space. This is the core logic of SourcererCC.
6.3 Legal Documents: Boilerplate Removal
	•	Structure: Long, verbatim paragraphs (disclaimers).
	•	Approach: Winnowing is most effective here.
	•	Boilerplate Detection: If a text block (Winnowing fingerprint) appears in > X\% of documents across different clusters (e.g., different lawsuits), it is boilerplate.
	•	Skeleton: The "Document Skeleton" is the set of unique text remaining after boilerplate (maximal repeats) is removed.
7. Architectural Recommendations for Wring
Based on this exhaustive analysis, we propose the following architecture for the Wring repeat extraction engine:
	1	Hybrid Indexing Strategy:
	•	Use Robust Winnowing (w=10, k=15) to shard the dataset.
	•	Use SA-IS + LCP + Stack Traversal for exact extraction within shards.
	•	Use RLBWT for log compression and in-memory traversal of high-redundancy blocks.
	2	Primitive Selection:
	•	Target Closed Repeats primarily.
	•	Use Near-Supermaximal Repeats (NSMR) to filter output for top-level templates.
	•	Strictly avoid raw Maximal Repeats to prevent explosion.
	3	Filtration Pipeline:
	•	Pre-Filter: Entropy check (H > 1.5) during LCP traversal.
	•	Inline-Filter: Parametric Curve (F > K/L^\alpha) check during traversal.
	•	Post-Filter: Dominance Graph pruning to merge nested templates.
	4	Skeletonization:
	•	Cluster Closed Repeats by edit distance (or locality).
	•	Derive the "Literal Skeleton" by computing the intersection of clustered repeats. Gaps become variables.
8. Conclusion
The "Wring" project requires a sophisticated approach to repeat extraction that moves beyond simple string matching. The Suffix Array provides the necessary speed; Closed Repeats provide the necessary logic to "nail down" occurrences; and Entropy/Dominance filters provide the necessary noise control. By synthesizing these elements into a cohesive pipeline, Wring can effectively "wring out" the structure from chaos, delivering high-signal templates from massive, unstructured corpora. The transition from Maximal to Closed repeats is the single most critical factor in ensuring the system's output remains intelligible and actionable.
Works cited
1. Trie vs. suffix tree vs. suffix array - Stack Overflow, https://stackoverflow.com/questions/2487576/trie-vs-suffix-tree-vs-suffix-array 2. Suffix Arrays: A New Method for On-Line String Searches, https://users.cs.utah.edu/~pandey/courses/cs6968/spring23/papers/suffixarray.pdf 3. Weighted minimizer sampling improves long read mapping - bioRxiv, https://www.biorxiv.org/content/10.1101/2020.02.11.943241v1.full.pdf 4. Trie, Suffix Tree, Suffix Array - HackerEarth, https://www.hackerearth.com/practice/notes/trie-suffix-tree-suffix-array/ 5. Understanding Suffix Trees and Arrays: Advanced Data Structures for String Processing, https://algocademy.com/blog/understanding-suffix-trees-and-arrays-advanced-data-structures-for-string-processing/ 6. Relative Suffix Trees - PMC - NIH, https://pmc.ncbi.nlm.nih.gov/articles/PMC5956352/ 7. Difference between Suffix Array and Suffix Tree - GeeksforGeeks, https://www.geeksforgeeks.org/dsa/difference-between-suffix-array-and-suffix-tree/ 8. SuffixArrays, https://www.cs.yale.edu/homes/aspnes/pinewiki/SuffixArrays.html 9. Suffix Tree Application 3 - Longest Repeated Substring - GeeksforGeeks, https://www.geeksforgeeks.org/dsa/suffix-tree-application-3-longest-repeated-substring/ 10. R-enum Revisited: Speedup and Extension for Context-Sensitive Repeats and Net Frequencies - arXiv, https://www.arxiv.org/pdf/2511.11057 11. (PDF) Winnowing: Local Algorithms for Document Fingerprinting - ResearchGate, https://www.researchgate.net/publication/2840981_Winnowing_Local_Algorithms_for_Document_Fingerprinting 12. Minmers are a generalization of minimizers that enable unbiased local Jaccard estimation, https://pubmed.ncbi.nlm.nih.gov/37325780/ 13. Improving the performance of minimizers and winnowing schemes - bioRxiv, https://www.biorxiv.org/content/10.1101/104075v1 14. Space-Efficient Computation of Maximal and Supermaximal Repeats in Genome Sequences, https://www.researchgate.net/publication/261851051_Space-Efficient_Computation_of_Maximal_and_Supermaximal_Repeats_in_Genome_Sequences 15. (PDF) Efficient repeat finding in sets of strings via suffix arrays - ResearchGate, https://www.researchgate.net/publication/267436646_Efficient_repeat_finding_in_sets_of_strings_via_suffix_arrays 16. (PDF) Maximal Closed Substrings - ResearchGate, https://www.researchgate.net/publication/363208685_Maximal_Closed_Substrings 17. Closed Repeats - arXiv, https://www.arxiv.org/pdf/2410.00209 18. Closed Repeats, https://arxiv.org/abs/2410.00209 19. Computing All Repeats Using Suffix Arrays. | Request PDF - ResearchGate, https://www.researchgate.net/publication/220520502_Computing_All_Repeats_Using_Suffix_Arrays 20. Dominance Pruning in Machine Learning for Solving Financial Trading and Real-Time Multimedia Applications - ResearchGate, https://www.researchgate.net/publication/371647098_Dominance_Pruning_in_Machine_Learning_for_Solving_Financial_Trading_and_Real-Time_Multimedia_Applications 21. Multi-threshold token-based code clone detection - arXiv, https://arxiv.org/pdf/2002.05204 22. (PDF) Multi-threshold token-based code clone detection - ResearchGate, https://www.researchgate.net/publication/339252314_Multi-threshold_token-based_code_clone_detection 23. Filter reads for low complexity - Chipster, https://chipster.csc.fi/manual/prinseq-complexity-filter.html 24. Minimum description length - Wikipedia, https://en.wikipedia.org/wiki/Minimum_description_length 25. A Tutorial Introduction to the Minimum Description Length Principle - CWI, https://homepages.cwi.nl/~pdg/ftp/mdlintro.pdf 26. (PDF) Comprehensive Log Compression with Frequent Patterns - ResearchGate, https://www.researchgate.net/publication/220802378_Comprehensive_Log_Compression_with_Frequent_Patterns 27. COBRA: Closed Sequential Pattern Mining Using Bi-phase Reduction Approach, https://www.researchgate.net/publication/220802359_COBRA_Closed_Sequential_Pattern_Mining_Using_Bi-phase_Reduction_Approach 28. SourcererCC: Scaling Code Clone Detection to Big Code - Cheriton School of Computer Science, https://cs.uwaterloo.ca/~m2nagapp/courses/CS846/1171/papers/sajnani_icse16.pdf 29. Web2Text: Deep Structured Boilerplate Removal - Health NLP, https://health-nlp.com/files/pubs/ecir18a.pdf
