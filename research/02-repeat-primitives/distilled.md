# Distilled Findings: Repeat Primitives + Candidate Control

## Executive Summary

The central challenge in repeat detection is avoiding **pattern explosion** while maintaining high signal. A naive approach finds every repeated substring, which in pathological cases can produce O(n²) candidates—most being redundant fragments of longer patterns. The research identifies three repeat types with dramatically different properties: **Maximal repeats** (cannot extend without losing occurrences) capture everything but produce massive redundancy; **supermaximal repeats** (not contained in any longer repeat with same frequency) provide extreme compaction but destroy hierarchical structure by discarding useful sub-components; **closed repeats** (no superstring shares the same occurrence set) occupy the theoretical sweet spot, producing O(n log n) patterns that eliminate redundancy without losing structural relationships. Closed repeats directly correspond to interpretable template skeletons—they capture the longest stable literal segments where each occurrence set is unique.

The choice of enumeration algorithm determines both performance and memory viability. **Suffix trees** expose repetition structure explicitly in their topology but require 20–40× text size in memory, making them impractical for multi-MB documents in browsers. **Suffix arrays with LCP arrays** provide the same capabilities with 3–5× lower memory (4–8 bytes per suffix) and better cache locality through contiguous array scans rather than pointer chasing. The SA+LCP combination enables "virtual suffix tree" traversal via interval stack processing, enumerating all repeats in O(n) time. **Suffix automata** naturally merge patterns sharing occurrence sets, effectively producing closed repeats by construction—each state represents an equivalence class of substrings with identical end positions. **Winnowing/fingerprinting** sacrifices completeness for extreme scalability, selecting representative k-grams via rolling hashes to seed pattern discovery, making it ideal for pre-filtering very large inputs before applying exact methods.

Candidate control requires layered filtering beyond algorithmic choice. **Length and frequency thresholds** are essential—requiring minimum occurrence counts (≥3–5 for logs, ≥2 for legal text) and minimum lengths (≥8 characters or 2–3 tokens) eliminates trivial patterns like common words or punctuation. **Dominance/containment pruning** removes patterns that are strictly contained in longer patterns with identical occurrence sets, which is automatically enforced by closed repeat definitions. **Parametric curves** (F_required = K/L^α) adaptively adjust thresholds so short strings need high frequency while long strings can be rare. **Entropy filtering** rejects low-information separators (runs of dashes, repeated characters). The MDL principle naturally guides these choices: only extract patterns where dictionary overhead plus pointer costs are less than literal repetition costs, automatically pruning insignificant repeats.

## Key Insights

- **Closed repeats solve the redundancy-hierarchy trade-off**: Unlike maximal repeats (too many fragments) or supermaximal (loses nested components), closed repeats preserve both completeness and structure—bounded at O(n log n) versus O(n²) for maximal
- **Suffix array + LCP is the practical workhorse**: 4–8 bytes per character versus 20–40 bytes for suffix trees; linear-time interval stack traversal emulates tree functionality with far better memory locality and cache performance
- **Suffix automaton naturally produces closed repeats**: States represent equivalence classes of substrings with identical endpos sets—taking the longest substring per state directly yields closed patterns without post-processing
- **Winnowing enables scale-out**: Guarantees that any substring ≥ w+k-1 shares at least one fingerprint; robust variants with weighted minimizers prevent density bias in repetitive regions; ideal for sharding multi-GB inputs before exact analysis
- **Thresholds must be adaptive, not static**: A 100-character boilerplate appearing twice is significant; a 3-character fragment appearing 1000 times is noise—parametric length-frequency curves (F ∝ 1/L^α) handle this naturally
- **Domain dictates strategy balance**: Logs need aggressive pre-normalization (replace IDs/timestamps) to expose structure; legal text has verbatim repeats needing minimal normalization; source code requires tokenization (identifiers→ID) to find Type-2 clones as exact repeats in token space
- **Containment creates the explosion**: In "User alice logged in" and "User bob logged in", maximal repeats include "User ", " logged in", "ser ", "ogged in"—dozens of overlapping fragments; closed repeats yield just "User " and " logged in" as distinct occurrence sets
- **Output-sensitive complexity is the real cost**: Enumeration is O(n) once indexed, but if output is O(n²) patterns, filtering becomes the bottleneck—closed repeats' O(n log n) bound makes post-processing tractable
- **r-enum for highly repetitive data**: Operates on run-length BWT with O(r) space where r = number of runs << n; logs often have 90%+ redundancy, making compressed-domain extraction viable without full decompression
- **Near-supermaximal repeats isolate standalone templates**: Net frequency (occurrences not covered by longer patterns) > 0 identifies patterns that appear both embedded and independently—critical for disentangling nested log events

## Recommendations

| Aspect | Primary Approach | Alternative/Complement | Rationale |
|--------|-----------------|------------------------|-----------|
| **Repeat primitive** | Closed repeats (CR) | Near-supermaximal (NSMR) for filtering | Eliminates redundancy (O(n log n) bound) while preserving hierarchy; NSMR identifies standalone vs. embedded patterns |
| **Enumeration algorithm** | SA + LCP interval stack traversal | Suffix automaton for natural merging | SA provides 3–5× memory advantage over trees; O(n) construction and enumeration; excellent cache locality |
| **Large-scale pre-filter** | Robust winnowing (k=8–15, w=10–20) | — | Reduces multi-GB inputs to candidate regions; weighted minimizers prevent sampling bias in repetitive data |
| **Highly repetitive data** | r-enum on RLBWT | — | Operates in O(r) space where r = BWT runs; logs with 90%+ redundancy compress to r << n |
| **Length threshold** | Minimum 8 characters or 2–3 tokens | Parametric: F > K/L^α (α=1.0–1.5) | Static threshold blocks short but meaningful patterns (e.g., "SEGFAULT"); adaptive curve balances length vs. frequency |
| **Frequency threshold** | Min 3–5 for logs, 2 for legal/code | Adjust by document redundancy | Higher threshold for high-volume logs; lower for contracts/code where patterns may appear exactly twice |
| **Containment pruning** | Enforce closed repeat definition | Dominance graph with tolerance ε | Automatically removes patterns with identical occurrence sets; graph-based relaxation handles near-identical frequencies |
| **Entropy filtering** | Reject H < 1.5 during traversal | MDL test for compression gain | Eliminates separators (====, ----, etc.) and low-complexity runs; MDL ensures extraction cost < literal cost |
| **Implementation** | WASM for SA construction, JS for filtering | Suffix automaton in JS for <2MB | Native SA-IS or DC3 algorithm in C++/WASM for speed; automaton feasible in pure JS for moderate sizes; transferring huge SA across boundary is memory concern |
| **Hybrid strategy** | Winnowing → SA+LCP on shards | Parameterized + raw text mining in parallel | Fingerprint-based sharding for scale; exact methods on identified regions; dual mining (normalized + raw) catches both exact and structural matches |
| **Domain: Logs** | Aggressive pre-typing + line clustering | Frequent closed itemsets for skeletons | Replace timestamps/IDs/IPs before mining; cluster by edit distance; intersection of cluster = literal skeleton |
| **Domain: Source code** | Tokenize (ID/STR/LIT) + SA on token stream | AST-based for Type-3 clones | Finds Type-2 clones (renamed identifiers) as exact repeats in normalized token space—SourcererCC approach |
| **Domain: Legal text** | Minimal normalization, focus on long verbatim | Winnowing for cross-document boilerplate | Exact phrasal repeats common; if fingerprint appears in >X% of documents, it's boilerplate to exclude |
