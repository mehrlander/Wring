Comparative Evaluation of Repeat Enumeration Techniques for Template Induction

Introduction

Template induction aims to discover recurring structural patterns (literal “skeletons” with variable slots) from a single large document. This is challenging because a naive enumeration of all repeated substrings can produce a pattern explosion, overwhelming the algorithm and obscuring meaningful templates ￼. We evaluate four core repeat enumeration primitives for this task: (1) Suffix Array + LCP interval traversal, (2) Suffix Tree traversal, (3) Suffix Automaton, and (4) Winnowing/Fingerprinting. We compare them in terms of the quality of candidates (how interpretable the repeated patterns are), available control mechanisms to prune noise (thresholds, pruning of contained patterns), and computational performance (especially in JavaScript/WASM environments). We also examine how choosing different repeat types (maximal, supermaximal, closed repeats) impacts the clarity of template skeletons in various document types (logs, legal text, source code). Finally, we discuss hybrid strategies and heuristics that can improve precision and recall – for example, using fingerprinting to seed candidate regions and a suffix-based structure to refine them. The goal is to identify the best repeat-detection strategy for high-fidelity template induction in the Wring project’s context (single-document, 100KB–10MB, browser-based analysis).

Repeat Enumeration Approaches

1. Suffix Array + LCP Interval Traversal

Suffix arrays (SA) provide a memory-efficient index of all suffixes of the text, and the Longest Common Prefix (LCP) array between adjacent suffixes can be leveraged to enumerate repeats ￼. The idea is that any substring that appears at least twice will show up as a prefix common to multiple suffixes. By scanning the LCP array and tracking LCP intervals (continuous regions where the LCP is at least a certain length), one can enumerate each repeated substring (with its occurrence count and positions) when an interval closes ￼. This essentially finds maximal repeats (substrings that cannot be extended without losing an occurrence) by identifying when the LCP drops, indicating divergence. SA+LCP traversal thus yields all repeated substrings (typically maximal by default) along with their frequency and occurrence list.

Candidate Quality: In practice, a raw SA+LCP enumeration produces a very exhaustive list of repeats – including many short or low-value patterns (e.g. common words, single characters, or whitespace). Without filtering, the result can include a lot of trivial or fragmentary repeats that are not meaningful template anchors. For example, given a log file, SA+LCP might output common tokens like "Error " or " at " as well as partial words, if those character sequences recur. Many of these are not directly useful for template skeletons without further processing. However, the strength of this approach is that it captures all exact repeats (no false negatives), so the high-value patterns will be among the candidates, just potentially hidden in noise. Candidate interpretability can be improved by applying thresholds (discussed below) to focus on longer substrings that repeat enough times to signify a structural pattern (e.g. full phrases rather than 2-3 character fragments).

Control Mechanisms: It’s straightforward to impose minimum length or frequency thresholds during the SA+LCP scan – e.g. only emit a repeat if it has length ≥ L or occurs ≥ k times. This immediately cuts off the explosion of short or infrequent repeats. Additionally, one can apply containment pruning post hoc: if a reported repeat is entirely contained in a longer repeat with the same occurrence set, the shorter can be dropped as redundant. (This is how we enforce finding closed or supermaximal repeats – more on repeat types below.) For example, suppose the substring " wo" appears in a text multiple times. If " wood" appears with the same frequency and covers those same occurrences, then " wo" is not a useful anchor on its own – it’s strictly contained in the longer repeat. In practice, supermaximal/closed filtering removes such dominated fragments, leaving only the largest literal skeleton for each set of occurrences ￼. Suffix arrays make it easy to detect this: a longer LCP interval covering the same suffixes indicates a longer repeat with identical frequency. By scanning intervals, we can identify maximal repeats and then filter out those that are part of a longer repeat with equal frequency ￼.

Performance: A suffix array can be constructed in O(n) or O(n * log n) time for a document of length n, and the LCP array in O(n) time. The approach is output-sensitive – enumerating repeats is linear in the number of reported patterns (which in worst-case can be large). In the best case (e.g. a mostly non-repetitive text), only a small fraction of suffixes share common prefixes, so the overhead is modest. However, in a highly repetitive document, the number of LCP intervals (and thus candidate repeats) can be enormous, making traversal slower. Memory-wise, the suffix array and LCP each require space proportional to n (e.g. 4–8 bytes per suffix index) ￼. This is linear, but the constants are significant – for a 10MB text, SA+LCP might consume on the order of tens of megabytes (e.g. ~80 MB if 8 bytes per entry). While this is still linear space, it “requires significantly more space than the string itself” ￼, which is a consideration in a browser setting. A pure JavaScript implementation of suffix array construction may be too slow for multi-MB input (due to sorting costs). In practice, one would likely use a WASM-optimized routine for SA construction to achieve near-linear time. With that, SA+LCP is viable up to moderate sizes, but pushing toward the upper 10MB range in JS may be borderline without optimizations. The advantage, however, is that once the SA and LCP are built, iterating to find repeats with thresholds is efficient and simple.

2. Suffix Tree Traversal

A suffix tree is a compressed trie of all suffixes; all repeated substrings correspond to internal nodes in this tree. Intuitively, any internal node (with at least two leaf descendants) represents a substring that appears in at least two places (the path from the root to that node). Traversing all internal nodes thus enumerates all repeats. In fact, a path that ends at an internal node and branches into multiple children is by definition a maximal repeat – you cannot extend that path by one more character without branching (because the children start with different characters or one occurrence ends) ￼ ￼. So a full traversal of the suffix tree’s internal nodes yields all maximal repeats and their occurrence counts (often by counting the leaves under that node).

Candidate Quality: Like SA+LCP, a raw suffix tree traversal will list every repeated substring, which can be too comprehensive. All sorts of repeated fragments (down to length 1 if a character appears twice) will appear as some internal node. Without filtering, this produces a huge set of candidates in a repetitive document. The patterns themselves are the same as one would get from SA+LCP (these structures are equivalent in the information they capture), but the tree structure can additionally reveal how repeats nest inside each other (hierarchical relationships). That hierarchy might be exploitable for template induction (for example, a shorter repeat node being a prefix of a longer repeat node), but it can also be overwhelming. In terms of interpretability, we again need to focus on longer, high-signal patterns. One benefit of the suffix tree approach is that it can directly compute properties like the number of occurrences (leaf count) and perhaps left/right extension contexts, which helps classify repeats as maximal or not. Still, without pruning, you will see many small tokens and fragments.

Control Mechanisms: We can integrate threshold pruning into the tree traversal: for instance, only explore or report nodes that have ≥ k leaves (occurrences), or that correspond to substrings of length ≥ L. This prunes subtrees that are not promising. The tree structure allows a pruning on the fly – e.g. if a node has fewer than 2 leaves, you stop exploring that branch, and if it has fewer than k, you might skip reporting it (though it could still have deeper nodes that meet the threshold if occurrences split further down). Frequency threshold is easy to apply because each node can store the count of leaves in its subtree. Dominance/containment pruning can also be applied: if a node’s substring is contained in a longer substring with the same occurrences, it means that node has a single child path that continues with all leaves, which in a properly built compressed suffix tree generally wouldn’t appear as a separate branching node. (In other words, suffix trees inherently collapse non-branching paths, so a repeat that can be extended on both ends in all occurrences tends not to form an internal node on its own.) Thus, many “dominated” repeats never show up as separate nodes – the tree naturally represents the longest common prefix of those occurrences as one node. This is effectively capturing closed repeats by design: no internal node’s substring is strictly contained in another internal node’s substring with the same support, because that would imply a redundant node. In cases where a shorter repeat has the same occurrence set as a longer one, they would appear along the same root-to-leaf path, with only the longest forming a branching node. For example, in the SeqAn library demo, the substring " wo" was a maximal repeat (cannot extend to the right universally) but not supermaximal, since it is part of " wood" which has the same frequency ￼. In a suffix tree, " wo" would not appear as a separate internal node if " wood" encompasses all those occurrences; the branch would simply extend to " wood". That said, explicit post-processing can still be done: after collecting repeats from the tree, filter out any that are a subset of another’s occurrences (to ensure closedness ￼).

Performance: Suffix trees can be constructed in linear time O(n) with Ukkonen’s or other algorithms, and require linear space in theory ￼. However, the constant factors are high: “storing a string’s suffix tree typically requires significantly more space than the string itself.” ￼ Each node and edge store pointers/indices; a 10MB text can easily turn into tens of millions of nodes (in worst-case) and a large memory footprint (4–10 times the text size is common in practice). This is problematic in a browser/JS environment, both in terms of memory consumption and allocation overhead. Building a 10MB suffix tree in pure JavaScript is likely infeasible due to speed and memory garbage-collection issues. In WASM with optimized code and careful memory management, it’s more achievable, but still heavy. Traversal of the tree is output-sensitive – it will visit every internal node. If the document has many repeats (e.g. highly repetitive or many repeating units), the number of internal nodes can be O(n) or more (in pathological cases, though typically the count of maximal repeats is sub-linear or O(n log n) in worst case ￼ ￼). In summary, suffix trees provide rich information (useful if we wanted to do more complex pattern analysis), but for the limited goal of listing repeated substrings they may be overkill. They scale less well in JS due to implementation complexity. A suffix array or automaton is usually preferred for practical reasons (or a compressed suffix structure like an FM-index, if we were focusing on memory – but that’s beyond our scope).

3. Suffix Automaton

A suffix automaton (also known as a Directed Acyclic Word Graph, DAWG) is a compact automaton that recognizes all substrings of the text ￼. It has the remarkable property of representing all distinct substrings in O(n) states and transitions, on average, and can be built in O(n) time ￼. Each state in a suffix automaton corresponds to an equivalence class of end positions (endpos set) of substrings – effectively grouping substrings that appear in the same set of locations. This structure makes it very convenient to retrieve substring frequency: each state can store the number of endpos in its set, which equals the number of occurrences of all substrings represented by that state. To get repeated substrings, one can simply take all states with occurrence count ≥ 2 (excluding the initial state which represents the empty substring). The longest substring corresponding to each state is typically a maximal repeat for that occurrence set (since if you could extend it and still have the same occurrences, those extensions would be in the same state). In other words, each state inherently represents a closed repeat pattern together with all its sub-substrings. For example, if a substring P occurs in positions {10, 50, 90} (frequency 3) and is represented by a state, that state will also implicitly represent any shorter substring of P that shares exactly those 3 end positions – but those shorter ones would usually have additional occurrences if they appear elsewhere. Only the longest one remains “closed” with respect to that set. Thus, by focusing on the longest substring per state, we get the closed repeats directly ￼.

Candidate Quality: The suffix automaton essentially gives us a deduplicated set of repeats. Unlike suffix trees or arrays which enumerate possibly overlapping intervals, the automaton merges together patterns that have the same occurrence distribution. This means it naturally avoids listing trivially redundant substrings. Using the automaton, we could get high-level candidates like “ wood” and “chuck” from the earlier example, and it would not separately list “ wo” because that smaller repeat did not have a strictly larger occurrence set (it’s contained in the same endpos set as “ wood”) ￼. This is a big win for pattern interpretability: many extraneous fragments are merged into a larger representative. That said, the automaton still represents all substrings – if a short substring like “ error ” appears in many places (with possibly different larger contexts each time), it will have its own state with a high count. So you still need to filter by length or context to avoid very short anchors that are semantically meaningless. Another point is that the automaton by itself doesn’t directly give the substring lexicographically or by content; it gives you states and transitions. However, one can store for each state the length of the longest substring in that state, and one example end position. With that, you can extract the actual substring (by taking the text segment ending at that position with the given length). This extra step yields the actual repeat string for output.

Control Mechanisms: It’s easy to apply frequency and length thresholds with a suffix automaton. During or after construction, one can compute the occurrence count of each state (propagating counts from end states via suffix links). Then simply consider only states with count ≥ k and with longest-substring length ≥ L. This will drop all low-frequency or short repeats from consideration. Because of the endpos merging, the automaton inherently implements dominance pruning: if a shorter pattern always occurs as part of a specific longer pattern, they share the same end positions and thus end up in the same state. We would output only the longest substring for that state (ensuring no strictly contained duplicate patterns with identical occurrence sets clutter the output). In effect, it outputs closed repeats by default if we take one representative per state ￼. For example, if all occurrences of " at " in a document are followed by "/" (making the substring " at /" appear just as often), then " at " and " at /" initially both qualify as repeats, but the automaton will treat them differently: " at /" might form a state with those end positions, and " at " will either be in the same state (if it never occurs outside that context) or have a higher count (if it also appears elsewhere). If it’s the same count (same positions), they collapse. If " at " also appears elsewhere, then it is a meaningful repeat on its own with a larger occurrence set, and the automaton will reflect that with a different state (and higher frequency). We can then choose to output both because they represent different sets of occurrences (one broader, one narrower). However, the narrower one (" at /") is the closed pattern for that subset of occurrences and arguably the better template anchor for those particular instances (since it captures the trailing slash that consistently appears). Thus, the automaton gives flexibility: we can output all maximal repeats (all states count ≥2) or only closed ones (longest per endpos set), depending on what yields better templates. Typically, choosing the longest repeat for each occurrence set yields cleaner anchors.

Performance: Suffix automaton construction is very efficient: it runs in O(n) time for a string of length n (assuming constant alphabet, which is true for e.g. ASCII/UTF-8 text) ￼. Memory usage is also linear in n; a suffix automaton has at most 2n - 1 states and ~2n - 2 transitions in the worst case. In practice it often uses about 2–3 times the text size in memory for the data structure. For example, building an automaton for a 1MB string might use on the order of a few MB of memory for states and transitions. This is somewhat lighter than a suffix tree (which can be ~10x text size) but heavier than a suffix array (~4–8x if 8-byte indices). One reason is that each state typically stores a map of outgoing transitions. If implemented in C++ with an array of size equal to alphabet or a compact vector, memory is predictable; in a dynamic language like JavaScript, representing the automaton (states as objects with dictionaries) can add overhead. Still, the simplicity of implementation is a plus – a suffix automaton can be coded relatively straightforwardly in JS, whereas a suffix tree or an efficient suffix array construction is much more involved. For a 100KB–1MB document, a JS suffix automaton should run comfortably (tens of milliseconds to a small number of seconds). For a multi-MB (5–10MB) document, it might start pushing the limits of JS in terms of speed and memory, but using WASM or a highly optimized approach can mitigate this. Notably, suffix automaton is also friendly to streaming addition (it’s an online algorithm) and doesn’t require random access or large intermediate arrays, which could be beneficial in constrained environments. Overall, scalability in JS/WASM is quite good: many competitive programming tasks use suffix automata for large strings because of their linear performance ￼. The main caution is that in a pathologically repetitive string (like “AAAA…A”), the automaton will still create ~2n states (because each new suffix adds a new state), and listing all repeats (which are basically all substrings of that string) is infeasible regardless of method without thresholds. But in realistic data, automaton provides an excellent balance of speed and manageable output via its implicit merging of patterns.

4. Winnowing (Fingerprinting)

Winnowing is a local fingerprinting technique originally developed for document similarity detection (e.g. plagiarism detection) ￼. Instead of indexing all substrings, winnowing selects a representative subset of substrings (of a fixed length k) by hashing them and sliding a window. Specifically, one chooses a window of width w (≥ k) and computes hash values for every k-gram in the text; in each window of w consecutive k-grams, it picks the minimum hash (the “fingerprint”) ￼. This yields a set of fingerprint hashes and their positions that characterizes the document. The key property is that any substring of length ≥ k that appears in the text at least twice will almost certainly produce a common fingerprint in those two occurrences (assuming a good hash and appropriate window) ￼. In effect, winnowing guarantees detection of sufficiently long repeats above a threshold length t (which is usually k or slightly more) ￼, while ignoring substrings shorter than k entirely (a built-in noise filter).

For template induction, using fingerprinting means we do not enumerate all repeated substrings; instead we identify regions of the document that are likely repeated by looking for matching fingerprints. For example, if the text has a repeated 50-character segment, by hashing every say 20-character substring and winnowing, we might pick one or two fingerprints that occur in both instances of that segment. Those matching hashes tell us “these two positions share at least a 20-character substring in common.” We can then align or extend around those positions to recover the full common substring (by direct comparison of the text). The winnowing approach is essentially a seed-and-extend strategy: fingerprints serve as seeds for repeated regions, which we then verify and potentially extend to maximal length matches.

Candidate Quality: The main advantage of winnowing is that it drastically reduces the number of candidate substrings to consider. We only get anchors that are likely part of a relatively long repeat. All short repeats (below the k-gram length) are ignored from the start. This means the output is much smaller and focused on longer patterns. It inherently avoids the explosion of trivial patterns. However, the trade-off is that winnowing might miss some repeats that are borderline or that don’t have a consistent k-length chunk across all occurrences. In theory, if a pattern is longer than the guarantee threshold, at least one k-gram from it will be picked as a fingerprint ￼. But consider scenarios: if k is large (to filter noise) and a repeat is just at that length or has high variance, you might not capture it if by chance its hashes aren’t minimal in their windows. The algorithm design usually ensures that if the pattern length ≥ w, you’ll catch it at least once ￼, but very sparse repeats or those shorter than k are deliberately not detected. For template induction, this means you have to set k to the smallest interesting substring length (maybe a few characters or a whole word). Anything repetitive below that length will be ignored, which is probably desirable (we’re not interested in 1-2 character repeats). The fingerprints themselves (the k-grams) may not correspond to entire template skeletons, but rather parts of them. For instance, using winnowing on logs might yield an anchor like "logged in from 10.0.0." as a selected k-gram if k covers that phrase; if that hash occurs for multiple log lines, we know those lines share that segment. We would then extend to see how far that commonality goes (e.g. include preceding "User " and perhaps following static text). Thus, fingerprint candidates are coarse anchors – potentially very interpretable (because k can be chosen as a moderate phrase length), but they might not directly be maximal repeats. We must refine them to get the full repeated pattern. On the positive side, since only one or a few fingerprints might represent a large repeated region, the output list is very manageable and high-precision: each matching fingerprint points to a real repeated chunk of text of length ≥ k.

Control Mechanisms: Winnowing inherently provides a length threshold (the parameter k). Any repeat shorter than k will not be picked up as a fingerprint match ￼. This acts as a strong noise filter – for example, if k = 5 characters (or perhaps 2-3 tokens), then common 3-character words or token fragments will simply never appear in the fingerprint set. Additionally, you can adjust the window w to tune how many fingerprints you get; a larger window means fewer fingerprints (since only the minimum in a bigger window is kept), thus more aggressive down-sampling. There is also an implicit frequency threshold step: after generating fingerprints, you can count how often each hash occurs. Any fingerprint that occurs only once can be dropped (it doesn’t indicate repetition). The ones that occur 2 or more times point to repeated content. In practice, one might index the fingerprints by their hash and position, then group identical hashes to find candidate repeats. There is no concept of dominance or containment in the fingerprint stage because we aren’t enumerating all substrings – we only have representative k-grams. However, if multiple fingerprints point to overlapping repeats, we will merge them when extending. For example, if a very long region is repeated, it will likely produce several fingerprint matches (different segments of it). In the extension step, those will coalesce into one large repeated span. We must be careful to not double-count overlapping seeds; typically, one would merge or skip seeds that fall inside a region already identified as part of a longer repeat. The winnowing paper also discusses selecting fingerprints such that they are position-independent and robust to noise ￼ – for our scenario (exact repeats within one doc), position independence isn’t a major concern, but it means even if the repeated segments occur in different parts of the file, we catch them as long as content matches.

Performance: Fingerprinting is extremely fast and lightweight. It requires a single pass through the text to compute rolling hashes of length k substrings (O(n) time), and a second pass to slide the window and select minima (another O(n)). The memory overhead is minimal – storing a few integers for hashes and the list of selected fingerprints (which is at most O(n/w)). For instance, if w = 4 and k = 3, then roughly 1 in 4 k-grams will be kept, so at most ~0.25*n fingerprints; often even fewer, because many windows overlap. Winnowing was designed for efficiency in comparing large files, so it’s ideal for our scale. In a JavaScript context, computing hashes with a rolling scheme (like Rabin-Karp) is straightforward and fast. The critical point is that winnowing scales linearly and has low constant costs, making it suitable for multi-MB inputs. The heavy lifting of comparing all substrings is avoided – we only compare hashes. After obtaining fingerprints, the extension step (to find actual repeat spans) is somewhat heavier, because for each matching fingerprint we might do a character-by-character comparison outward to see how far the match extends. But since fingerprints already guarantee a match of length k, these comparisons usually remain linear in total work (each character of the text is part of at most a couple of extensions before mismatches stop them). In the worst case of extremely repetitive input (like a thousand identical lines), many fingerprints will match many positions, and a naive extension could become quadratic. In practice, one would mitigate this by only considering pairs of positions from the same fingerprint group and perhaps limiting extension when too many overlaps occur. Even then, fingerprinting tends to handle high repetition better because it does not explicitly list all repeated substrings: it will identify a large region once (maybe by a few seeds) rather than enumerate every sub-repeat of it. This output-sensitivity is a big advantage – the number of fingerprint matches is much smaller than the number of all substrings. Overall, winnowing/fingerprinting scales best in JS out of the approaches, at the cost of completeness. It’s a heuristic that trades a bit of recall (and exactness) for speed. But it can be combined with exact methods: e.g., use fingerprints to find candidate areas, then use a suffix structure on those areas for exact results.

Repeat Types and Their Impact on Template Clarity

When enumerating repeats, we have different definitions of what constitutes a “repeat pattern.” The main categories are: maximal repeats, supermaximal repeats, and closed repeats ￼. These definitions influence how many patterns we get and how useful they are as template anchors:
	•	Maximal Repeats: A substring is a maximal repeat if it occurs at least twice and you cannot extend it by one character in either direction without losing an occurrence ￼. In other words, it is a repeat that has different characters either to its left or right (or reaches string boundary) in at least two of its occurrences. Maximal repeats are essentially the raw material most algorithms initially find (suffix tree internal nodes correspond to maximal repeats by this definition). Maximal repeats include a lot of patterns – for example, in a log, the string "User " might be a maximal repeat (because after "User " comes different usernames in different occurrences, so you can’t extend the space after “User” consistently). Similarly, "logged in from " might be maximal if what comes after diverges (an IP address or location). Maximal repeats give fine-grained anchors, but many are fragments of larger structures. They ensure no single-character extension is common to all occurrences, yet a maximal repeat could be wholly contained in a larger repeat if the larger one differs at some other position not at the immediate boundary. Thus, listing all maximal repeats can still include redundant info.
	•	Supermaximal Repeats: A supermaximal repeat is a maximal repeat that is not a substring of any longer repeat with the same frequency ￼. Equivalently, it’s a repeat that stands on its own as the longest common substring for that set of occurrences. If you have a maximal repeat that can be found as part of a bigger repeat (with no loss of any occurrence), then it’s not supermaximal – the bigger one is more “informative.” For example, consider a legal document where the phrase “the provisions of Section 5 shall apply” appears in one place and “the provisions of Section 7 shall apply” in another. The substring “the provisions of Section “ is common to both (maximal repeat until the number differs). But that substring is also part of a longer repeat “the provisions of Section __ shall apply” if we consider the whole phrase except the number; however, the entire phrase including the number is not identical, so actually the longest common literal substring might stop before the number or resume after. Let’s use a clearer example: in the phrase ” woodchuck chuck”, we have maximal repeats like " wood" and "chuck" (they appear multiple times). " wood" appears twice and cannot be extended to the right (because one occurrence is followed by "chuck" and one by something else), and to the left it has different contexts ("many" vs start of string) ￼. " wo" was also a maximal repeat (it appears inside " wood" occurrences, and extending one more letter to " woo" would fail for one occurrence) but " wo" is contained in " wood" which appears with the same frequency. Therefore, " wo" is not supermaximal ￼. The supermaximals are " wood" and "chuck" in that example ￼. For template induction, supermaximal repeats tend to correspond to more complete phrases. In log files, supermaximal repeats often capture whole message skeleton segments. For instance, if every log line starts with "ERROR Code: " followed by a number, then "ERROR Code: " (including the trailing space) might be a supermaximal repeat – it occurs in all those lines and any attempt to extend further right (into the code number) fails across all occurrences, and there’s no longer repeat containing it with the same frequency (because including any part of the number would lower the frequency or differ per line). A shorter piece like "ERROR " might also be maximal, but since it is part of "ERROR Code: " which has the same frequency (assuming every line that has "ERROR " continues with "Code: "), the shorter "ERROR " would not be supermaximal. Thus, supermaximal filtering yields the largest stable chunks of text that repeat in exactly the same set of places. These are highly interpretable as candidate template literals (they often include surrounding punctuation or spaces that delineate a field). They significantly reduce pattern explosion relative to all maximal repeats.
	•	Closed Repeats: The notion of closed repeats is very similar to supermaximal, defined in frequent pattern mining terms: a repeat is closed if no strict superstring of it has the same occurrence set ￼. This is effectively the same criterion as supermaximal, but applied in both left and right extension directions. In string terms, a closed repeat is a substring that is maximal in the sense above (cannot extend without losing occurrences) and additionally no longer substring shares all the same occurrences. Most literature treats “closed repeats” as synonymous with “supermaximal repeats” in exact pattern mining (recent work formalizes closed repeats and shows they are at most O(n log n) in number) ￼ ￼. For our purposes, we can treat closed = supermaximal (both give the minimal set of longest patterns). One minor nuance: it’s possible to consider left-closed or right-closed repeats separately (a substring that can’t be extended on one side). But the fully closed repeat requires both sides – which is essentially the maximal repeat condition – plus the uniqueness of the occurrence set. So in practice, “maximal + not contained in a same-frequency longer repeat” is the test ￼. Closed repeats are the most consolidated output: they remove any redundancy where a shorter pattern is always part of a longer one in the same places. They generally produce the cleanest skeleton pieces.

Impact on Template Clarity: Choosing which type of repeat to output has a big effect on the clarity of the inferred templates:
	•	Using Maximal Repeats: If we output all maximal repeats above some threshold, we ensure we don’t miss anything, but many reported anchors will be overlapping or nested. For example, a source code file might have a function definition repeated with slight variations. Maximal repeats might include the function signature up to the point of variation, and also separately include parts of the body if they repeat, etc. You might end up with multiple anchors per template that overlap. In a log file, maximal repeats might give you both "Error " and "Error Code: " as separate anchors, where the latter is actually more informative. Similarly, it might list "file not found at " and "file not found at /" separately if one occurrence had a different following char. This can lead to confusion in assembling templates: the algorithm would have to decide which of these overlapping anchors to use. Maximal repeats are useful internally (they are the building blocks), but presenting them directly as template skeletons can require heavy post-processing.
	•	Using Closed/Supermaximal Repeats: This yields far fewer anchors, each of which is a maximally informative literal. In logs, this often corresponds to entire constant parts of the message. For instance, consider log lines:

User alice logged in from 10.0.0.1  
User bob logged in from 10.0.0.2

Maximal repeats here include "User ", " logged in from 10.0.0." (with the final dot, as both lines share 10.0.0.), and even "0.0.0." or " logged in from " separately might appear. The closed repeats would be more specific: likely "User " (if nothing longer covers all occurrences of that) and " logged in from 10.0.0." as separate anchors. "User " might actually be extendable rightwards by one character in both lines ("User a" vs "User b" differ at that char, so “User “ is maximal and not extendable; it appears in exactly those two places with no longer repeat covering both, so it is closed). " logged in from 10.0.0." is closed (cannot extend further because the next character differs (1 vs 2), and any shorter prefix like " logged in from 10.0.0" is extendable by the dot so not maximal). These closed repeats align well with what a human might identify as the template literal parts: “User “ at the start, and ” logged in from 10.0.0.” as the middle chunk. The variable parts would then be the username and the last octet of the IP. If we had stuck with just maximal repeats without containment pruning, we might also list " logged in from 10.0.0" (no dot) or "0.0.0." as separate anchors, which are less clear. Closed repeats give the cleanest segmentation of the line. For legal text, closed repeats correspond to boilerplate phrases in full. E.g., if a contract repeatedly says “Party A shall indemnify Party B for any losses…” with variations in names, the closed repeat could be the entire phrase including “shall indemnify” etc., stopping just before the varying party name. If some shorter phrase like “shall indemnify” appears everywhere inside a larger clause, the closed pattern might be the whole clause. This results in anchors that are complete legal clauses or sub-clauses, which are quite interpretable as template skeletons. For source code, closed repeats will find exact code clones (if a block of code is copy-pasted unchanged, that whole block is a closed repeat). If code is repeated with consistent differences (e.g., same logic but different variable names), exact closed repeats might only capture smaller pieces (like keywords or punctuation sequences that remain identical). This is where adding a step of parameterization can help – by replacing variable names with a placeholder, the code blocks become literally identical and thus emerge as closed repeats. In fact, Baker’s algorithm for parameterized duplication detection does exactly this: normalize identifiers so that “same structure, different atoms” becomes a verbatim repeat ￼. Using such a heuristic on code can greatly improve repeat detection (increasing recall of meaningful patterns) at the cost of potentially merging things that aren’t truly the same role. But as a heuristic, it can turn near-duplicates into exact duplicates that the closed repeat finder can catch.

In summary, maximal repeats are exhaustive but can overwhelm the system with fragments; closed/supermaximal repeats reduce redundancy and highlight the largest stable anchors, which usually correspond to clearer template skeleton parts. For Wring’s goal of interpretability, leaning toward closed repeats is beneficial: it yields a smaller, high-signal candidate set from which to attempt stitching into templates. Empirically, this means, for example, on log data we would get one anchor covering the constant part of each log message type (rather than many sub-anchors); on legal text we get full repetitive clauses; on code, we get either entire clone blocks or, if code differs too much, at least common snippets like function signatures or boilerplate lines (which might then be combined). One must be cautious that if the data has nearly identical repeats with one extra constant character in one occurrence, closed repeat definition will exclude the shorter pattern that appears in more places. But that shorter pattern might still be useful as an anchor across those other places. In such cases, we might consider allowing patterns that are almost closed (or simply ensure our grouping algorithm later can handle that one extra difference as a slot). Generally, though, focusing on closed repeats maximizes template clarity and minimizes explosion.

Candidate Control and Pruning Strategies

Beyond the choice of algorithm and repeat type, practical template induction relies on controlling the output via thresholds and pruning, to achieve usable results. All approaches require some tuning to avoid an unmanageably large set of candidates:
	•	Frequency Thresholds: Setting a minimum frequency (occurrence count) for repeats can dramatically cut down the output. Many substrings may repeat only twice (the minimum to qualify as a repeat), and not all of those are interesting (e.g. two occurrences of a common short word might be coincidental). By requiring, say, at least 3 or 4 occurrences, we focus on highly redundant patterns that are likely structural. This improves precision (high-frequency patterns are often boilerplate or format strings) at the risk of missing legitimate two-occurrence templates. In some domains, a template might only appear twice (and that could still be worth extracting, e.g. a unique section that is repeated once). So the threshold should be chosen with domain knowledge. Logs often have many lines of the same form (frequency could be high), whereas a legal contract might have a clause repeated exactly twice. One approach is to set the threshold to 2 (to catch all repeats) but then rank or score patterns by frequency, prioritizing higher ones for further analysis, effectively ignoring the tail of low-frequency ones unless needed.
	•	Length Thresholds: As mentioned, a minimum length for repeated substrings (in characters or tokens) filters out the numerous trivial repeats (like “a “ or “; ” or common short words “the”, “if”, etc.). For example, one might require a repeat of at least 4 characters or better, use a token-based length (e.g. at least 1 whole token or 2 tokens). Tokenization can help here: ignoring repeats that consist solely of a stop-word or punctuation token. The risk is if a domain’s meaningful anchor is short (e.g. maybe the word “IF” repeated in some programming context meaning an important keyword), but usually those can be discovered as part of larger patterns anyway. Winnowing inherently uses a length threshold k. For suffix-based methods, you can apply the threshold either during enumeration (skip emitting patterns shorter than L) or afterward (filter them out). Early filtering can save time in traversals, but one must be careful not to prune too aggressively in a tree traversal (since a short repeat might be part of a longer one – though if we only care about output, that’s fine). In practice, a threshold like 3 or 4 characters (or equivalent tokens) is very common to remove noise ￼.
	•	Containment Pruning (Dominance): We have touched on closed patterns – the idea of removing any repeat that doesn’t add new occurrence coverage because it’s contained in a larger repeat that appears in all the same places. Enforcing this greatly reduces duplication in the output list. It means we prefer the broadest context for each cluster of occurrences. This is especially important when later forming templates: we usually want the largest literal skeleton and then treat whatever interrupts it as a slot. If smaller pieces that never appear independently are still listed, they could mislead the template construction by suggesting splits where none are needed. For example, if "file not found at /" is a closed repeat in logs, we don’t need to separately consider "file not found at " as an anchor – it never occurs without the “/” following in those instances. Containment pruning would drop the latter. Algorithms implement this by either merging (suffix automaton’s approach) or post-filtering (comparing occurrence lists). The cost of checking dominance is manageable if done smartly (sorting patterns by occurrence lists or using a suffix structure to imply the relation). Since our focus is single-document, occurrence lists can be large but manageable (worst-case, a repeat occurs at thousands of positions; comparing sets can be expensive if done pairwise across many patterns). However, if we generate closed repeats directly (via automaton or by tracking in tree traversal), we avoid heavy post-processing.
	•	Output-Sensitive Algorithms: The notion of output-sensitive complexity means the runtime is proportional to n plus the size of the output. If the output (number of reported patterns) is huge, the algorithm will inherently take long. In template induction, we want to manage output size as much as possible not just for efficiency, but for the next stages of processing. If we feed thousands of candidate anchors into the template stitching stage, it might become intractable to consider all combinations. So controlling output is both a performance and a clarity concern. Setting thresholds and using closed repeats are ways to reduce output size. Another trick is to limit overlap: for instance, if a repeat is very similar to another (e.g. one occurs at positions that mostly overlap with the other’s positions), perhaps we only need one of them. This ventures into pattern set optimization, which is complex (almost like hitting the compression objective early). But simple rules like “prefer longer repeats over shorter overlapping ones” already help.
	•	Dominance vs. Frequency trade-off: One interesting scenario is when a shorter pattern appears in all occurrences of a longer pattern plus elsewhere. Then the shorter has higher frequency and wouldn’t be pruned as closed (because the occurrence sets differ). For example, in code, a keyword for ( might appear in 10 places, and the longer pattern for(i=0; i<n; i++) appears in 3 of those places. The shorter for ( is a valid repeat on its own (with freq 10, occurrence set being all loops), and the longer is closed for the subset (freq 3). Both would appear in a closed-repeat list because their occurrence sets aren’t identical. In template induction, we might actually use both: for ( could be part of a more generic template, whereas the specific loop structure is a template for those 3 instances. This suggests sometimes we want all closed repeats, not just the global longest. But that can reintroduce a lot of patterns. A strategy could be to first use high thresholds to get big obvious skeletons, then possibly relax thresholds if needed to capture smaller patterns for leftover parts of the document (the “residual”). Wring’s goal is also compression, so ultimately many of those repeats might be used if they help compress the text. But initially, focusing on the top-level structure (largest repeats) yields the most interpretable templates.

In practice, combining these controls is key. For instance, one recipe might be: collect all repeats of length ≥8 characters that occur ≥3 times, then filter those down to closed repeats. This might yield a few dozen strong candidates even from a large text, which is manageable. If the document is extremely repetitive (say a log with 1000 identical lines, or a codebase with lots of boilerplate), even closed repeats could number in the hundreds (each piece of boilerplate might come out separately). In such cases, additional heuristics might be needed (for example, group repeats by proximity or only take the longest in each region of text, etc.). But generally, applying length and frequency thresholds plus closedness gives a robust, practically usable set of anchors.

Performance and Scalability in JS/WASM

When implementing these techniques in a JavaScript or WebAssembly context (the target being in-browser analysis), we have to consider both raw speed and memory constraints:
	•	Suffix Array+LCP: Likely needs a WASM implementation for speed on large inputs. There are known suffix array construction algorithms (like DC3 / skew or SA-IS) that run in linear or near-linear time, but implementing them from scratch is non-trivial. Using a well-optimized library in C/C++ and compiling to WASM is a good route. Memory usage, as noted, can be tens of megabytes for a 10MB string, which might be acceptable on desktop but could be an issue on memory-constrained devices. Also, transferring a 10MB string to WASM memory and building a 80MB index might push browser limits if not careful. On moderate sizes (say up to 1–2MB), SA+LCP could potentially be done in pure JS with a suffix-sort algorithm, but performance might degrade quickly beyond that. One might also consider a suffix array suffix automaton hybrid: build a suffix array for a subset of suffixes or some coarse sampling, though that’s not standard. More practically, if the text is very large but highly repetitive, one could segment it (e.g. by paragraphs or sections) and build smaller indices, though then cross-boundary repeats are missed.
	•	Suffix Tree: Directly building a suffix tree in JS is not recommended beyond small sizes. A WASM Ukkonen implementation could handle a few MB, but memory blow-up to hundreds of MB could happen at 10MB worst-case. An alternative is to build a compressed suffix tree / suffix automaton or use an FM-index (compressed suffix array) which uses less space. However, those add complexity and are probably overkill for a single document scenario. Given that suffix tree offers no big advantage over suffix array for just finding repeats (and SA+LCP is simpler to memory-manage), it’s unlikely to be the top choice for Wring. If deeper structural analysis (like parsing hierarchical patterns) was needed, a suffix tree or DAWG could help, but Wring’s pipeline seems to lean towards simpler anchor stitching.
	•	Suffix Automaton: As a linear algorithm with low overhead, a suffix automaton is an attractive choice for JS. There are known JavaScript implementations in competitive programming circles for strings up to a few million characters, which suggests it’s feasible. The automaton will allocate an object (or struct in WASM) for each state and each transition. If using an array of transitions (size = alphabet), that’s state_count * alphabet_size pointers – too much if alphabet is large (e.g. Unicode). Using a dictionary for transitions (only store those present) is more memory-efficient for typical text (where each state might have a handful of transitions on average). The initial state might have transitions for many distinct characters, but deeper states often have 1 or 2. The overall number of transitions equals the number of different substrings minus 1, which in worst case is about n(n+1)/2 (all substrings distinct, which is massive) but the automaton compresses that to at most 2n transitions. Typically, English text or code has lots of repeated substrings so it compresses even more. For example, a 1MB text might end up with ~1.5 million states and slightly more transitions – that’s still large for JS to handle, but with WASM and careful memory (like preallocating arrays for states) it could be done. On a 10MB text, the worst-case 20 million states is definitely not possible in JS, but average-case might be far fewer states if the text has structure. We should assume near worst-case though, and thus consider suffix automaton viable up to a few MB in practice, unless using WASM with a lot of memory reserved. Another approach: one can chunk the input and build partial automata, but merging them for repeats that span chunks is complicated. Alternatively, use automaton for moderate size and revert to fingerprinting for very large size to reduce input (for instance, use fingerprinting to identify maybe duplicate blocks and cut down the text by replacing large repeated blocks with a token, thereby reducing length).
	•	Fingerprinting: This is by far the safest for scaling. It only uses linear time and a small multiple of the input size in memory (for storing hashes and fingerprints). For example, hashing 10 million characters with a rolling hash (like a 64-bit polynomial) is quite fast in C (a few hundred milliseconds perhaps); in JS it might be slower, but still likely linear in 10 million (which might be ~0.01 seconds per million chars if optimized, so ~0.1s for 10M, or maybe more realistically 0.5-1s). With WASM, it can be extremely fast. The number of fingerprints stored might be, say, if window = 20, roughly 5% of all k-grams (as a rule of thumb from the winnowing paper) – so maybe 0.5 million fingerprints for a 10M doc. Storing those (as 64-bit hash + position) would be ~12 bytes each, so ~6MB of data, which is fine. The memory and speed are very manageable. The challenge with fingerprinting is the extra logic to extend seeds to full repeats. But that can be done on the fly: for each fingerprint hash that occurs multiple times, take each pair (or cluster) of positions and try to extend. There could be many pairwise comparisons if a fingerprint is common (e.g. a common word hashed and chosen might appear 50 times, leading to 1225 pairs). However, we can optimize by only extending the nearest neighbor occurrences or doing a multi-sequence alignment if needed. In practice, you might limit to finding maximal matches via suffix array anyway – interestingly, one hybrid could be: use fingerprint matches to identify candidate pairs, then use a suffix array or suffix automaton on those specific positions or substrings. For instance, if fingerprint says positions 1000 and 5000 share a 20-char match, you could start from there and extend backwards and forwards by direct comparison (which is essentially what a suffix array could tell you quickly, but you can just use the original text for brute-force extension since typically the differences will cut it short before too many characters). The cost of extension is bounded by n for each distinct repeated region (since each character will only be extended until a mismatch once per region). So overall complexity remains linear or linearithmic in output size.
	•	JS/WASM Trade-offs: A likely outcome is to use fingerprinting as a first pass for very large inputs to reduce the problem size, and use suffix automaton or SA+LCP on smaller inputs or the identified regions for precision. For example, if a 8MB document is mostly unique but has some repeated sections, fingerprinting will pinpoint those sections quickly. We could then extract those sections and run a suffix automaton on just those concatenated or treat each repeated section separately. Another heuristic could be hierarchical: use a coarse window (like fingerprint whole lines or paragraphs as units to find duplicate lines/paragraphs), which is essentially what log template extractors often do (clustering identical or similar lines). Then within each cluster or within each repeated block, use suffix methods to get the detailed substructure repeats.
	•	Output Post-processing Complexity: After getting candidate repeats, we must form templates (stitch anchors and identify slots). That process can be expensive if too many candidates. This is why controlling the number via the above methods is crucial. In terms of complexity, if we ended up with M candidate anchors, a naive pairwise combination search for co-occurring anchors would be O(M^2) which is not feasible if M is large. But if we keep M small (maybe in the low hundreds at most), we can do smarter template formation (like the offset joining approach described in the README ￼). The chosen repeat detection strategy should thus aim to minimize M while retaining the true structural pieces.

Given all these, a recommended strategy emerges:

Hybrid Approach: Use winnowing fingerprinting as a fast pre-processing to detect obvious large repeats and duplicate blocks, then apply an exact method (suffix automaton or SA) on the text to enumerate repeats, but guided by the fingerprint results for pruning. Concretely, one could:
	1.	Fingerprint the document with a moderate k (e.g. 8 or 10 characters, or 2-3 tokens) to identify all regions that appear at least twice. This can produce a set of candidate intervals in the text that are likely repetitive.
	2.	Perhaps mark those regions or even replace each unique repeated region with a marker (temporarily compressing the document by collapsing large verbatim repeats).
	3.	Run a suffix automaton on the transformed document (which is shorter if large repeats were collapsed) to find smaller-scale repeats (like recurring phrases that fingerprinting might miss if below window length).
	4.	Take the automaton’s results and filter to closed repeats, then combine with the known large repeats (from step 1) – which are inherently closed by virtue of being large blocks.
	5.	The combined anchor set can then feed the template stitcher.

Another hybrid approach is Baker’s parameterization as mentioned: if we know certain tokens (like numbers, dates, variable names) are likely to vary, we can replace them with wildcards up front to increase the literal matches ￼. This boosts recall of structural patterns. For example, in logs, replacing all numbers with <NUM> would make "Error 404: ..." and "Error 500: ..." literally share "Error <NUM>: ...", which a repeat finder would catch as a long repeat, rather than just "Error " and ": ...". The downside is you might merge patterns that aren’t truly the same format (if numbers appear in different contexts). A careful approach might mine repeats on both the raw text and a normalized text in parallel, then intersect the results. The README hints at such hybrid mining on “skeleton and value streams” ￼.

JS/WASM scaling summary: Suffix automaton likely offers the best balance for medium-sized inputs (fast, linear, and easier to implement than suffix tree) and naturally gives closed patterns. SA+LCP is equivalent in result but requires heavy sorting or suffix-index construction – feasible with WASM but more memory. Winnowing is excellent for large scale and can be combined as a filter. In a browser scenario, one might implement multiple fallback strategies: try automaton for up to, say, 1–2 million characters; if input is larger, either stream it in pieces or switch to a fingerprint-based approach first to whittle it down.

Comparison Summary of Methods

To crystallize the differences, the table below compares the four approaches on key dimensions:

Primitive	Candidate Quality & Pattern Output	Control Mechanisms	Performance (Time & Memory)
Suffix Array + LCP	Enumerates all repeated substrings (typically as maximal repeats). Can yield very fine-grained anchors, including many small or partial patterns. Without filtering, produces a flood of candidates (many trivial). With filtering, yields exact patterns with occurrence lists. High recall (no true repeat missed), but lower precision until pruning.	Easy to enforce min length and frequency during LCP scan. Can identify supermaximal/closed repeats by checking if a longer LCP interval has same frequency ￼. Post-processing to remove patterns contained in longer ones (dominance pruning) is straightforward. Thresholds essential to avoid tiny common substrings.	Suffix array construction is O(n) or O(n log n) (WASM recommended for speed). Needs ~8n bytes space for SA+LCP arrays (e.g. tens of MB for a 10MB text) ￼. Once built, enumeration is linear in output size. Scales to a few MB in-browser with optimization; at upper sizes, memory/time become concerns.
Suffix Tree	Also finds all repeats (as internal nodes = maximal repeats). Captures hierarchical relationships (repeats within repeats). Output before pruning is exhaustive and can include redundant subpatterns. Tends to produce interpretable full phrases once filtered to supermaximal, since shorter ones on the same branch are subsumed. Without containment pruning, many overlapping fragments appear.	Frequency threshold can be applied by ignoring subtrees with < k leaves. Naturally represents maximal repeats; no duplicate listing of identical occurrences. Must prune shorter repeats that share occurrences with longer ones (though tree structure already merges a lot). Can extract closed repeats by selecting deepest node for each set of leaves.	Linear time construction in theory ￼, but very high constant factors. Memory >> n (pointers, objects) ￼ – can be 5–10× text size or more. Not practical in pure JS for large n; a compact WASM implementation needed. Traversal cost proportional to number of repeats (output size). Suitable for small documents or for theoretical completeness; less so for 10MB scale in browser.
Suffix Automaton	Produces a minimal DFA of all substrings. Merges repeating patterns that share occurrences, inherently removing duplicates. Each state corresponds to a set of repeats with identical occurrence sets – the longest of which is a closed repeat ￼. Yields high-quality candidates: fewer, more inclusive anchors (closed patterns by default). Still needs length filtering to drop very short merges. Ensures no purely redundant patterns in output (each reported substring has a unique occurrence signature).	Can compute occurrence counts for each state and filter by frequency easily. Minimum length filtering via state’s max substring length. Naturally implements dominance pruning: shorter repeats that never occur outside a longer context collapse into same state as the longer repeat ￼. Thus, output one representative (longest) per state gives closed repeats. Can still output all maximal repeats by listing all states, but usually we choose the longest per state to maximize interpretability.	Builds in O(n) time and O(n) memory (at most ~2n states) ￼. Practically faster than suffix tree and similar order of memory, but worst-case can still be large (~2 * 10MB = 20 million states in theory). In typical text, much fewer states due to merging. Pure JS implementation feasible for ~1–2M chars; for ~10M, WASM or chunking recommended. Access patterns are simple (mostly sequential as we extend the automaton). Overall very fast, linear scaling ￼. Retrieval of actual substrings requires storing an index for each state (small overhead).
Winnowing / Fingerprint	Outputs only selected substrings of fixed length k as anchors/seeds. Greatly reduces candidate count – focuses on longer patterns only. Misses repeats shorter than k by design (noise is filtered out) ￼. Yields high-signal anchors but they may represent fragments of a larger repeat (requires extension to get full pattern). Candidate quality is high in precision (most fingerprint matches correspond to real repeats ≥k length), but recall is tuned by parameters (some edge-case repeats might slip through if they don’t share an identical k-gram due to small variations).	Built-in length threshold = k (any repeat ≥ k guaranteed to have a fingerprint) ￼. Window parameter w controls how many fingerprints (trade-off between catching more repeats vs. output size). After hashing, can drop unique fingerprints (only keep those that repeat). No explicit containment check, but extension step will naturally merge overlapping seeds into one larger match. Can also treat known “boilerplate” patterns as ignore-list by hashing them with a special marker (as MOSS does) ￼.	Very efficient: O(n) time hashing, O(n) time winnowing. Memory is a few arrays of length n and storage for fingerprints (roughly n/w entries). Scales to very large texts easily (used in tools comparing multi-MB documents). In JS, rolling hash and min-finding are lightweight; for 10MB text this runs in seconds or less. Extension of matches is also linear-ish in practice. Excellent scalability, low memory (e.g. ~5–10% of n for fingerprints). Completely feasible in-browser even at upper document sizes.

Table: Comparison of repeat detection primitives for template induction (candidate pattern quality, control options, and performance considerations).

Selecting the Best Strategy for Wring

Considering the above, the optimal strategy for Wring’s single-document template induction likely involves a combination of these approaches to balance completeness, precision, and efficiency:
	•	Primary engine for repeats: A suffix automaton with closed-repeat output is very attractive for Wring. It yields a condensed set of candidate anchors (each anchor being a maximal literal segment that recurs). This directly addresses the need for high-signal candidates without pattern explosion ￼ ￼. The automaton’s ability to naturally compress redundant patterns means we get interpretable pieces (e.g. full phrases rather than sub-fragments) which improves the “literal skeleton clarity.” It also offers easy controls (we can decide not to output states corresponding to short or low-frequency repeats). Implementing this in C++ and compiling to WASM could provide both speed and memory efficiency to handle multi-MB inputs. The result would be a set of closed repeats, which, as discussed, should correspond well to template skeleton components.
	•	Supplement with fingerprinting for scale: For very large or particularly structured inputs (e.g. log files with many repeated blocks), fingerprinting can act as a pre-filter. It can quickly identify large duplicated regions or very frequent patterns without building full suffix structures. For example, if the document contains 1000 nearly identical log lines, a suffix automaton will handle it but produce many states (though it will merge a lot too). Fingerprinting could immediately tell us “these lines are all the same format” by hashing each line or chunks thereof. In such cases, one could cluster identical lines and treat each cluster as one template instance (this is more like classic log parsing algorithms). Even within the automaton approach, one might use fingerprints to cut down the input or guide the automaton: e.g. skip adding parts that are extremely repetitive if already processed, etc. A concrete heuristic: use winnowing to find all pairs of positions with a common k-gram, group those into candidate repeat regions, and feed those regions to a suffix automaton for detailed analysis. This two-step approach might increase recall for structured but not verbatim-same patterns and keep the automaton’s workload focused.
	•	Hybrid parameterization for recall: As an optional heuristic, especially for source code or data with predictable variable tokens, apply Baker-style parameterization ￼. For instance, one could run the repeat detection on a version of the text where all numbers are replaced by <NUM> and all identifiers by <ID>, in parallel with the raw text analysis. The normalized run would find patterns that were identical up to those placeholders. The raw run finds exact repeats. The intersection or union of these can yield more complete templates – placeholders hint at where slots are. For example, in code, this could reveal a loop pattern even if variable names differ (because after normalization they match exactly). One must verify that the structures align with actual segments in raw text (to avoid false merging). This hybrid can boost detection of high-level templates that are otherwise split across multiple smaller exact anchors.
	•	Pragmatic thresholds: Based on Wring’s emphasis on interpretability over maximal compression ￼ ￼, we would use relatively aggressive filtering: e.g. ignore repeats shorter than a certain length (to eliminate noise) and possibly ignore very low-frequency repeats initially. The idea is to get a manageable set of strong candidates that clearly correspond to template skeletons. The template induction pipeline can then attempt to stitch these into larger templates (by checking which ones co-occur and aligning their instances). We can always relax thresholds if we find that some needed template piece was missed (for instance, a two-occurrence repeat that is key for a smaller template). But starting with a lean candidate list is key to avoid blow-up in the stitching phase.
	•	JS/WASM scaling: Implement the core repeat mining in native code (WASM) if possible, especially for suffix array or automaton. But if limited to JS, prefer the suffix automaton (which can be done with simple arrays/objects) or fingerprinting. In the worst case, a fallback could be to do a simpler algorithm like an LZ-based scan (not discussed above, but one could use LZ77 compression to find repeats greedily). However, that can miss some patterns and doesn’t enumerate nicely. Among the given primitives, the automaton gives a nice trade-off. If memory is a concern, one could limit the automaton’s state creation by, say, breaking the input into segments of 1MB each, building automata, and merging results superficially (though merging automata is complex, instead maybe gather repeats per segment and then also look for repeats that span segment boundaries via fingerprint). For logs, segmentation by line is natural: one could fingerprint line signatures to group lines, then within each group, use suffix automaton to find repeated tokens. But since Wring is supposed to handle arbitrary text too, a general solution is needed.

Conclusion: To achieve high-fidelity template induction, we want high-signal anchors that correspond to real structural constants in the document. The analysis suggests that focusing on closed (supermaximal) repeats is crucial for clarity – they give the literal skeleton with minimal redundancy. Among algorithms, the suffix automaton stands out for producing such repeats efficiently and in a compact form ￼. Suffix arrays are an alternative if a robust implementation is available; they would yield similar results (after filtering) but with potentially more overhead. Suffix trees are powerful but less practical here due to their resource usage. Winnowing/fingerprinting is extremely useful as a speed booster and noise filter, and it can be integrated as a pre-processing step to handle very large inputs or to guide the exact algorithms to the right spots ￼. A hybrid approach – fingerprint for seeding large-scale structure, suffix automaton (or SA) for finer exact enumeration – likely offers the best of both: precision, recall, and scalability. By tuning thresholds (length/freq) and employing dominance pruning, we ensure the output patterns are manageable and meaningful, avoiding the dreaded pattern explosion while still covering the repetitive structure needed to build correct templates. This approach will help Wring discover templates that compress the document well and remain human-interpretable, achieving the project’s goals of round-trip reconstruction with interpretable templates ￼ ￼.

Sources: The evaluation above references concepts and definitions from stringology (suffix structures and repeats) and leverages insights from related tools. Key definitions of repeat types (maximal, supermaximal, closed) are drawn from the Wring project notes ￼ and string algorithm literature ￼. Performance characteristics of suffix automata and suffix trees are based on theoretical results ￼ ￼ and practical considerations for implementation. The winnowing technique is described as per the original algorithm guaranteeing detection thresholds ￼. These inform the comparative conclusions on which combination of methods best balances the needs of template induction in a browser-based setting.