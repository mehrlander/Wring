Tokenization and Typing Strategies for Single-Document Template Induction (Wring Project)

Single-document template induction (as in the Wring project) aims to extract a set of parameterized templates – fixed literal skeletons with slots for variable content – that can exactly reconstruct the original document ￼. The challenge is to optimize for compression (following a Minimum Description Length (MDL)-style objective) while preserving interpretability of the templates ￼ ￼. A crucial design factor is how the input text is represented before mining patterns: the tokenization granularity and typing (normalization) strategy applied. These choices directly affect which repeated structures are discoverable, how slots are identified, and ultimately what templates yield the best trade-off between compression and human-readable structure ￼. In this report, we analyze different tokenization and typing approaches for template induction, compare their impact on pattern discovery and MDL costs, and consider practical implications for a browser-based Wring implementation. We also contrast how these strategies may play out across various document types (structured text, logs, HTML).

Token Granularity

Token granularity refers to the basic units into which the document is split before mining for repeats. The main options are: treating every character as a token (character-level), splitting into larger tokens such as words (e.g. whitespace-separated words), or using a more nuanced tokenization that keeps whitespace and punctuation as separate tokens. Each approach influences pattern frequency and interpretability differently ￼:
	•	Character-Level Tokenization: Each character is a token, so patterns are identified as repeated character sequences with no predefined token boundaries. This maximizes the opportunity to find repeats because any recurring substring is discoverable – patterns are not limited by word or token boundaries. Character granularity can reveal subtle or partial repeats that a coarser tokenization might miss ￼. For example, if a recurring format differs only in the middle of a word, a character-level search may still capture the common prefix/suffix as a repeat, whereas word-level would treat the whole word as different. However, the downside is that many discovered “patterns” may not align with meaningful linguistic units, reducing structural fidelity ￼. Character-level mining tends to produce a flood of trivial repeats (e.g. common letters or syllables repeated) – a pattern explosion that complicates downstream template formation and selection. It also risks splitting logical tokens (like words or identifiers) across template slots in unintuitive ways. In short, character tokens maximize pattern frequency but at the cost of interpretability and higher noise in candidates ￼.
	•	Word-Level Tokenization: Splitting the text on whitespace (and possibly stripping punctuation) yields word tokens. This aligns patterns with natural language units, improving interpretability of the resulting templates. Meaningful repeated phrases or sentences can be discovered as sequences of word tokens. By imposing word boundaries up front, we avoid many spurious sub-token patterns – improving structural fidelity of patterns to the text’s logical structure ￼. Fewer and more meaningful repeats will be found (e.g. whole recurring phrases rather than every common character sequence). On the other hand, strict word-level tokenization can hinder pattern discovery: any variation within a word or attached punctuation will break an otherwise repeated sequence. If a repetitive format includes, say, a variable suffix in a word (like code identifiers error1, error2), word-level tokens (error1 vs error2) appear completely different and the common prefix “error” wouldn’t register as a repeat. In general, word tokens may under-segment the text, causing the algorithm to miss patterns that do recur but not on exact word boundaries ￼. The question arises: should we accept a fixed token boundary scheme, or allow the algorithm to discover token boundaries from the repeated structure itself? ￼ This is open for exploration – a pure word-level approach might miss cross-word patterns that a data-driven (character-level) approach would reveal.
	•	Whitespace/Punctuation-Aware Tokenization: A middle ground is to tokenize in a way that preserves punctuation and spacing as separate tokens, rather than discarding or attaching them to words. Here, tokens might include words, numbers, punctuation symbols (commas, brackets, <tag> delimiters, etc.), and even runs of whitespace if relevant. This approach recognizes that punctuation often carries structural meaning (e.g. separators in lists, HTML tags, log line delimiters) and that consistent spacing/indentation can be part of a repetitive pattern. By making punctuation its own token, we can capture templates like an HTML tag sequence or a log message format more precisely (e.g. a pattern Error [ <NUM> ]: can be matched if we tokenized "Error", "[", number, "]:" separately). Punctuation-aware tokenization thus helps isolate variable content from its surrounding literal structure. It tends to improve alignment, since the template can include fixed punctuation tokens in the skeleton and leave only the content as slots. For instance, tokenizing key=value into ["key", "=", "<VAL>"] (with <VAL> as a placeholder for the value) yields a clear template key=<VAL> repeated across lines. If we had kept key=value as one token, different values would make the token unique and no repeat would be found. The drawback of this finer tokenization is a slight increase in token count (the text becomes longer in tokens, since every punctuation mark is now an item) and added complexity in implementation (defining which punctuation or letter-case changes warrant a split). There is also a risk of very short tokens (single-character tokens like =, {, }) forming trivial repeated patterns; though these are usually filtered by minimum length thresholds, they do contribute to candidate count. Overall, however, a whitespace/punct-aware stream often strikes a good balance: it retains much of the structural fidelity of word-level tokens, while capturing more pattern-bearing units (formatting symbols, etc.) that aid in discovering repetitive structure.

Imposing vs. Discovering Token Boundaries

A critical consideration is whether token boundaries should be fixed in advance or learned from the data. Imposing a tokenization (word or punct-based) provides a stable, interpretable unit of repetition, but as noted, it might miss patterns that span those boundaries ￼. The alternative is to start with a finer-grained sequence (like characters) and allow the algorithm to infer higher-level tokens from recurring sequences. For example, a character-level analysis might reveal that the string “Section 123” appears frequently, suggesting that “Section ” could be treated as a literal token and the number as a variable. In essence, token boundaries can emerge post hoc when we see which substrings repeat. This data-driven token discovery could prevent us from making premature splits that obscure structure. However, implementing such adaptive tokenization is complex – it requires detecting that a certain repeated char-sequence would be more suitably treated as a unit. It also risks forming tokens that are not traditionally “natural” (like merging a word and following space as one token if they always appear together). Wring’s design leaves this as an open question: “Should token boundaries be discovered from repeated structure rather than imposed?”, noting that character-level mining may reveal patterns missed by pre-tokenization ￼. One pragmatic approach is a hybrid: use a reasonable initial tokenization (e.g. words + punctuation) for efficiency, but allow refinement (merging or splitting tokens) if evidence from repeats suggests it would significantly improve the model. In fact, during template formation, one can optionally split tokens at specific points if it “materially improves MDL without harming interpretability.” ￼. For example, if many tokens share a common prefix, splitting that prefix off as a literal could turn a fragment of text into a repeated anchor with a slot for the remainder (improving compression). In summary, a priori tokenization simplifies the mining process and yields more interpretable units, whereas discovering boundaries from data can maximize pattern capture – in practice a combination can be used, starting with a punctuation-aware token stream and then adjusting boundaries based on repetition signals.

Comparison of Token Granularities: The table below summarizes the pros and cons of each token granularity strategy:

Granularity	Advantages	Drawbacks
Character-level	- Finds all repeated substrings, unrestricted by preset token breaks (maximizes pattern recall) ￼. - Can capture patterns that span across words or contain recurring sub-word fragments that static tokenization would miss ￼.	- Low structural fidelity: patterns may not align with meaningful units (e.g. partial words), reducing interpretability ￼. - Pattern explosion: many trivial repeats (letters, syllables) increase noise and computation load. - Harder slot boundaries: slots might begin/end mid-word without careful post-processing.
Word-level	- High interpretability: patterns consist of whole words, matching human-readable structure. - Fewer, more meaningful candidates (ignores insignificant sub-token repeats), easing pattern selection. - Smaller sequence (fewer tokens than chars), improving indexing speed and memory use.	- Misses intra-word or punctuation-bound patterns entirely – any difference within a word breaks the repeat. - Reduced pattern frequency: treating each distinct word as unique can yield fewer repeats (e.g. "error1" vs "error2" don’t match at all when tokenized as whole words). - Assumes predefined notion of word; may not suit all formats (e.g. code or IDs with no spaces).
Whitespace/punct-aware	- Captures structural symbols (punctuation, tags) as tokens, which helps detect repeated formats (e.g. punctuation sequences, HTML tags). - Isolates variable fields by surrounding them with literal tokens, improving alignment (e.g. constants like ":" or "[" can serve as anchors around slots). - Still largely interpretable: tokens correspond to recognizable text segments and delimiters.	- Slightly increases token count (each punctuation becomes a token), which can add overhead. - Requires custom tokenization rules (deciding which characters to split on, e.g. / or camelCase boundaries may need special handling). - Can introduce very short tokens (e.g. {, }) which might form low-value patterns (needs filtering).

In practice, tokenization for template induction often uses a punctuation-aware word tokenization as a baseline, given its balance of structural fidelity and pattern recall. Character-level analysis can be reserved as an auxiliary step to refine tokens or identify missed patterns, rather than as the primary representation (due to the explosion of candidates it produces).

Typing Strategy

Beyond how text is tokenized, typing (or normalization) strategies determine how literal vs variable content is represented during pattern mining. Typing refers to replacing certain tokens or sequences with generic type placeholders (like <NUM> for any number, <DATE> for any date, etc.), either before or after pattern discovery. The goal is to normalize variations that are not structurally important so that patterns emerge more clearly. However, over-normalization can obscure meaningful differences. We compare four approaches: pre-typing, post-typing, Baker-style parameterization, and hybrid strategies ￼.
	•	Pre-typing (Normalize Before Mining): In pre-typing, we pass an already normalized token stream to the pattern mining algorithm. This means identifying likely variable tokens (numbers, timestamps, GUIDs, etc.) and replacing them with a generic marker upfront (e.g., every numeric sequence replaced with a single <NUM> token) ￼. The advantage is that many strings which differ only in those variable parts become exactly identical in the input, dramatically increasing their apparent frequency. This makes it far easier for the algorithm to detect a repeated skeleton. For example, two log lines "User 123 logged in" and "User 456 logged in" become "User <NUM> logged in" in the normalized view – now the entire line is a repeat with a variable slot. Without pre-typing, the longest common subsequence might have been shorter (e.g. just "User " and " logged in" separately) because the differing number breaks the match. By increasing pattern frequency, pre-typing can lead to discovering larger templates that would otherwise be missed ￼. It also reduces the token vocabulary (e.g. all distinct numbers map to <NUM>), which can slightly ease the search complexity. The downside is loss of fidelity: the mining algorithm no longer distinguishes one number or identifier from another, so some detail is deferred until after mining. The literal skeleton of templates found in the normalized stream will lack specificity – e.g. "User <NUM> logged in" covers all users, losing the fact that different IDs were present. This can sometimes lead to over-generalization, where distinct patterns get merged because their differences were all typed away. For instance, if we also normalized <DATE> and <UUID>, two log lines with different formats might both map to "<DATE> <UUID>" structure if we aren’t careful, falsely indicating a repeat. Therefore, pre-typing must be done with informed rules: choose categories that truly represent “replaceable” values in the domain (numbers, random hashes, etc.) to avoid conflating genuinely different structures. In summary, pre-typing sacrifices some structural fidelity for higher pattern recall ￼, and tends to work best when the document contains many consistent templates distinguished only by variable data (as is common in logs and certain structured text).
	•	Post-typing (Infer Types After Mining): Post-typing takes the opposite approach: perform pattern discovery on the raw text (with full fidelity), and only after identifying a template, analyze its slots to assign them types or generalize them. In this approach, the input to the pattern mining has no prior normalization – every token is as-is – so repeats will only be found if they are exactly identical in those tokens. This preserves maximum fidelity during mining ￼: the algorithm won’t merge anything that isn’t a verbatim repeat (so it avoids the false merges that over-normalization could cause). Once a template is found, however, we can look at the set of values that filled each slot across instances and decide if they represent a type (e.g. all slot values look like integers, or all are dates). At that point we can label the slot as <NUM> or <DATE> in the template for readability and to inform the MDL cost model. The benefit of post-typing is that the templates reflect the text’s structure very faithfully – every literal in the skeleton was actually a literal that repeated exactly in those instances. The drawback is evident: many potential patterns won’t even surface in the first place because their differences prevented them from qualifying as repeats. In other words, patterns may not surface at all if values differ, since without normalization the mining process might treat each occurrence as unique ￼. This can lead to under-detection of templates, especially in domains like logs where few lines are exact matches without normalization. Post-typing thus prioritizes structural precision over recall of patterns. It might be suitable when we expect only moderately variable repetition or when false positives from over-generalizing would be especially harmful. It can also be complemented by a heuristic clustering: for example, first cluster similar texts and then apply post-typing within clusters to label slots. But purely done, post-typing means the burden is on the mining algorithm to find patterns in a sea of unique tokens.
	•	Baker-Style Parameterization (Placeholder Matching): This strategy is inspired by Brenda S. Baker’s approach to parameterized pattern matching in code clone detection ￼. The idea is to treat any differing token between two occurrences as a placeholder, so long as the overall structure is the same. In practice, one “normalizes identifiers and values to placeholders before mining, making ‘same structure, different atoms’ exactly matchable” ￼. This is like a supercharged form of pre-typing where we don’t rely on predefined token categories, but rather allow an arbitrary mapping: e.g., in one instance of a pattern a token ABC could consistently correspond to XYZ in another instance, and both would be seen as the same placeholder in the normalized representation. Baker’s parameterized matching essentially says two sequences p-match if one can be obtained from the other by a consistent one-to-one replacement of symbols (with the constraint that the mapping does not map two different symbols to the same symbol) ￼. For example, "int x = 5;" and "int y = 7;" would be considered the same structure under parameterization (map x→y, 5→7) because apart from those systematic substitutions, the sequences are identical ￼. Applying this concept, a Baker-style approach to template induction would replace every unique value or identifier with a generic placeholder symbol but in a way that distinguishes different placeholder roles. One might imagine it as assigning placeholder IDs: e.g., "Name: John, ID: 123" and "Name: Alice, ID: 456" could both map to "Name: $X, ID: $Y" (with $X and $Y as two distinct placeholder variables for that pattern). This yields the maximum generalization: if two regions have the same shape and length, they will match exactly after parameterization ￼, regardless of how many tokens differ. The benefit is discovering templates that would be invisible otherwise – it is particularly powerful for code or highly structured text where the structure is identical but literal names vary (Baker’s method was famously used to detect code clones by normalizing variables and constants ￼). It ensures we get the longest possible skeletons since all differing parts are turned into slots by default. The downsides are twofold: (1) Complexity – implementing parameterized pattern mining is more involved than standard exact matching. It typically requires building specialized data structures (e.g. parameterized suffix trees or automata) or employing algorithms that consider equivalence classes of tokens ￼. This can be heavy in terms of computation and memory, especially in JavaScript/WASM. (2) Over-generalization risk – by treating every difference as a placeholder, we might form templates that, while structurally consistent, could merge content that a human might not group. For instance, if two paragraphs have the same punctuation and word count, Baker-style would align them as the same template with many slots, even if semantically they are unrelated. In extreme cases, parameterization could find a “structure” in common between very different texts (as long as token counts and order match), which might not be a meaningful or interpretable template. Careful constraints (like only parameterizing within certain token classes, or requiring multiple occurrences to verify) are needed. Overall, Baker-style parameterization maximizes pattern frequency by abstracting all literals into placeholders, and is essentially the approach used in classical clone detectors (“abstract tokenization” in code clone context ￼). It can be very effective for domains like logs and source code where the variability is mainly in identifiers and values, but it should be applied with caution in free text.
	•	Hybrid Approaches: A hybrid strategy attempts to get the best of both worlds by mining on both the raw and normalized representations in parallel ￼. There are a few ways this could be realized. One idea is to create a “skeleton stream” from the original text by replacing certain tokens with a generic symbol (as in pre-typing or even fully parameterized), and run a pattern mining process on that in tandem with the original. The skeleton stream will surface high-level repetitive structure (since noise from differing values is reduced), while the original stream ensures that we only accept patterns that truly align in the unmodified text. In practice, the algorithm could first find candidate repeats in the normalized sequence (where patterns are longer and higher-frequency), then map those candidate occurrences back onto the original text to verify exact boundaries and collect the actual token differences. Another hybrid approach is iterative: gradually generalize tokens as you find evidence that they vary in otherwise-identical contexts. For example, a system might start with no typing; once it notices two occurrences differ by only a single token, it could mark that token position as a wildcard and re-evaluate, effectively learning a template on the fly (this is somewhat akin to how the Drain log parser works incrementally, generalizing tokens once a second occurrence indicates variability ￼). The benefit of hybrid methods is high recall (nearly as general as full pre-typing or parameterization) with some safety net for fidelity – because raw text alignment is still checked, and we don’t blindly merge structures that don’t actually match in layout. It also provides the opportunity to assign types to slots based on both prior knowledge and the observed values. However, hybrids come with higher complexity in implementation: one must maintain two representations and ensure consistent mapping between them. There is overhead in either running two mining passes or a more complicated single pass that accounts for placeholders. Still, given the importance of both pattern frequency and structural accuracy, a hybrid might be the most practical approach for Wring. For instance, Wring could “mine on both skeleton tokens and value tokens” simultaneously ￼ – effectively generating templates that are typed (skeleton) while verifying against the raw token sequences. This parallel approach could yield templates that have typed slots from the outset, combining the interpretability of a typed skeleton with the robustness of checking actual text alignment.

Comparison of Typing Strategies: The table below contrasts the strategies in terms of their mechanism and trade-offs:

Typing Strategy	Approach & Usage	Pros	Cons
Pre-typing (normalize before mining)	Replace certain token classes with generic markers prior to pattern search (e.g. <NUM> for all digits, <DATE> for dates) ￼. Common in log parsing (masking timestamps, IDs) to group similar lines ￼.	Boosts pattern frequency: Many otherwise-distinct occurrences become identical after normalization, yielding more and longer repeats ￼. Simplifies patterns: Reduces vocabulary and highlights structural text (constant parts) by removing low-level variability.	Loses fidelity: The literal skeleton no longer contains the original tokens for typed fields (all numbers look the same, etc.) ￼. Risk of merging non-analogous patterns: If typing rules are too broad (e.g. every capitalized word to <NAME>), structurally different phrases might erroneously look identical.
Post-typing (normalize after discovery)	Find exact repeats on raw text first; once templates are chosen, replace or annotate each slot value with an inferred type label (based on its content or range).	Full structural accuracy during mining: Only true verbatim repeats are considered, avoiding false pattern merges ￼. Detailed slot typing: Types can be assigned by analyzing slot values in context (e.g. all slot entries match a date regex -> slot type is Date). This can yield more accurate typing since it’s data-driven per slot.	Misses many patterns initially: Patterns that require generalization won’t be found at all, since no normalization aids the discovery ￼. Lower compression gain in first pass: Without grouping variants, the initial model may treat frequent structure with minor differences as separate, leading to suboptimal template set until after typing is applied (no opportunity to compress across those differences early).
Baker-style Parameterization (placeholder matching during mining)	Permit tokens to match even if they are different, by treating differing tokens as placeholders as long as the sequence of placeholders and literals aligns. In effect, any two segments of equal length that align in literal positions can be considered the same pattern with slots in the differing positions ￼. Implemented via specialized suffix structures or by encoding tokens into a normalized form per occurrence. Used in code clone detectors by abstracting identifiers ￼.	Maximally general pattern discovery: Finds structure without being hindered by any consistent differences – “same structure, different atoms” become exactly matchable ￼. This uncovers templates that involve systematic substitutions (e.g. function names, placeholders) which would be impossible to catch with strict matching. High compression potential: The literal skeletons can be very long since all varying parts are turned into slots; a single template can cover many instances that differ in numerous tokens.	Minimal literal fidelity in skeleton: Nearly all specific content can be abstracted away, so templates might consist of mostly placeholders. This can reduce interpretability – a template might read as a sequence of placeholders and a few fixed words. Complex and resource-intensive: Requires algorithms for parameterized pattern matching (e.g. parameterized suffix tree/automaton) which are more complex than standard suffix arrays ￼. Potentially heavy on memory/CPU, especially in-browser. Possible over-generalization: If not constrained, might unify coincidentally similar segments that aren’t meaningfully the “same” pattern (the structure matches purely by chance or generic form). Extra heuristics are needed to ensure templates are semantically coherent, not just structurally.
Hybrid (combined or iterative typing)	Use a mix of raw and normalized representations. For example, maintain a “skeleton” version of the text with certain tokens masked, and a raw version, and cross-check patterns between them ￼. Or iteratively generalize tokens as repeats are found. Essentially, normalization is applied gradually or in parallel rather than all-or-nothing.	High recall with safeguards: Can achieve similar pattern coverage as aggressive pre-typing, but because raw alignment is considered, it reduces false merges. We get the benefit of normalization (patterns surface more easily) and the ability to verify exact structure before accepting a template. Contextual typing: Because raw data is available, the approach can decide on-the-fly which differences warrant treating as slots. E.g. only generalize a token after seeing it vary in an otherwise identical context (as in Drain’s approach of discovering which positions vary after multiple instances) ￼.	Implementation complexity: Needs synchronization between two streams or multiple passes. Harder to implement in a single straightforward algorithm – likely involves more code and potential performance overhead (maybe twice the work: searching normalized and raw). Boundary mapping issues: One must map pattern occurrences in the normalized text back to exact positions in the raw text to form templates, which adds complexity in tracking indices. Still not foolproof: Some judgment calls remain (e.g. when to generalize a token type, how to handle one-off differences). Without careful design, could devolve into partial cases of the issues from the other approaches.

In summary, typing strategies involve a trade-off between pattern frequency vs. structural fidelity (much like token granularity did). Pre-typing and parameterization tilt towards higher frequency by normalizing differences, whereas post-typing preserves fidelity at the cost of missing some patterns ￼. A hybrid approach seeks a middle ground. The “best” strategy likely depends on the document domain and the goals: for highly templated data (like logs or code) more aggressive normalization (pre-typing or Baker-style) is beneficial, while for richly varying text one might start raw and apply typing cautiously. Notably, Wring’s MDL-driven objective can incorporate typing by assigning lower encoding costs to typed slots, which provides a natural incentive to include typing where appropriate (discussed next).

Implications for Pattern Mining and MDL

The choice of tokenization and typing strategy has profound implications for the pattern mining process and the MDL-based objective used to select templates. Key aspects affected include: slot encoding cost, the trade-off between pattern frequency and structural accuracy, and the quality of multi-occurrence alignment (how well instances align and where slots boundaries fall).
	•	Slot Encoding Cost and MDL: In an MDL framework, every bit of data – whether part of a template literal or a slot value – has a cost. Typing a slot can change its encoding cost significantly. For example, if the system knows a particular slot always contains a small integer (bounded range), it can encode those slot values more compactly than if the slot were an arbitrary string of characters ￼. A slot typed as a bounded integer might have a much lower cost than an unconstrained string of text ￼. This means that the MDL gain from using a template with a typed slot can be higher than using a template with an untyped slot, even if both cover the same occurrences. A pre-typing strategy essentially assumes certain types from the start, which could front-load these cost advantages – the mining algorithm might be encouraged to form a slot at <NUM> because it “knows” that’s a generic number token (and the model can assign it a shorter code). Post-typing, conversely, would only realize the cost reduction after identifying the pattern and then labeling the slot. In MDL terms, any pattern that reuses content yields a saving (by not repeating those bits in the description), but if the content that varies is well-characterizable (e.g. digits), the penalty for encoding those variations is smaller. This suggests that from a compression standpoint, having types for slots is advantageous. However, one must also account for the cost of including a template and its slots in the model (the model gets more complex when you add templates and type information). If over-typing leads to too many slots or too generic a template, the savings might be offset by the cost of encoding lots of slot data. The MDL objective in Wring roughly looks at “savings_from_reuse – (template_cost + slot_cost + residual_cost)” ￼. Using this lens: aggressive normalization (pre-typing/Baker) boosts savings_from_reuse by increasing reuse (capturing more occurrences per template), but it can also boost the slot_cost if we introduce many slots or if those slots are high-entropy. The best representation will maximize the net gain: enough generalization to get significant reuse, but not so much that the slot data becomes expensive or the template becomes too generic to be worth it. By assigning appropriate slot costs (lower for well-typed slots, higher for wildcard strings), the MDL model naturally favors templates that normalize in the “right” way. For instance, if replacing digits with <NUM> yields a slot known to be numeric, the slot encoding is cheap (perhaps just a few bits per instance if range is small), so the model reward is high ￼. If one left those as raw strings, each different number might have to be encoded literally, costing more bits and reducing the net gain. Thus, there is an MDL incentive to use typing for repetitive numeric or structured data. Conversely, if a slot is very heterogeneous (high entropy), MDL will penalize it – sometimes it might be better not to form a template at all if the slot values are too random. In summary, tokenization and typing affect how the compression objective evaluates a pattern: a good representation will increase pattern frequency (improving reuse savings) and yield low-cost slots.
	•	Pattern Frequency vs. Structural Fidelity: Both tokenization and typing decisions ultimately reflect a balance between favoring higher frequency vs. preserving faithful structure ￼ ￼. Higher frequency (more generalized patterns) usually means more compression potential, but lower fidelity (skeleton is less like the original text). Structural fidelity (keeping things literal) means templates align closely to the original text, aiding interpretability, but many repeats remain hidden (so compression is forfeited). For example, consider a repetitive report where each section has a unique title. Without typing, the section title differences prevent the section boilerplate from compressing into one template; with typing (placeholder for title), all sections compress nicely at the cost of abstracting the title content. From an MDL standpoint, if the section boilerplate is long and repeats many times, the savings from compressing it will likely outweigh the cost of adding a slot for titles – so the best model would use a template with a title slot. This is an example where normalization (here, treating titles as a slot) favors compression. However, if something repeats only a couple of times, forcing a slot might not pay off, especially if the slot content is large; MDL might then favor leaving it verbatim in the residual. The trade-off extends to interpretability: a template that is too generic (too many slots normalized out) might be hard for a user to interpret, even if it compresses well. Wring explicitly prioritizes interpretability over absolute maximal compression in its use cases ￼. This suggests we should not normalize every possible variation – only those that meaningfully contribute to a reusable structure. There’s also the risk of “structure merging”: if we normalize aggressively, we might combine what are actually different template structures. For instance, two log message types that share a similar wording but have different meaning might get merged into one template if we ignore certain keywords. This would yield a higher frequency pattern but lower fidelity (since the template is now covering two semantically distinct cases). The resulting alignment might look okay, but it could confuse interpretation (one slot might take on values of different semantic types in different occurrences). In summary, the pattern frequency vs. fidelity trade-off means we must choose a representation that captures generalizable repetition (improving frequency and compression) without lumping together parts that aren’t truly the same format (maintaining clear structure). The optimal representation likely uses moderate normalization – enough to expose common structure (especially for very dynamic fields like IDs or timestamps) but not so much that we destroy the identity of distinct templates or produce an overly slot-ridden skeleton.
	•	Alignment Quality and Slot Boundary Precision: Once candidate patterns are found, the alignment of multiple occurrences to form a template is a critical step. Tokenization and typing directly affect how clean this alignment is. If we have chosen sensible token boundaries (e.g. at word or punctuation) and used typing to handle variable fields, then aligning occurrences is relatively straightforward: ideally, all occurrences of a template can be laid out token-by-token, with tokens either matching exactly (literals) or differing (potential slots). If patterns were found in a normalized skeleton format, those placeholders essentially mark perfectly aligned slots. For example, if we found a pattern <NUM> units of <ITEM> appearing in text, aligning each occurrence is trivial since the structure is identical; we just map each occurrence’s actual number and item name into the slots. In contrast, if we had not normalized at all, we might have had to align occurrences character-by-character to figure out what differs. Consider two similar sentences that differ in a name: without any typing, a sequence alignment algorithm would need to identify the name as the differing span. This is doable (via longest common subsequence or similar), but more error-prone. You might accidentally mis-align if, say, parts of the name coincide with parts of the surrounding text. Normalization (like replacing the names with a uniform placeholder) pre-empts this issue by signaling “these positions are different, but treat them as equivalent structure.” In essence, typing provides anchor points for alignment – the surrounding literal tokens act as anchors, and the placeholder represents the entire differing segment consistently across instances. This typically leads to higher quality alignment, as all instances share the same token sequence skeleton ￼. Conversely, if tokenization is too coarse or inconsistent, alignments can suffer. Imagine if we tokenized whole lines as one token each (extreme example): we’d never align anything because either the tokens match or not at all. Or if we tokenized poorly such that a slot’s content and a following punctuation ended up in one token for one occurrence but two tokens in another occurrence, alignment would become tricky. Using a consistent tokenization scheme (especially aligning with natural boundaries) makes it much easier to line up occurrences and identify exact slot boundaries. Wring enforces that the primary mode for slot boundaries is to align with token boundaries ￼ – i.e. slots should start and end at token edges. This is important for interpretability (you wouldn’t want half a word as a slot if it can be avoided). With a good initial tokenization, most slots will naturally coincide with whole tokens (or sequences of tokens). If a difference occurs within what was initially a token, we have two choices: either consider the entire token a slot (losing some literal content that might actually repeat) or refine the tokenization to split that token. Wring suggests optional refinement in such cases: if splitting a token into parts (some literal, some variable) significantly improves compression without hurting interpretability, do it ￼. For example, if many occurrences have IDs like “ABC123” and “ABC456”, and we treat those as one token, the whole token would be a slot in alignment. But we notice that “ABC” is common in all occurrences and only the number differs – splitting into two tokens “ABC” and “123” allows “ABC” to remain as a literal in the template and only the numeric part to be the slot, which is more informative. Character-level analysis might reveal this internal pattern, prompting a token split for alignment precision. In general, the strategies that impose more structure (like punctuation-aware tokens and pre-typed placeholders) yield alignments where slot boundaries are clear and precise, often exactly at one or multiple whole-token positions. Strategies with less upfront structure (raw char or word with no typing) require more complex alignment algorithms (e.g. dynamic programming over characters) to determine where one occurrence deviates from another, and can result in less clean slot boundaries (possibly needing to break a token). To illustrate: in log parsing, once a template is identified, tools often simply mark certain token positions as “variable” – this works only because the log was tokenized and you know token #2 and #6 differ ￼. If logs were just strings, finding those boundaries would be harder. In Wring’s multi-occurrence alignment phase, having the right representation means high-entropy segments (which likely correspond to variables) will show up as gaps or mismatches that align in the same positions across all instances ￼. Typing in advance helps by making those segments literally the same placeholder token, so alignment sees them as one column of uniform tokens (which then become a slot). Without it, alignment must rely on heuristics like gap entropy (if a region in the alignment has a lot of variance, it’s deemed a slot) ￼. Both approaches aim for the same result, but a good representation simplifies the logic. In summary, appropriate tokenization and typing smooth out the alignment process: they ensure that repeated structures line up naturally and that variable parts are either pre-marked or at least isolated to specific token spans. This yields precise slot boundaries and less ambiguity in identifying template skeletons.

Implementation Considerations (JS/WASM)

From an implementation perspective – especially given Wring’s target of running in-browser (JavaScript with possible WebAssembly for heavy lifting) ￼ – the choice of tokenization and typing has practical ramifications for performance and complexity:
	•	Memory and Performance: The input size for Wring can be large (100KB to 10MB per document) ￼, so the efficiency of pattern mining is critical. Tokenization affects the length of the sequence we need to process. A character-level approach will produce an array of length equal to the number of characters (potentially millions for multi-MB text). A token-level approach produces a shorter sequence (number of tokens, which could be an order of magnitude smaller if many characters form one token). Algorithms like suffix arrays or suffix trees that might be used for repeat mining typically have complexity tied to sequence length (often O(n) or O(n log n) construction) and use memory proportional to n. Thus, tokenizing into fewer tokens can yield a sizable performance improvement. For example, a 1MB text (~1 million chars) might tokenize into ~200k tokens; building a suffix structure on 200k symbols is much faster and lighter than on 1 million symbols. Additionally, by reducing irrelevant distinctions (via typing), we also reduce the effective alphabet size and diversity of the sequence, which can sometimes improve pattern mining performance (e.g. suffix sorting might be faster with more repeated content and smaller alphabet). However, there’s a flip side: if tokenization is too fine (char-level), the pattern search space explodes – every substring is a candidate. This can strain memory and CPU when enumerating repeats (the dreaded pattern explosion issue ￼). Coarser tokens inherently limit the search space to more meaningful repeats. Wring acknowledges this risk of output-sensitive explosion if the design is not careful ￼. In practice, a carefully chosen tokenization (words + punctuation) combined with frequency/length thresholds will dramatically cut down the number of candidate repeats that need to be considered, making the problem tractable in JS. Another consideration: many string processing libraries or algorithms in JS might assume a string or char array input. To use token-level, one often maps the token sequence to an integer array. This is feasible (assign each unique token an ID). The implementation just needs to ensure consistent mapping and maybe handle very large token alphabets (e.g. if every word were unique – but in repetitive documents, many tokens repeat). WebAssembly can be leveraged for heavy string algorithms; for instance, constructing a suffix array or suffix automaton in WASM (C++/Rust) and then passing results to JS. In doing so, one must consider the memory overhead of transferring data. A sequence of length n might produce suffix array and LCP arrays of length n as well – if n is in millions, that’s a lot of data. Using tokens to shorten n helps here. Also, if we keep text in a certain normalized form, we can potentially compress it better or use compact data structures (like using Uint32Array for token IDs instead of 1-byte per char if alphabet >256). The WASM-JS boundary issues (like copying large arrays) can be mitigated by shared memory buffers and careful layout ￼, which again is easier with smaller n. So, from a memory/performance stance, tokenizing to reduce sequence length (and hence suffix array size) is beneficial, as is using typing to reduce needless distinctions (which can accelerate pattern scans).
	•	Ease of Alignment and Template Construction: Once repeats are identified (often as substrings or sequences of tokens with multiple occurrences), building templates requires aligning those occurrences and marking slots. If we have a tokenized representation, each occurrence of a pattern can be referenced by token indices (e.g. pattern = tokens i..j, and we have occurrences at positions p, q, r in the token sequence). It is then straightforward to cut out those token spans and align them. For example, using a center-star alignment or pairwise alignment on token sequences is relatively straightforward since we deal in units that should align exactly or not at all. If the alignment needs to introduce a gap (for an extra token present in one occurrence), that gap likely corresponds to a slot absorbing an optional token. Handling this at token granularity is cleaner than at character granularity. If we operated at the character level, an alignment algorithm might produce a very fine-grained mapping (some characters aligned, some as gaps) which then has to be interpreted in terms of tokens – this adds a layer of complexity to reconstruct template structure. By aligning on tokens from the start, we inherently align on token boundaries, satisfying the preference that slots align to token boundaries ￼. Another point is that when reconstructing the final templates, we want to output them ideally in the original text form (with original spacing, punctuation, etc.). If our internal representation preserves all those as tokens, it’s trivial to output the template by concatenating the literal tokens and substituting slot placeholders. If instead the representation had discarded or merged some of that (like if we ignored whitespace or lowercased everything), we’d have to recover or track original text to output properly, complicating the pipeline. Thus, a representation that stays close to the original (just tokenized and typed, but not losing information) makes the round-trip reconstruction easier.
	•	Implementing Typing (Practical): Pre-typing by simple rules (regex replacements for number sequences, etc.) is straightforward in JS and can be done as a preprocessing step. It has negligible cost compared to the pattern mining itself. Post-typing, being after discovery, doesn’t affect performance of mining but requires an extra analysis step (e.g. apply regex or entropy tests to collected slot values). Baker-style parameterization is the most challenging to implement. A naive simulation of Baker’s approach might be: take each occurrence of a candidate pattern and “renormalize” its tokens by mapping the first differing token to a placeholder A, the second new differing token to placeholder B, etc., then compare these normalized strings. There are known algorithms (parameterized suffix trees ￼ ￼, etc.) for this, but implementing them from scratch in JS/WASM is a project in itself. It might be easier to approximate Baker’s results with heuristic pre-typing (e.g. treat all capitalized words as <WORD> if needed) rather than fully general placeholders, to avoid excessive complexity. Hybrids similarly require more complex code: perhaps running two parallel suffix array constructions (one on raw, one on typed text) and intersecting results – which doubles work but might be acceptable on moderate sizes, or doing a custom search that can treat certain tokens as equal under some conditions. The engineering trade-off is that a simpler representation (like tokenizing only, maybe mild pre-typing) allows leveraging well-known string algorithms (standard suffix array, LCP, etc., which are easier to implement or may even have existing libraries ￼). A very complex representation (like full parametric matching) might require custom algorithms for which no off-the-shelf JS library exists, meaning more development and potential bugs. Given browser constraints – limited memory, single-thread performance – one likely wants to lean on efficient C/C++ in WASM for core algorithms. We should aim for a representation that those algorithms can handle readily (e.g. a sequence of integers for suffix array). Tokenization is actually helpful here: by converting tokens to an integer sequence, we are in the right form for a suffix array implementation. If we needed a parameterized suffix array, that’s more exotic and likely not available; we’d have to implement something like Baker’s algorithm ourselves or do multi-pass scans.
	•	Balancing Effectiveness vs. Feasibility: Considering all the above, a practical recommendation is to use tokenization and lightweight typing to simplify the problem, rather than relying on extremely advanced generalization algorithms. For example, splitting the input into tokens (including punctuation) immediately cuts down problem size and avoids awkward alignments, and is easy to implement (tokenizer with regex or simple FSM). Then, apply pre-typing for the most obvious variable tokens (digits, perhaps hex strings, GUID patterns, etc.), which is also easy to do (simple regex replacements or token class mapping). This will handle a large class of patterns (especially for logs and technical data) by turning them into repeated skeletons with placeholders. For the remaining patterns that weren’t caught because they had more complex differences, the mining can proceed on the partially normalized text. If needed, a second pass or post-processing can catch those by either (a) doing Baker-style alignment on the smaller set of candidates or (b) simply presenting slightly under-generalized templates which a user or a heuristic can further generalize. This approach keeps the core mining (which might use suffix arrays to get repeats) within well-understood boundaries. It avoids the need for a fully parameterized suffix tree in the first implementation. In a browser environment, where performance must be carefully managed, every simplification helps. Moreover, by aligning templates at token boundaries and using types for slots, the output templates are easier to interpret (they look like sentences with some <TYPE> markers, rather than random character segments). This aligns with Wring’s goal of interpretability.
	•	Example: Suppose we implement Wring with a suffix array repeat miner on a tokenized+pretyped text. For a log file input, we might tokenize each line into tokens (words and punctuation) and replace all digits with <NUM>. The suffix array then finds long repeated substrings in this token sequence. Thanks to normalization, an entire log message template might appear as a repeated substring (because all lines look identical after <NUM> masking). We retrieve that and align – because of tokenization, alignment might just be a direct match (the substring is identical across occurrences). We then output a template replacing the <NUM> tokens with a slot (type “number”). This yields a clear template, high compression (since maybe hundreds of lines share it), and we did it using a standard suffix array approach. If we hadn’t tokenized and pretyped, we might have found only shorter repeats like common phrases within the lines, or none at all, and we would need a more complicated clustering approach to group those lines.

In conclusion on implementation: the representation that best supports template discovery is one that simplifies the structure for the algorithm without discarding the ability to reconstruct the original. Tokenization (especially with punctuation awareness) and moderate typing are simple yet powerful tools to achieve this simplification. They reduce the problem size and guide the pattern mining, making a browser-based solution feasible. Overly complex schemes (like full dynamic parameterization) might yield marginally better compression but at a steep cost in algorithmic complexity and runtime, which might not be justified in a client-side context.

Behavior Across Different Document Types

The optimal tokenization/typing strategy can also depend on the type of document being processed. Wring’s target use cases span structured documents (e.g. legislation), semi-structured logs, and markup-heavy formats like HTML ￼. These domains have different kinds of repetition and variability, so the representation must adapt to each:

Structured Documents (Legal/Procedural Text)

Structured texts such as budget bills or legislation have a lot of internal redundancy in format: for example, repeated clause structures, numbered sections, enumerated lists, boilerplate phrases (“the sum of ___ is appropriated for ___”). They are written in natural language, often with consistent terminology and formatting (indentation, subsection labels, etc.). Here, tokenization at the word level (with punctuation tokens) is very appropriate, since the text is word-based and we want the templates to align with human-readable units (phrases, sentences). Character-level patterns would likely produce many meaningless fragments (like parts of words or legal citations). Word tokens capture the structure (e.g. “Section”, “shall be”, punctuation like “;”) clearly. Typing strategy in legal text should be cautious: one wouldn’t want to normalize every uppercase term or number indiscriminately because some of those carry meaning (e.g. Section numbers vs monetary values vs dates). However, certain normalizations make sense: e.g. all numeric section references might be considered structurally similar. Replacing those with a placeholder could help find that every section title is “Section  [Title]” for instance, which is a clear template pattern. Similarly, if the document has placeholders like “[year]” that change, pre-typing years as <YEAR> could reveal patterns. But in general, a light touch is advisable: let the mining occur mostly on the raw text, because legal texts often have almost exact repeats that are easy to find (e.g. a clause repeated verbatim in multiple places). Once those are found, you can apply post-typing to label obvious slots (numbers, names of agencies, etc.). This preserves the precise language in templates, which is important for interpretability (stakeholders likely want to see the actual phrasing of the law, not an overly abstracted form). Also, structured docs may have nested structure (sections containing sub-sections). A hierarchical template model might be needed, but as far as tokenization goes, ensuring punctuation like section delimiters (“Section 1.”) and formatting markers are separate tokens will help detect these boundaries. For example, if each section starts on a new line with a pattern, tokenizing the newline or section symbol can be useful if treat them specially. In many cases, these documents use consistent phrasing – pattern mining will succeed with relatively straightforward tokenization. Memory and alignment: these texts can be large, but repetition tends to be of moderate-sized chunks (sentences or paragraphs). Suffix-based mining should cope as long as tokens are used. Since interpretability is key (as noted, Wring aims to prioritize interpretability in structured docs over pure compression) ￼, we’d lean towards minimal normalization: enough to combine obviously identical structures, but not so much that templates lose the actual wording.

Example: A budget law might list many line items like “For the Department of , the sum of $ is appropriated.” The structure is the same, only <X> (department name) and <Y> (amount) vary. Tokenization will break this into words and symbols (“For”, “the”, “Department”, “of”, <X>, “,”, “the”, “sum”, “of”, “$”, <Y>, “is”, “appropriated”, “.”). We might pre-type the amount to <NUM> or just rely on the presence of “$” to delineate it. The pattern “For the Department of ___, the sum of $___ is appropriated.” will be discovered either way if multiple lines share it. Pre-typing <NUM> will make different amounts identical, helping the pattern frequency. The output template would then likely have two slots (one for department name, one for amount), which is exactly what we’d want, and those slots can be post-typed as <ORG> and <MONEY> perhaps for clarity. Without any typing, the algorithm would still find a long common sequence, but might stop before the numbers if they differ in digit-length (unless punctuation around “$” helps anchor it). So a bit of normalization (for the numeric amount) improves it. That template compression is highly interpretable and compressive.

Semi-Structured Logs

Logs are a canonical case where aggressive normalization is essential to uncover templates. Log files typically consist of many entries (often line-separated) that follow a set of formats (templates), but with variable fields like timestamps, IDs, user names, etc. In raw form, each log line is often unique (especially if it includes a timestamp or unique ID), which means straightforward substring search won’t find long repeats. However, we know by domain knowledge that these lines are generated by code from a format string – they do have a common skeleton. The common approach in log template mining is to parse each line into tokens (usually split by whitespace and certain delimiters) and then mask out constant-changing fields using heuristics or regex (for example, any token that matches \d+ is replaced by <*> wildcard) ￼ ￼. This is essentially pre-typing. Tools like Drain and Spell use this method: they treat logs as pre-segmented (each line separately) and apply typing + clustering ￼. For Wring (single-document approach), we can similarly tokenize the log by spaces and punctuation. We might even treat the newline as a token or at least ensure we don’t search for patterns across line boundaries unless needed (assuming each log entry is independent). Then, we absolutely should normalize obvious volatile tokens: timestamps (2025-12-20 12:00:00 -> <DATE>), hex IDs or GUIDs -> <ID>, numbers -> <NUM>, etc. Without this, it is unlikely two log lines will appear identical enough to count as a repeat; with normalization, dozens or hundreds of lines reduce to the same token sequence. In fact, after such normalization, the document (sequence of tokens) will contain large repeated segments corresponding to each log template. For example, suppose we have log lines:

2025-12-20 12:53:19 INFO User 123 logged in from 10.0.0.5
2025-12-20 12:55:42 INFO User 456 logged in from 10.0.0.8

Tokenizing (with punctuation, etc.) might give tokens like [ "2025-12-20", "12:53:19", "INFO", "User", "123", "logged", "in", "from", "10.0.0.5" ] for the first line. If we treat the whole file as one sequence including newlines, the patterns might not easily show up across newline boundaries. Instead, one approach is to handle logs line by line: Wring could either explicitly segment by line and find repeats among lines (a clustering approach), or include a special token for newline and rely on pattern mining to detect repeated sequences that end at newline boundaries. Either way, with normalization, both lines become [ <DATE>, <TIME>, "INFO", "User", <NUM>, "logged", "in", "from", <IP> ]. Now, a longest common subsequence (or suffix array repeat) can easily spot "INFO User <NUM> logged in from <IP>" as a repeated pattern (assuming we allow it to span tokens 3 to end of line). If newline is considered, the entire line minus the timestamp is identical. We might get a template like: <DATE> <TIME> INFO User <*> logged in from <*>. Actually, we might even want to treat the timestamp as separate structured tokens or remove it if it’s always unique. Many log parsers drop the timestamp entirely from template analysis because it never repeats exactly. Doing so (or typing it as <TIMESTAMP>) ensures the repeating portion “INFO User X logged in from Y” is clearly found. This highlights that logs benefit from heavy pre-typing and even rule-based ignoring of certain fields to maximize template extraction ￼.

The implication for MDL is that without such typing, each log line would be described individually (no compression); with typing, one template can cover potentially thousands of lines, paying a small slot cost for each variable field. The compression gain is huge. In fact, real-world log mining has shown that raw logs have a “vocabulary explosion” problem (nearly every line unique) which templating reduces to a manageable set ￼.

In implementation, logs are simpler in one respect: each entry is clearly delimited (by newline or other separators). Wring might exploit this by treating each line as a somewhat independent region when mining repeats. However, one must be careful: if a pattern spans multiple lines (stack traces, multi-line messages), the algorithm should handle that, but typically logs are line-oriented. The tokenization should definitely include punctuation in IPs, paths, etc., so that structure like 10.0.0.* or file paths can be broken into tokens and generalized. Baker-style parameterization is perhaps less needed because our pre-typing already catches most differences. But if there are cases where certain constant words differ (maybe one log says “started” vs another “initiated” for the same event), parameterization could unify them, but that might actually be a different template (so we likely don’t want to merge those). So for logs, pre-defined typing and simple clustering is usually sufficient and safer. Post-typing in logs is not effective because, as noted, without normalization you won’t even find the patterns. So we would not choose a raw approach here.

In summary, for logs: use punctuation-aware tokenization and aggressive pre-typing of variable fields. This will yield clear templates (which are essentially the log message formats). This approach mirrors state-of-the-art log parsers which use extra delimiters and masking rules to achieve high grouping accuracy ￼. The result is a dramatic reduction of unique lines into a small set of templates ￼, which is exactly what we want for compression and interpretability (operators can then see the general patterns of events from the log).

Markup-Heavy Formats (HTML, XML)

Markup documents like HTML or XML pose a somewhat different challenge. They often contain repeated structures (like multiple items in a list, table rows, etc.), but the presence of tags, attributes, and free text intermixed requires careful tokenization. An HTML page, for instance, might have a list of nearly identical <div> blocks for each search result on a page. Those blocks might differ only in the data (names, links, images). The repetition is usually verbatim at the character level if you ignore the data content. Tools like IEPAD have leveraged this by treating the HTML as a long string and finding longest repeated substrings to identify record boundaries ￼. A suffix tree (Patricia tree) can indeed find the longest common segments which often correspond to the shared prefix of each repeated HTML segment, etc. ￼. However, a naive character-level approach on HTML might be overwhelmed by the volume of repeated tag sequences at different nesting levels (and things like angle brackets can create many small repeats). Instead, a tokenization that separates tags and text is very useful. For instance, one can tokenize an HTML by treating < and > as separate tokens (or each tag as one token), attributes as tokens, and text content as tokens. This way, a repeated <div class="result"> ... </div> structure will appear as a repeated sequence of tokens: <div, class, =, "result", >, (some content tokens…), </div>. Many of those tokens (“<div”, class, "result", </div>) will repeat exactly across instances. The varying parts (maybe an image URL or a name in the content) will be distinct tokens inside. If we apply typing, we might replace those content tokens with placeholders (e.g. actual product names replaced by <NAME> or just treated as generic text). The combination of tokenizing and typing in HTML helps ensure that the repetitive scaffold of tags is captured as the template skeleton, with slots for the dynamic data.

One challenge is that HTML repetition can be hierarchical – e.g., each item might contain a smaller repeated pattern (like a star icon repeated 5 times for rating). But Wring’s approach to support hierarchical templates (flat vs hierarchical coverage) would take care of nested templates if needed. At the token level, it’s fine – it will find the smaller repeats too.

Typing in HTML is often about attribute values and content. Certain attributes like id="item123" might be unique per item; replacing those numeric parts with a placeholder will allow matches. However, we usually do not want to treat tag names themselves or structural keywords as placeholders – a <span> is different from a <div>; merging those in a template would break the validity or at least not make sense. So we preserve all tag tokens literally. We might normalize case or quote style if needed to avoid superficial differences. We might also normalize irrelevant differences like self-closing vs explicit closing tags if they vary (but usually they don’t in repeated elements).

Example: Suppose an HTML page has a table with multiple rows of similar structure. Each row is like: <tr><td>Product Name</td><td>$Price</td><td>In Stock</td></tr>. The product name and price vary per row, but the HTML tags and the text “In Stock” might be constant. Tokenizing on angle brackets and so forth, one row might produce tokens: <tr, >, <td, >, Product, Name, </td, >, <td, >, $, 19.99, </td, >, <td, >, In, Stock, </td, >, </tr, >. Another row will be similar but with different product name tokens and different price tokens. A suffix array or repeat finder over this token list will likely find a long repeat starting from <tr > <td > through </td > <td > perhaps up until the text of the name, then maybe a gap, then </td > <td > then another gap, then </td > </tr >. We can align these and see that <tr><td> and the third <td> containing “In Stock” are common literals, whereas the second <td> content is numeric and varies, and the first <td> content (product name) varies. That yields a template like <tr><td>{ProductName}</td><td>${Price}</td><td>In Stock</td></tr> with two slots. Achieving this is easier if we’ve separated $ and the number, etc. If we also pre-typed the number as <NUM>, the sequence becomes identical for the price part except the <NUM> token, which is fine (that becomes a slot directly). If we didn’t, “19.99” vs “5.49” might still be caught as different tokens of the same token type (digit sequences) if the algorithm can treat digits generically, or might be missed if lengths differ. But likely the tags around them provide enough anchor to align.

Interestingly, HTML often has exact repeats if content is identical. But more often, content differs. Parameterization (Baker style) could, in theory, align even if one row had “In Stock” and another had “Out of Stock” (if those are considered different data, maybe treat them as variables if context suggests they might vary). But if one expects “In Stock” to be a literal constant in all, then that difference means a different template (or a variant template). We might not unify those because that changes meaning. A careful approach might consider those separate patterns.

Wrapper induction research like IEPAD indeed found that pattern mining plus alignment works well: it identifies the repeating region and then uses multiple sequence alignment to identify slots ￼. Wring can do similarly – once repeats are found (via suffix structures), align them to decide literal vs slot. The good news is HTML tokens make alignment easier because the sequences are highly similar.

From an implementation perspective, tokenizing HTML into tokens is straightforward (could even parse into a DOM and linearize, but a regex for tags might suffice). The sequence might be quite long (HTML could be many tags), but largely repetitive, so suffix structure performance is improved by all the repetition. Pre-typing numeric attributes or random strings can further reduce unique tokens. The MDL model will definitely reward capturing large repeated HTML chunks since those can be huge (thus big savings). And interpretability is maintained because the template will literally show the markup structure, which is useful (one use case is converting repetitive HTML into data-driven generation ￼).

One must be mindful of noise: HTML pages might have some repeated boilerplate (headers, footers) that are not part of the main data records but still repeat – the algorithm might find those too. That’s fine; Wring can output templates for those as well.

In summary for HTML: use a structure-aware tokenization (each tag/attribute as token, etc.), and use typing primarily for data values embedded in the markup (numbers, IDs, maybe long text as a single placeholder if needed). This will let the system discover repeated tag patterns and align them. Essentially, treat it similarly to logs in terms of finding structure vs values, but with the complexity that structure is hierarchical (tags). Given that wrapper induction has shown success using repeated pattern mining on HTML ￼, the approach is validated. Wring just needs to implement it carefully to handle possibly nested repeats and not to be overwhelmed by the size of HTML (which can be quite large but usually repetitive).

To contrast the three domains: Legal text has moderate, often exact repetition and benefits from minimal normalization; Logs have high repetition in structure but none in raw content without heavy normalization (so aggressive pre-typing and tokenization are essential); HTML has highly structured repetition where tokenizing the markup is key and moderate typing of content yields clear templates (similar to wrapper induction techniques). These differences align with the intuition that in logs and code, variable data is noise that must be abstracted ￼, whereas in prose or legal text, much of the content is meaningful and should only be abstracted when obviously formulaic.

Conclusion and Recommendations

Bringing the analysis together, the representation that best supports single-document template induction in Wring is one that exposes repeated structure clearly while minimizing false generalization. Concretely, the following strategy is recommended:
	•	Use a token stream with punctuation and structural markers preserved. This means splitting the document into tokens such that words, numbers, and meaningful symbols (punctuation, HTML tags, etc.) are separate. Doing so strikes a balance between character-level and pure word-level approaches, retaining structure (like delimiters) as their own tokens. This tokenization ensures that templates align with intuitive boundaries (mostly whole tokens) and that structural patterns (like XML tags or log delimiters) are capturable as literal tokens in skeletons.
	•	Apply selective pre-typing normalization for high-variability tokens before mining. In domains where certain tokens are essentially unique per occurrence (identifiers, timestamps, GUIDs, random numbers), replacing them with a generic placeholder (or category-specific markers) greatly increases pattern frequency with little downside. For example, normalize all purely numeric tokens to <NUM> ￼, all hex-like strings to <HEX>, emails to <EMAIL>, etc., as applicable. This should be done conservatively: use it where the token’s specific value is not important to the document’s structure (e.g. log IDs, temporal stamps, auto-generated keys). The result is that repetitive formats become identical in the normalized view and will be discovered as templates. The fidelity loss is minimal for these cases because we know those details can be slotted back in without confusion. Indeed, experiments in log parsing show this drastically reduces “vocabulary” and reveals underlying patterns ￼.
	•	Leverage a form of Baker-style parameterization for cases of structured substitution, but in a controlled way. Rather than full arbitrary placeholder matching, implement a mechanism to unify tokens that consistently play the same role across occurrences. This could be done post-hoc: if a template is discovered and its instances show that, say, two different tokens occupy the same position across instances, treat that position as a slot (even if those tokens weren’t pre-typed). In effect, this is dynamic typing: the system infers that those differing tokens are interchangeable in context, similar to Baker’s idea of parameterizing them. This approach was exemplified in the log example where seeing two lines allowed identifying which positions vary ￼. For initial implementation, one can approximate Baker’s results by strong pre-typing rules (like “all capitalized words that only appear once -> <VAR>”), but the ultimate approach might be to incorporate a check during alignment that any consistently differing token sequence becomes a slot. This way we achieve much of the placeholder power without needing a fully generalized suffix tree for placeholders. Essentially, align first on obvious anchors, then treat whatever doesn’t align as potential slots.
	•	Maintain parallel information for raw and typed forms as needed. While mining primarily on the normalized tokens (for efficiency and recall), keep a mapping to the original text. Once a candidate pattern is found in the normalized sequence, we can fetch the corresponding raw text segments to verify that they align exactly (they should, barring differences in the placeholder tokens). This ensures no template is accepted that wouldn’t truly reconstruct the original. It also allows us to output the template with proper slot values and to double-check interpretability. If a pattern in normalized form spans an odd boundary in raw text, we might reject or adjust it. This parallel approach is a lightweight version of the hybrid strategy, giving us confidence in results.
	•	Align on token boundaries and refine tokenization where necessary. During multi-occurrence alignment, enforce that slots start and end at token boundaries for clean templates ￼. If an alignment suggests a slot that covers only part of a token and a literal part of that token is actually constant across all instances, consider splitting that token in the input model and rerunning alignment for that template. Such a scenario might be detected by high entropy in one portion of a token versus stability in the rest (for instance, variable suffixes). Implementing this as a refinement step ensures we don’t miss patterns that a slightly different tokenization would have exposed. This is a targeted use of the “discover token boundaries from structure” idea: we mostly impose boundaries, but we’re willing to adjust them when evidence shows it would yield a better template.
	•	Use MDL-driven selection to calibrate how much normalization is beneficial. Because Wring uses an MDL-ish objective, it can naturally balance complexity. If we over-generalize (too many slots, too generic), the model’s cost will reflect that (many bits to encode slot values or extra template overhead) ￼. If we under-generalize, we miss compression opportunity (residual cost remains high). By evaluating the description length, we can iteratively tune our representation. For example, if a certain token category yields slots that have extremely high entropy (meaning no compression gain), we might decide not to normalize those tokens after all (treat them as part of literal). Conversely, if a pattern appears just below the frequency threshold until we normalize one more token, MDL will show a big gain once we do – indicating that extra normalization is warranted. In essence, the MDL score can guide refining the tokenization/typing after an initial pass, ensuring the chosen representation is indeed the one that maximizes compression without creating nonsense templates.

In conclusion, the best representation is a tokenized, lightly normalized token stream that captures the document’s repetitive scaffolding while abstracting away incidental variations. This typically means: split text into tokens at natural boundaries, preserve all structural symbols, replace clear “random” values with placeholders, and let the pattern mining do its job on this simplified sequence. For structured narrative text, this might mean almost raw tokens with maybe numbers normalized; for logs, it means heavy masking of variable fields; for HTML, it means tokenizing tags and normalizing data fields. By doing so, Wring can successfully induce templates that exactly reconstruct the input, compress it by identifying repeated structures, and remain interpretable (the templates look like the original text with slots in place). This approach is validated by adjacent domains: log parsers achieve large template reductions with tokenization+masking ￼, clone detectors find abstract clones via token normalization ￼, and wrapper induction finds repeated HTML records via pattern discovery and alignment ￼ – all pointing to the efficacy of a balanced tokenized & typed representation. Adopting these strategies will ensure that Wring meets its goals of compression and interpretability within the practical constraints of a browser-based tool.

Sources:
	1.	Wring Project Documentation – Research Questions on Tokenization and Typing ￼ ￼; Slot Costs and Boundaries ￼ ￼; Adjacent Domain Strategies ￼.
	2.	Srikrishnan Shankar, “Drain3: The Unsung Hero of Templatizing Logs for ML” – Example of log tokenization and masking benefits ￼ ￼.
	3.	Brenda S. Baker, “On Finding Duplication and Near-Duplication in Large Software Systems” – Parameterized (p-string) matching concept for code templates ￼.
	4.	Chien-Yu Chang et al., “IEPAD: Information Extraction Based on Pattern Discovery” – Wrapper induction by repetitive pattern mining and alignment in HTML ￼.