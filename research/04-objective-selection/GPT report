Scoring and Selection Regimes for MDL-Based Template Induction (Wring Project)

1. Approximations from Grammar Compression

Minimal Grammar vs. MDL Templates: Selecting an optimal set of templates under an MDL gain objective (reuse savings minus costs) is equivalent to finding a smallest context-free grammar generating the document ￼. This smallest grammar problem is NP-hard ￼ ￼, so Wring relies on approximation strategies from grammar compression literature. Grammar-based compressors seek a compact grammar for a single string, analogous to finding reusable templates in the document. Key approaches include Sequitur, Re-Pair, and related small-grammar heuristics:
	•	Sequitur (Linear-Time Grammar Induction): Sequitur is an online, single-pass algorithm that infers a hierarchical grammar by replacing repeated phrases with rules as the input is read ￼. It maintains two invariants that mirror MDL principles: (1) Digram uniqueness – no pair of symbols appears twice, so any repeated bigram triggers creation of a new rule ￼; (2) Rule utility – any rule (nonterminal) that ends up used only once is deleted (its expansion is inlined) ￼. These ensure repeated substrings are always factored out, but trivial or degenerate rules are pruned. In practice, Sequitur runs in linear time and yields a compact grammar (templates) that often reflect meaningful hierarchy ￼. The trade-off is that Sequitur’s greedy, local decisions (replacing the first repeated digram encountered) might not always produce a globally optimal set of templates. It favors fidelity to the input structure (maintaining exact substrings) and yields interpretable hierarchical rules, but it might introduce some rules that a more global algorithm would skip. Still, by removing single-use rules, it inherently avoids degenerate templates (any template is guaranteed to cover at least two occurrences) ￼. Sequitur’s linear complexity and one-pass operation make it suitable for browser-scale inputs (streaming-friendly and memory-efficient), albeit at the cost of possibly slightly larger grammars than an optimal solution.
	•	Re-Pair (Frequent Pair Substitution): Re-Pair is an offline grammar compressor that repeatedly replaces the most frequent adjacent symbol pair in the string with a new nonterminal ￼. This greedy strategy maximizes immediate compression gain at each step: each substitution can be seen as saving (frequency - 1) occurrences of a pair. Re-Pair tends to produce very small grammars (often close to minimal) because merging high-frequency pairs captures large redundancies early. It naturally approximates the MDL gain – frequent long pairs have high reuse savings relative to the cost of one new rule. Complexity: Re-Pair can be implemented in near-linear time, though it requires scanning for frequency counts and updating them iteratively. Memory use can be significant for very large inputs, but recent improvements achieve linear space overhead ￼. For browser-scale (up to ~10MB text), Re-Pair is feasible in optimized C++/WASM, but might be heavy in pure JS. Fidelity: Re-Pair’s substitutions preserve exact substrings as rules, so like Sequitur it does not introduce approximation. It is purely data-driven – it might create rules that a human wouldn’t consider “intuitive” templates (e.g. a rule might combine fragments if that pair was common). However, it captures repetition effectively, often yielding fewer, longer templates. One trade-off is over-merging: on some inputs, blindly merging most frequent pairs can produce a grammar that, while small, may not align with human-interpretable patterns (e.g. concatenating two distant patterns if they frequently co-occur as a pair). Another trade-off is that Re-Pair doesn’t inherently prevent creating rules that might be mostly slots – but since it only replaces identical pairs, any rule it makes represents a purely literal substring, avoiding the issue of mismatched slots altogether.
	•	Small-Grammar Heuristics: Beyond Sequitur and Re-Pair, research on the smallest grammar problem provides other approximation approaches ￼. LZ77/LZ78-based methods treat repeated substrings similarly, using earlier text as a dictionary; these can be transformed into grammars and achieve $O(n/\log n)$ grammar size in theory ￼. Greedy longest-match algorithms choose the longest repeated substring or a high-gain pattern to replace at each step, which aligns with maximizing MDL gain per template. Such greedy algorithms have proven approximation bounds (within $O(\log n)$ of optimal size in the best cases) ￼, but they can be too slow if implemented naively (due to searching for the longest repeat globally each time). A suffix tree or suffix array can speed this up by listing all repeats, but the number of candidates can be huge without pruning. In practice, grammar compressors impose thresholds (min length or frequency) to keep candidate counts manageable ￼ ￼. Suffix automaton–based approaches enumerate all distinct substrings in linear time and can identify frequent ones (via end-counts), which is another avenue: one could approximate template selection by picking substrings with highest frequency × (length-1) (which indicates how many characters would be saved by compressing that substring). This heuristic is essentially the criterion Re-Pair uses for pairs, extended to any substring. It sacrifices fidelity to the true MDL formula but is computationally cheap to compute and often correlates with compression gain.

Trade-offs: Fidelity – All these methods produce exact templates (no fuzzy matching), ensuring perfect reconstruction. They differ in how closely they approximate the optimal MDL gain. Sequitur enforces a structure (no repeating digrams) that can overshoot minimal grammar length, but typically not by much. Re-Pair’s greedy frequency strategy usually yields very good compression, but has worst-case scenarios where it performs poorly ￼ (though those are contrived inputs). More advanced algorithms with better approximation ratios exist ￼, but they involve more complex multi-level substitutions and are not as simple/fast for practical use. Complexity – Sequitur is linear-time and low-memory, ideal for streaming or in-browser use ￼. Re-Pair is also linear-time in theory, but maintaining frequency counts and replacing symbols can be memory-intensive; careful implementations run efficiently in practice ￼. Suffix-tree or suffix-array methods for finding repeats are $O(n)$ to build and can enumerate repeats in $O(n)$ as well, but the number of repeats can be quadratic in worst case (pattern explosion) ￼. Thus, pruning (e.g. only consider repeats above a length/frequency threshold ￼) is crucial to keep it browser-feasible. Browser-Scale Suitability – Simpler linear heuristics (Sequitur, LZ, suffix automaton counts) are most suited to run in a browser or WASM module. They avoid superlinear explosion and handle input up to a few million tokens. More exhaustive or optimal algorithms that require heavy computation (ILP solvers, or deep search for minimal grammar) are impractical at that scale. In summary, adapting grammar compression gives Wring a pragmatic path: use a fast compression heuristic to propose candidate templates (repeated substrings) and approximate their MDL gains. This yields a near-optimal template set with manageable complexity, at the cost of possibly missing some global-optimum patterns or producing a few extra templates. The benefit is that these methods naturally reuse internal redundancy without needing an external corpus ￼, aligning with Wring’s single-document assumption.

2. Cost Modeling and Calibration

Selecting templates under MDL requires careful cost modeling for templates, slots, and residuals. A well-calibrated cost model steers the algorithm away from degenerate templates (those that don’t actually compress or that harm interpretability) ￼. Key considerations:
	•	Template Definition Cost: Each template has an overhead cost in the description length: you must encode the template’s literal text and its structure (including placeholders for slots). Best practice is to charge a fixed cost per template plus a cost proportional to its literal length. This ensures that introducing a new template is only beneficial if its reuse (savings) outweighs this overhead. For example, a template that appears only once will incur cost (definition + one usage) with no reuse savings – a net loss, so it should be disallowed or automatically pruned. In MDL terms, any template must appear at least twice to have non-negative gain (two occurrences are the break-even point for one definition). Sequitur’s rule utility invariant essentially encodes this best practice by deleting any rule used once ￼. Tiny frequent literals (very short patterns) also might need penalization: if a 1-character sequence appears 100 times, compressing it saves some space, but introducing a template for a single character might not be worthwhile if the single-character “template” encoding isn’t much shorter than just writing the character (especially if the character is common anyway). Thus, template cost can include a minimum literal length – e.g. don’t form templates shorter than 2 or 3 tokens, or impose a smaller relative gain for short templates. This prevents pathological cases where the algorithm would otherwise use many trivial templates to cover common words or characters and complicate the output.
	•	Slot Encoding Cost (Type-Aware): Every slot (variable field) in a template has to be encoded for each instance. The cost for a slot value depends on its type or entropy. Best practice: assign lower cost to low-entropy slots and higher cost to high-entropy slots ￼. For example, if a slot is a bounded integer or an enumeration with few possible values, it can be encoded in fewer bits (or even by a short codebook) – making such slots “cheap.” A timestamp or date might have moderate entropy (structured but many possibilities) – higher cost than a small integer but still with some possible compression if formatted similarly each time. An unconstrained string (e.g. an error message detail or UUID) is essentially high entropy – encoding each occurrence might cost nearly as much as the raw text, so such a slot provides little compression benefit. Reflecting this, the MDL model can assign slot cost based on estimated entropy or an explicit model per type (e.g. log₂ of the value range, or using character-level entropy for string slots). This discourages templates that have slots full of random content. In practice, before mining templates one can pre-type the input (e.g., replace all numbers with a <NUM> token, etc.) to boost pattern frequency ￼. This increases compression (common structure emerges) but loses fidelity in the template literals. Alternatively, types can be inferred after discovering patterns ￼. Either way, once we know a slot is, say, numeric, we can model its cost as smaller (a fixed-length binary encoding) than if we treat it as an arbitrary string of digits. Calibration: The slot cost model might be dynamically adjusted if we observe, for instance, that a particular slot takes only a few distinct values across instances – then its actual entropy is lower, and we might lower its encoding cost in the MDL calculation.
	•	Residual Penalty: Residual text is content not covered by any template (left as literal output). In an MDL sense, residuals are encoded verbatim, so they incur full cost (essentially no compression). There isn’t a “penalty” beyond that – except that if too much text remains residual, it means templates failed to capture structure. We might impose a soft penalty on leaving text uncompressed to encourage more coverage: e.g., the algorithm could slightly favor using a template even if it has borderline gain, to reduce residual size. However, one must be cautious – forcing compression of truly random or unique text can backfire (it introduces template overhead for little actual reuse). A balanced approach is to treat residual encoding with a fixed model (like each character costs log₂|alphabet| bits, or each token costs some bits). If the document’s overall entropy is known (say $H$ bits per token), we use that for residual encoding. Calibration: If the document is very large, even a small fraction of residual might correspond to many bytes – in such cases, one might allow templates with smaller gains to go through (since even a tiny percentage compression on a huge document is worthwhile in absolute terms). Conversely, for a short document, we should be stricter (any template must have a strong gain to justify itself). Thus, the document size can scale the thresholds: e.g., require a minimum absolute savings of X bytes or X% of the document size for a template to be accepted.
	•	Avoiding Degenerate Templates: Degenerate cases include mostly-slot templates (templates whose literal part is extremely small, and nearly the entire content varies per instance) and single-use templates. To avoid these, the cost model can impose explicit penalties. For instance, charge an extra cost for each slot (making too many slots expensive) or for a high slot-to-literal ratio. If a template has more slots than literal tokens, it might be a red flag – the algorithm could either reject such a template or require a higher gain threshold for it. One can also set a minimum literal length or a max slots per template as a rule. These heuristics reflect the interpretability goal: a template should represent a common structure (substantial literal skeleton) with some varying parts, not a case where almost the entire text is variable. Another safeguard is to ensure each template has a minimum frequency > 1 by definition, and perhaps >2 or 3 for very short templates to ensure significant total savings. In summary, calibrating the cost components means tuning these parameters (template overhead, per-slot cost, residual cost model) so that the MDL objective aligns with intuitive outcomes. During development, one would likely adjust these based on document statistics: e.g., if the average token entropy is low (lots of repetition), template overhead can be relatively higher (since many patterns will clear the bar); if entropy is high (less repetition), overhead might be set lower to still catch whatever small patterns exist.
	•	Dynamic Adjustments: The model should not use completely fixed costs regardless of input. Document size: As noted, in larger documents the absolute overhead of one template is negligible, so we can afford more templates; in small docs, be frugal. Token entropy: If the document’s vocabulary or content is highly repetitive (low entropy), it means many good templates likely exist, but also encoding those repeats is cheap (they could be captured by fewer bits). In such cases, we might tighten the slot cost (because even slots might be predictable) and lower the gain threshold – effectively, the data “wants” to be compressed heavily. If the content is very unpredictable, we do the opposite – raise thresholds to avoid chasing tiny patterns. Slot type frequency: If a certain type of token (e.g. numbers, or a specific identifier pattern) appears extremely often, we might decide to pre-treat it as a slot everywhere (since it clearly is not random text but a class of data) – lowering its encoding cost and increasing overall savings from templates that involve that type. On the other hand, if a type is rare, no need for special-case cost reduction.
	•	Proxy Scoring for Efficiency: Computing the full MDL gain for every candidate pattern can be expensive, so a proxy score is often used to rank or filter candidates before detailed evaluation. One effective heuristic is: Score = (Coverage × LiteralLength) – (SlotEntropyPenalty). Here Coverage might be the number of characters or tokens covered by the pattern across all its instances (for example, frequency $f$ times literal length $L_{\text{lit}}$). LiteralLength is the length of the template’s fixed parts, and SlotEntropyPenalty is a term that grows with the unpredictability of the slots. This score intuitively rewards patterns that cover a lot of text with stable content. For example, a template that has 50 literal characters and appears 10 times covers 500 literal-character occurrences; if its slots are small and low entropy, the penalty is small, giving a high net score – likely a good compression candidate. Conversely, a template that appears 10 times but has only 5 literal chars and the rest are wildcard slots might score low, because $L_{\text{lit}}$ is small and we subtract a penalty for those high-entropy slots. This proxy is essentially measuring reuse benefit minus variability cost in a simplified way. It’s similar to the metric used in some compression and pattern mining algorithms: e.g., Sequitur implicitly uses occurrence count and rule length, and Re-Pair explicitly targets high frequency pairs (where savings = (freq–1) * 1 char saved per pair). By including slot entropy, we extend the idea to non-exact repeats. Effectiveness: Such proxy scores are very useful for quickly filtering out poor candidates (e.g., any pattern with score <= 0 can be discarded without full MDL calculation). They may not perfectly match the true MDL gain, but they correlate well. In practice, one might generate many candidate repeats, score them with the proxy, and only attempt full MDL encoding computation on the top N candidates. This keeps the algorithm efficient and focused on the most promising templates. If tuned correctly, proxy scores can approximate MDL enough that the final chosen set is near-optimal. Literature in pattern mining often uses similar heuristics; for instance, frequent itemset compression algorithms sort patterns by frequency * size as an initial ordering ￼. The Krimp algorithm’s greedy selection also starts from candidates ordered by a heuristic (like support × (pattern length–1)) and then validates MDL gain ￼. Wring could adopt a similar approach: rank by coverage and literal length, penalize high slot entropy, then evaluate true gain for the top-ranked patterns to make final decisions.

3. Selection Algorithms for Template Selection

Once candidate templates have been scored, the system must select an optimal subset to apply, considering overlaps and diminishing returns. Wring is essentially doing a covering of the document with template instances (for a flat model) or a parse for a hierarchical model. We compare several selection strategies:
	•	Weighted Interval Scheduling (WIS) – Optimal Non-Overlapping Selection: In the flat coverage scenario (templates produce disjoint segments of text that together cover parts of the document), the selection problem can be cast as weighted interval scheduling. Each instance of a candidate template occupies a span (interval) in the document (e.g. from character position i to j) and has a weight equal to its MDL gain (the net bits saved if that occurrence is encoded by the template rather than left raw) ￼. We then seek a set of non-overlapping instances with maximum total weight. This can be solved in $O(N \log N)$ or $O(N)$ time with dynamic programming (where N is number of candidate instances), yielding the optimal flat coverage for the given gains. Advantages: It guarantees the maximum total compression (for the given candidate set) under the no-overlap constraint. It handles overlap resolution cleanly: if two templates cover overlapping text, the DP will choose whichever combination yields higher total gain. For example, if a long template overlaps with two smaller ones covering the same region, WIS will include the long one only if its gain is greater than the sum of the two smaller gains; otherwise it will take the smaller ones. This naturally handles redundancy – no two templates covering the same part of text will both be chosen, avoiding double-counting savings. Trade-offs: A challenge in applying WIS directly is that templates come as sets of intervals (multiple occurrences). One must ensure that if a template is chosen, all or most of its occurrences are taken to realize its full gain (since template overhead was paid assuming reuse). The interval scheduling DP typically considers each interval independently, so naively one would need to model the group selection (either you take all instances of template A or none). A common heuristic is to treat each occurrence’s weight as the gain assuming the template is already defined. In practice, Wring can first decide which templates to define (using a higher-level selection, e.g. via greedy or DP on groups), then use WIS to choose the best subset of their occurrences if some occurrences could be optional. However, if we enforce that templates cover all their occurrences (for exact compression, you usually would), then the “interval” is not a single contiguous span but multiple disjoint spans. This becomes a set-packing problem rather than simple intervals. To keep it tractable, WIS can be applied if the document is segmented or if templates do not intermix occurrences heavily. For a flat, disjoint cover model, one can linearize this by pre-splitting the document at boundaries of candidate occurrences and still use DP. In summary, weighted interval scheduling is very effective for flat template selection, giving an optimal solution and working in linear/sublinear time in practice. It’s been proposed in Wring for the flat model ￼ ￼. The output quality is high in terms of compression (maximized gain), but one must be careful to integrate template costs properly into the weights (to avoid picking an instance without accounting for template overhead if it’s the only one selected – which should be disallowed or heavily negative weight).
	•	Greedy Selection (Krimp-Style MDL Greedy): Greedy algorithms build the template set iteratively, always adding the next best template (or pattern) that yields the largest remaining gain. The Krimp algorithm from pattern mining is an example: it orders candidates by a heuristic (e.g. frequency × (literal length – slots)) and then inserts them one by one if they reduce the total description length ￼ ￼. In Wring’s context, a greedy approach would:
	1.	Sort candidate templates by some gain proxy (as discussed above, or initially compute exact gain for each).
	2.	Take the top template, apply it (cover all its occurrences or as many as beneficial).
	3.	Update the state: mark those parts of text as covered (or replace them in the representation with a token referring to the template), and possibly update scores of remaining candidates (since their frequency or residual might change once some text is taken out).
	4.	Repeat: from the remaining candidates, pick the next that yields positive additional compression, until none yield a net gain.
Advantages: Greedy is straightforward and usually faster than a full DP if the number of candidates is large, because it prunes as it goes. It’s also very flexible – it can incorporate complex effects, such as choosing a template might slightly reduce another template’s frequency or remove overlaps, and the greedy process can account for that on the fly. The MDL recalculation ensures we truly improve compression at each step, avoiding overshooting (Krimp only accepts a pattern into the code table if it actually decreases total encoded length ￼). This inherently prevents negative-gain templates. Greedy selection often finds a good (though not provably optimal) set of templates that compress well. In practice, Krimp showed that MDL-driven greedy selection yields a concise, high-quality pattern set (massive reduction of patterns compared to frequency-based selection) ￼. For Wring, we expect a greedy approach to drastically reduce the number of templates relative to all candidates, focusing on the most impactful ones. Overlap & Redundancy: A greedy algorithm must handle overlaps by itself. Typically, once a template is chosen, you would remove or mark all its occurrences as covered, so overlapping candidate occurrences (from other templates) are either eliminated or their frequency reduced. Greedy inherently avoids overlapping selection because after covering text with the first template, those parts are no longer available for others. This one-pass cover can sometimes be suboptimal (maybe picking template A first blocks template B, whereas picking B first could have yielded more total gain). However, if the heuristic ordering is good (e.g., templates with large gain first), it tends to get the big fish and not regret it. Greedy can also handle diminishing returns: as it goes, the remaining uncovered text shrinks, and eventually no candidate will have positive gain. At that point, it stops naturally (diminishing returns criterion). Speed: Greedy selection can be very fast, especially if using a priority queue for candidates. The heavy part is updating candidate scores after each selection, but this can often be localized – when a template covers some text, only candidates that overlapped those regions need updating. If the input and candidate list are large, greedy might still examine thousands of candidates, but it’s polynomial and often near-linear in practice with efficient data structures. Compared to WIS, greedy doesn’t guarantee optimality – it might miss combinations of smaller templates that together beat one large template, if the heuristic ordering isn’t careful. However, many MDL pattern mining studies found that greedy (especially with iterative improvement like the Slim algorithm that can adjust after initial greedy) gets very close to optimum compression ￼. For Wring, a greedy strategy is attractive for its simplicity and its ability to incorporate the actual cost model at each step, rather than relying purely on fixed weights. It can also accommodate hierarchical selection (if we allow templates within templates) by greedily replacing parts of templates too, though that adds complexity.
	•	Other Approaches: Aside from DP and greedy, one could consider global optimization formulations:
	•	A set cover or set packing ILP: where each template is a set of positions it would cover, with a cost (negative gain as weight) and a constraint that each position can be covered at most once. This would find an optimal selection even when templates have multiple disjoint occurrences. However, this is essentially NP-hard (set packing is NP-hard), and solving it optimally for large texts is not feasible. One might use an ILP solver for small cases or to verify results, but not for browser-scale input.
	•	Hierarchical parse optimization: If a hierarchical model is used, selection becomes even more complex (templates can contain other templates). One approach is a top-down parsing: always substitute the most compressive rule anywhere it appears (like a global greedy), which is akin to how grammar compressors operate. Another approach might use a variant of CKY parsing or A* search where the “score” is negative MDL cost, to build a parse tree with maximum compression. This is heavyweight and probably not needed given the near-optimal results simpler methods yield.
	•	Krimp vs. Slim: Krimp generates a large candidate set then greedily filters. Slim (an improvement by the same authors) iteratively generates new candidates on the fly that best improve compression given the current model ￼. A similar idea for text could be: start with no templates, then iteratively find the repeat that currently yields the best gain (taking into account what’s already compressed). This is like a guided greedy search. It’s more complex but can find patterns that the initial candidate list might miss or de-prioritize.
	•	Weighted greedy by length×freq: One simple heuristic is to just sort candidates by freq * (literal_length - some_factor*slot_count) and pick greedily without updates. This is even simpler but can overshoot (it might not recalc MDL precisely). It was mentioned in Wring as a baseline heuristic ￼. It’s fast but likely to include a few suboptimal choices because it doesn’t account for overlaps well.

Comparison of Approaches: The following table summarizes key trade-offs:

Selection Method	Overlap Handling	Optimality	Speed/Complexity	Output Quality & Coverage
Weighted Interval DP	Eliminates all overlaps (disjoint intervals by design); chooses globally best non-overlapping set ￼.	Optimal for flat (given fixed gains). Might miss interactions if gains aren’t truly independent.	DP in O(n log n) or O(n). Scales to large N if N (instances) is reasonably pruned.	Maximizes total compression for flat model. Yields highest coverage for given candidates. May under-utilize a template’s all occurrences if not modeled as group.
Greedy MDL (Krimp-like)	Handles overlaps incrementally by covering text as templates are added (no two chosen templates overlap in final cover).	Approximate – not guaranteed optimal, but tends to be good if gain heuristic is accurate ￼.	Very fast in practice; iterative updates needed. Complexity depends on candidates count and overlap density (often near-linear).	Produces a compact set of high-impact templates. Usually high coverage, though possibly slightly less compression than DP optimum. Prioritizes interpretability (selects largest patterns first, avoiding numerous small ones).
Global ILP / Search	Can handle overlaps and complex interactions exactly if formulated properly.	Optimal (if solved exactly), but NP-hard.	Infeasible for large inputs (exponential). Only for theoretical analysis or tiny inputs.	N/A for practical use – would give best compression but not browser-feasible.
Hybrid / Others	E.g. Slim’s iterative generation can refine overlaps by re-checking after each addition. Overlaps managed similar to greedy (text is rewritten as compressed form stepwise).	Approximate, potentially better than one-pass greedy by refining.	Moderate – still polynomial; each iteration involves scanning data for best new pattern. Possibly slower than simple greedy if many iterations.	Could yield slightly better compression than naive greedy, with still relatively few templates. More complex implementation.

In practice, weighted interval scheduling vs. greedy is a choice between guaranteed optimal coverage vs. algorithmic simplicity and flexibility. Wring could even use WIS as a final check after greedy: use greedy to choose templates (which determines gains assuming full usage), then run an interval scheduling DP on all possible template instances to verify no different combination yields better compression. If the greedy strategy and MDL weights are well-calibrated, both methods should converge to similar results because MDL gains are mostly additive for disjoint segments. Weighted scheduling is naturally limited to the flat, non-overlapping case. If Wring adopts a hierarchical model (templates can nest), selection becomes more complex and a greedy or grammar-growth approach is the typical solution (similar to how Sequitur always takes the first available repeat). For browser-scale inputs, the greedy approach might be more practical since it can continuously produce intermediate results and doesn’t require storing a huge DP table if there are extremely many small intervals. Meanwhile, weighted scheduling can handle a large number of intervals efficiently in C++/WASM, but implementing it in JS for thousands of intervals is also fine (a few thousand intervals DP is negligible for modern hardware). Therefore, the decision may hinge on ease of respecting template-level costs: greedy naturally handles the template overhead by checking total MDL after adding, whereas WIS would need a preprocessing to include template overhead into instance weights or separate template selection phase.

Redundancy & Diminishing Returns: Both approaches incorporate diminishing returns in different ways. In DP, any template with negative or zero weight simply won’t be chosen, and once high-weight intervals are picked, remaining intervals are by definition lower weight, so total gain added eventually drops off – we can detect that and stop. In greedy, one can monitor the gain slope: if the best next template yields very little improvement (below a threshold), that can be a stopping criterion (see Section 4). Redundancy (two templates covering similar text in different ways) is resolved by selection: ultimately only one will survive if they truly overlap. If two templates are partially overlapping in some occurrences but also each has some unique occurrences, the selection algorithm might choose both but assign them to disjoint subsets of occurrences. For instance, Template A and Template B might share some potential instance positions – a greedy algorithm might choose A and then, for the overlapping positions, not use B, but B could still be used in other places where it doesn’t conflict. This is a nuanced scenario that WIS doesn’t naturally handle (WIS would force a binary choice per interval), but a greedy or custom selection can allow that flexibility: effectively, the overlap is resolved on a per-occurrence basis. This leads into partial matches/outliers, discussed next.

4. Termination Criteria and Handling Partial Matches

Even with a good selection algorithm, Wring needs clear termination criteria to decide when to stop discovering templates, and policies for dealing with “almost” patterns that aren’t perfectly consistent across all occurrences:
	•	Stopping Conditions: An MDL-driven process can, in theory, keep adding templates as long as there is any tiny gain. However, for practicality and interpretability, we define cut-offs. Common and interpretable criteria include:
	•	Coverage Threshold: Stop when a certain percentage of the document is covered by templates (i.e. when residual uncovered text falls below a threshold). For example, if 95% of the text is now explained by templates and only 5% remains residual, further compression may not be worth the added complexity. Coverage is easy to measure and ensures diminishing returns (each new template would now address a very small portion).
	•	Gain Slope Threshold: Monitor the compression gain of each newly added template (in greedy selection) or overall gain improvement (in DP, consider the weight of the last interval added). If the next best template would contribute less than a small threshold (e.g. <0.1% compression improvement, or its gain in bits is below some constant), then deem it not worth adding. Essentially, when the marginal gain drops off, stop. This is analogous to an “elbow” in the MDL curve: as soon as templates start providing negligible benefit, further templates likely just complicate the model.
	•	Template Count Limit: Impose an absolute cap on the number of templates (say, at most N templates). This could be a user-tunable parameter reflecting how many patterns they want to see at most. It ensures interpretability (users can only digest so many templates). If the process hits the limit, it stops even if some compression opportunities remain. Ideally, this is used in conjunction with a gain threshold – so we don’t stop too early if there are still big gains, but we also don’t produce hundreds of tiny templates.
	•	In practice, Wring might use a combination: e.g., stop when either coverage > X% or gain from next template < Y or templates count = N. These criteria together guard against both underfitting and overfitting. Coverage ensures we don’t terminate with large areas unexplained, gain slope ensures we don’t pursue minuscule improvements, and count prevents a flood of templates.
	•	Additionally, a sanity condition is round-trip reconstruction must hold at termination (which it will if we always ensure templates plus residual cover the whole text exactly). If any uncovered bits remain, they are by definition residual – we don’t need to parse further if all uncovered parts are deemed residual by choice.
	•	Partial Matches (Near-Miss Patterns): Often, in real data, a repeated structure is almost consistent, but has a few outlier instances or minor variations. Wring must decide how strictly a template is defined. Exact matching (as in grammar compression) means a template’s every occurrence must have identical literal parts. This yields clean templates but can fragment what humans might view as “the same pattern.” Options to handle near-matches:
	•	Strict Residuals: The simplest approach is to be strict: if an occurrence deviates in any way, do not include it in the template. It remains (or becomes) part of the residual. For example, if 19 log lines match a template exactly and the 20th line has one extra word, the strict approach would either leave that 20th as completely residual or perhaps form a separate single-use template (which MDL would likely reject). The benefit is that templates remain perfectly consistent and reconstruction is trivial. The downside is loss of compression (that 20th line might share a lot with the template except one word, but we’re not capturing that commonality).
	•	Variant Templates: A compromise is to allow multiple versions of a template to capture variations. In the example, we might induce two templates: one for the 19 lines, and a slightly different one for the single variant. This acknowledges the difference explicitly. It’s akin to clustering the pattern into sub-clusters. The advantage is each variant is still internally consistent (exact for its subgroup). The disadvantage is you now have two templates that are largely similar, which might be less interpretable (could we convey that they’re related? Perhaps not easily in a flat list of templates). MDL will permit variant templates if the cost of a second template is outweighed by compressing that outlier – usually, for a single outlier, it’s not (likely residual is cheaper), but if there are a few occurrences of each variant, two templates might be reasonable.
	•	Fuzzy Matching / Slots for Differences: The most aggressive approach is to allow a template to absorb small differences by treating them as part of slots or optional literals. This is essentially how some log parsers work: e.g., Spell merges messages with minor differences by inserting wildcards (*) in the template ￼. For instance, if one instance has an extra token, the template could have an optional slot that is empty in other instances and carries that token in one instance. However, allowing optional content or fuzzy slots breaks the strict context-free grammar model (it introduces alternatives or regex-like flexibility). Exact reconstruction becomes trickier: you need a scheme to encode whether the optional part is present or not for each instance. It can still be exact if done carefully, but it complicates the template language. Spell algorithm example: It uses longest common subsequence to merge two log strings, marking positions where they differ as wildcards ￼. This effectively yields a template with * in places of differences and can handle an extra token as a * in one instance. Spell requires that the common subsequence (LCS) is at least a certain length (50% of the message) to merge two logs ￼, ensuring they are mostly similar. This threshold-based fuzzy merging is a policy Wring could emulate (maybe with a higher threshold like 90% for general text, since log lines are often highly structured). If an outlier instance is, say, 90% similar to a template, the system could “force-fit” it by treating the differing 10% as part of a slot value or an optional segment ￼. MDL can guide this: if including the outlier requires adding an optional literal of a few tokens to the template (increasing template cost slightly) but saves re-encoding the other 90% of the structure, it might still yield net gain.
	•	Outlier Policies in Prior Work: In structured log parsing, common practice is to use a similarity threshold: if a new log line doesn’t match any existing template within threshold, start a new template for it (treat it as a new pattern) ￼. This means true outliers each get their own template (often effectively one-off, which is like residual). Some systems designate a special “Miscellaneous” category for logs that don’t fit any pattern well. From an MDL perspective, that’s just residual by another name. On the other hand, if a nearly-matching log does fit except for one token, Spell would merge it, thereby tolerating minor variations as parameters. This reduces the number of templates (which is good for generalization), but at risk of making templates a bit less precise.
	•	In grammar induction research, strictness is usually maintained (a rule either applies or not). However, there is the concept of error-correcting grammars or allowing a small fraction of mismatches to still be covered by a rule, often for robustness. For example, some grammar inference for bio-sequences allows a few mutations. Those approaches often keep track of errors separately (which could correspond to a residual override for that instance).
	•	Recommended Approach (Outliers in Wring): Given Wring’s goal of exact round-trip reconstruction and interpretability, the likely approach is mostly strict, with explicit handling of outliers. Specifically:
	•	If a template covers most instances of a pattern and a few instances differ slightly, Wring could either (a) issue a variant template for the deviants, or (b) include them anyway and note them as “dirty” instances. The README suggests deciding whether to “force-fit as dirty instance or keep separate” ￼. Force-fitting would mean we allow an instance that doesn’t 100% match the template skeleton by treating its deviations as part of slot content, even if that slot content for other instances is empty or different. This maintains one template, but we must accept that one instance’s slot might contain something that in other instances is actually a literal part of the template. That indeed breaks the assumption that the template’s literal part is constant across all instances. So implementing that is complex.
	•	More straightforward is (b): keep them separate (as residual or their own template). If the outlier count is very low, it’s probably fine to leave them as residual – the compression gain from covering them is minimal and might not justify an extra template.
	•	If outliers are a bit more frequent, variant templates make sense: e.g., Template A covers 10 instances, Template A’ (variant) covers 3 slightly different instances. The two share a lot of literal text but we don’t unify them in one template; we present them as separate patterns. This is similar to how a log parser might output two templates that are very similar except one token. It’s then up to the user (or a post-processor) to notice the similarity. But from a compression standpoint, this is slightly suboptimal because you defined two templates with mostly duplicate literals. However, MDL might actually penalize that – it might have been cheaper to have one template with an optional part. To reconcile that, Wring could allow merging those templates if the cost of combining is lower.
In prior work like Drain (a log parser with fixed depth tree), they treat each distinct structure as separate – no fuzzy merging beyond a point. Spell is more flexible with LCS. There’s also LogSig which uses clustering and can tolerate a wildcard token in positions that frequently change. Those algorithms implicitly handle partial matches by clustering similar messages together and using wildcards for inconsistent tokens.
	•	Residual Classification: After template induction, whatever text remains untemplated is the residual. We should analyze it: are these purely random fragments or do they indicate patterns we failed to capture? Prior work in log analysis often expects some portion of logs might be noise or unique messages. Wring should categorize residual portions:
	•	If the residual is scattered noise (e.g., truly random IDs, or one-off sentences), that’s fine – it’s just output as-is.
	•	If the residual contains larger chunks that look structured or repeated, then our algorithm missed those patterns, perhaps due to too strict thresholds or because they were “outliers” to a pattern. For example, suppose 90% of lines followed template A, and 10% were slightly different and left as residual. That residual 10% is actually a recurring pattern of its own (just low frequency). In such a case, outlier promotion might be considered: promote that residual pattern to an official template, even if it’s low frequency, as long as it appears multiple times. This was hinted at: “if a residual is 90% similar to existing template, force-fit as dirty instance or keep separate?” ￼. Alternatively: if a particular residual substring appears, say, 3+ times, maybe it should be a template. Ideally, our initial mining wouldn’t miss any substring that repeats 3+ times (since we enumerate repeats). But if those repeats were filtered out (maybe because they were considered part of a bigger pattern that got partial coverage), a second pass on residual could catch them.
	•	Policies from grammar induction: typically, grammar compressors handle everything in one go (no concept of residual versus templates – everything is explained by the grammar). In Wring, residual is essentially the uncompressed remainder. Perhaps a strategy is to run a second compression pass on the residual alone (i.e. treat the residual text as its own input to find if it has any internal repetition worth templating). This could net a bit more compression and identify if residual segments themselves have structure. But if residuals are mostly one-offs, a second pass will find nothing significant.
	•	Interpretable Stopping: It’s useful if the stopping criteria can be explained: e.g., “We stopped because the next best template would only save 20 bytes, which is below our threshold,” or “We have covered 98% of the document; the rest appears to be unique content.” This gives the user confidence that the algorithm didn’t just arbitrarily stop.

In summary, Wring should stop when it has captured the majority of meaningful repetition and additional templates would yield diminishing returns or be too specific. A combination of coverage and gain threshold is a robust way to do this. For partial matches, a conservative approach is recommended initially: use strict templates and leave outliers as residual or separate templates. This keeps the model simple and exact. If needed, one can introduce a post-processing step that examines residuals for near-misses and either merges them with existing templates (if within a similarity tolerance) or elevates them to their own templates if they recur. This two-stage approach (strict first, then handle outliers) is easier to manage than trying to build fuzzy matching into the core algorithm from the start. It’s also in line with how some log parsers operate: first cluster by exact structure, then possibly merge clusters that are similar. By sourcing strategies from log parsing literature, we know that a moderate amount of controlled fuzziness (like allowing one wildcard token if it consistently differs) can greatly reduce the template count without much loss of accuracy ￼. The key is to incorporate that only when beneficial: MDL can actually decide it – if combining two variants into one template (with an optional slot) yields a shorter overall description length than keeping them separate, then it’s the preferred model. If not, keep them separate. Thus MDL itself can guide partial match handling, as long as our model is flexible enough to represent that merging.

Finally, whatever residual remains, Wring could label it clearly in output as, say, “Residual (not covered by any template)”. If the residual segments are few and large, that might indicate some missed pattern (or genuinely unique content). If they are many tiny bits, it likely indicates fine-grained differences that weren’t worth templating. Prior work on outlier detection (e.g. identifying the “odd one out” in data compression ￼) suggests that anomalies can be spotted as those parts of data that resist compression by the learned model. Wring’s residual essentially flags those anomalies or unique bits, which might themselves be of interest to a user (for example, unusual log lines that did not fit any template could indicate novel events). Thus, handling partial matches and residuals isn’t just a technical detail – it can be turned into a feature: templates cover the routine structure, and residuals highlight the exceptions.

References:
	•	Sequitur grammar induction and rule constraints ￼ ￼
	•	Re-Pair compression strategy ￼
	•	MDL and pattern selection (Krimp algorithm) ￼ ￼
	•	Wring project notes on selection algorithms and costs ￼ ￼
	•	Spell log parser (LCS-based merging for partial matches) ￼ ￼