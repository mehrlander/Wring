Operationalizing Minimum Description Length for Pattern Mining: A Comprehensive Framework for Objective Scoring and Selection Regimes
1. Introduction: The Imperative for Information-Theoretic Rigor in Log Analysis
The exponential growth of machine-generated data has transformed system logs from ephemeral diagnostic artifacts into critical assets for operational intelligence, security auditing, and anomaly detection. However, the unstructured nature of these logs—typically semi-structured text generated by printf statements—presents a fundamental barrier to automated analysis. The task of Log Parsing or Template Induction aims to reverse-engineer the generation process, extracting structured templates (e.g., Connection from <IP> accepted) and dynamic parameters (e.g., 192.168.1.5) from raw text streams.
While early approaches relied on regular expressions and heuristic clustering, these methods suffer from fragility; they require extensive parameter tuning (e.g., similarity thresholds, tree depths) and often fail to generalize across diverse system environments. The Minimum Description Length (MDL) principle offers a robust, statistically grounded alternative. By equating "learning" with "compression," MDL posits that the best set of templates is the one that allows for the most compact lossless encoding of the raw log data.
However, the theoretical elegance of MDL often clashes with the harsh realities of implementation. "Ideal MDL" relies on non-computable Kolmogorov Complexity, necessitating the use of "Practical MDL" or "Two-Part Code" approximations. Furthermore, the application of MDL to log data requires specific engineering decisions regarding cost modeling for heterogeneous data types (integers, timestamps, strings), efficient algorithms for selecting optimal templates from a combinatorial search space, and robust criteria for handling partial matches and residuals.
This report articulates a comprehensive, expert-level framework for operationalizing MDL in the context of log pattern mining. It synthesizes findings from algorithmic information theory, combinatorial optimization, and contemporary log parsing research (including analyses of algorithms like Krimp, Slim, Spell, and Drain) to define a rigorous scoring and selection regime. We demonstrate that by rigorously modeling the "physics of information"—specifically through the use of Universal Integer Codes and Weighted Interval Scheduling—we can construct a parameter-free, self-regulating parsing system that maximizes signal extraction while naturally identifying anomalies.
2. The MDL-Style Objective: From Philosophy to Functions
The core of any MDL-based system is the objective function. This function serves as the arbiter of quality, determining whether a proposed template represents a genuine structural regularity or merely a coincidental overlap. Unlike heuristic methods that maximize "similarity," MDL minimizes "redundancy."
2.1 The Two-Part Code Formulation (Crude MDL)
In practical pattern mining, we adopt the Two-Part Code formulation (also known as Crude MDL). This approach decomposes the total description length into two distinct components: the cost of the model (the hypothesis) and the cost of the data given the model.
Formally, given a dataset D (the raw log stream) and a set of candidate hypotheses \mathcal{H} (collections of templates), we seek the specific hypothesis H \in \mathcal{H} that minimizes the total description length L(D, H):
This equation embodies the principle of Occam's Razor.
	•	L(H) (The Model Cost): This term measures the complexity of the template set itself. A hypothesis containing a distinct template for every unique log line would have a massive L(H), as the "dictionary" would be as large as the data. This penalizes overfitting.
	•	L(D|H) (The Data Cost): This term measures the residual information required to reproduce the original dataset using the templates in H. A hypothesis consisting of a single generic wildcard template (<*>) would have a negligible L(H) but an enormous L(D|H), as every token in the log would need to be encoded literally as a parameter. This penalizes underfitting.
The optimization process, therefore, searches for the "sweet spot" where the sum of these two costs is minimized, identifying patterns that capture statistically significant regularities.
2.2 Decomposition of the Data Cost
The term L(D|H) is not a monolithic value; it is the sum of the encoding costs for each log entry. Understanding this decomposition is crucial for implementing a scoring engine. If a log stream D consists of a sequence of lines \{l_1, l_2,..., l_n\}, and the hypothesis H (the Code Table) contains a set of templates CT, the data cost is:
When a specific log line l_i is "covered" or "explained" by a template t \in CT, the encoding cost L(l_i | t) comprises two parts:
	1	Code Identification Cost: The cost to specify which template from the table is being used.
	2	Parameter Encoding Cost: The cost to encode the dynamic values (residuals) that fill the template's variable slots.
2.2.1 The Code Identification Cost
To minimize the expected description length, the codes assigned to templates must be optimal prefix codes (e.g., Huffman or Shannon-Fano codes). Information theory dictates that the length of the code for a template t should be inversely proportional to the logarithm of its usage probability (relative frequency).
Insight: This logarithmic cost creates a "rich get richer" dynamic during the selection process. A template that covers many log lines becomes "cheaper" to use (shorter code length), which increases its Compression Gain, making it more likely to be selected for future lines. This naturally biases the system toward discovering dominant, high-frequency behaviors—the "laws" of the system—while relegating rare events to longer codes.
2.2.2 The Parameter Encoding Cost
The second component of the data cost is the encoding of variables. If template t has slots \{v_1, v_2,..., v_k\}, the parameter cost is the sum of the description lengths of the values found in those slots:
This requires rigorous Cost Modeling for different data types (integers, dates, strings), which is detailed in Section 3. The accuracy of these cost models effectively determines the "resolution" of the pattern mining. If the cost model for integers is too expensive, the system will refuse to parameterize numbers, resulting in distinct templates for Error 404 and Error 500. If the cost is efficient (e.g., using Elias codes), the system will correctly merge them into Error <INT>.
2.3 The Compression Gain Metric
To drive the selection algorithms (discussed in Section 4), we must quantify the marginal benefit of adding a candidate template to our hypothesis. We define the Compression Gain (\Delta L) of a candidate pattern X as the reduction in total bits achieved by including X in the Code Table.
This can be expanded to:
Typically, the model cost increases when a new template is added (L(CT) < L(CT')), so the first term is negative. The selection is driven by the second term: the massive reduction in data description length achieved by replacing long literal strings with short template codes. A candidate is accepted only if \Delta L(X) > 0.
2.4 Refining the Objective: The Normalized Maximum Likelihood (NML)
While Two-Part MDL is the standard for engineering implementations due to its computational feasibility, advanced theoretical treatments suggest the Normalized Maximum Likelihood (NML) or "Refined MDL" as a superior objective. NML avoids the arbitrary separation of "model" and "data" encodings, instead evaluating the complexity of the model class as a whole (the "stochastic complexity").
In the context of log parsing, NML provides a theoretical justification for handling the "parametric complexity" of variable slots. It suggests that the cost of a variable slot should not just be the bit depth of the integer, but should include a term related to the curvature of the parameter space—essentially, how sensitive the likelihood is to changes in the parameter. While fully implementing NML is computationally prohibitive for high-throughput streaming logs, the insight informs our cost modeling: variable slots that exhibit high variance (high parametric complexity) should be penalized more heavily than those with low variance.
3. Cost Modeling and Calibration: The Physics of Information
The abstract MDL objective must be grounded in concrete cost functions. The statement "encode the parameters" is insufficient; we must define how to encode them to reflect their true information content. This section details the "physics" of information for the specific data types found in logs: integers, timestamps, and strings.
3.1 Integer Encoding: The Necessity of Universal Codes
Log files are replete with integers: process IDs, port numbers, error codes, line numbers, and status flags. A naive approach might cost every integer as 32 bits (4 bytes). This is disastrous for MDL pattern mining.
	1	Inefficiency: Most integers in logs are small (e.g., error codes 0, 1, 404). Encoding 1 as 32 bits is a massive waste of "description length."
	2	Lack of Discrimination: If all integers cost 32 bits, the system cannot distinguish between a highly entropic random seed and a low-entropy enum.
To accurately model integer costs, we must employ Universal Integer Codes, specifically Elias Gamma (\gamma) and Elias Delta (\delta) coding.
3.1.1 Elias Gamma Coding
Elias Gamma coding is efficient for small, positive integers. It represents an integer x by a unary prefix indicating the magnitude, followed by the binary representation. The length of the code L_\gamma(x) is:
	•	Example: x=9 (binary 1001). Magnitude is 3 bits. Prefix is 000. Code is 0001001 (7 bits).
	•	Application: Ideal for small counts, error codes, and low-magnitude identifiers.
3.1.2 Elias Delta Coding
For larger integers, Elias Gamma becomes inefficient because the unary prefix grows linearly. Elias Delta improves this by encoding the length of the binary representation using Gamma coding. The length L_\delta(x) is:
	•	Application: Ideal for ProcessID, ThreadID, memory offsets, and other potentially large values. It is asymptotically optimal for uniform distributions over large ranges.
3.1.3 Impact on Template Selection
By using Elias codes in the cost function, the selection algorithm naturally adapts to the data distribution.
	•	Scenario A: A log contains Error <CODE> where <CODE> is always between 1 and 5. The Elias Gamma cost is tiny (~3 bits). The system accepts the template Error <INT>.
	•	Scenario B: A log contains Session <ID> where <ID> is a uniform random 64-bit integer. The Elias Delta cost is high (~76 bits). The compression gain of the template Session <INT> is reduced. If the "Session" keyword is short, the system might decide that the template doesn't offer enough compression over the literal string, effectively identifying the line as high-entropy noise (or "Residual") rather than a pattern.
3.2 Date and Time Encoding: Exploiting Sequential Correlation
Timestamps are typically the first field in any log line. Encoding them as strings (e.g., ISO-8601 2023-10-27T10:00:00.000Z) costs roughly 24 bytes (192 bits). This is prohibitively expensive and dominates the description length, obscuring the patterns in the actual message.
However, logs are sequential. The timestamp t_i is highly correlated with t_{i-1}. We exploit this using Delta Encoding combined with Universal Integer Codes.
	•	Mechanism: We calculate the difference \Delta t in milliseconds. For high-frequency logs, \Delta t is often small (0 to 100ms).
	•	Compression: An Elias Gamma code for \Delta t = 50 takes only 11 bits. This is a ~95% reduction compared to the raw string.
	•	Structural Implication: This massive compression is only possible if the parser correctly identifies the timestamp field. Therefore, the MDL objective function powerfully incentivizes the system to isolate timestamps into a dedicated slot, separating them from the static template text. This naturally solves the "header parsing" problem without explicit rules.
3.3 String and Text Encoding: Entropy and Dictionaries
Variable string parameters (e.g., user names, file paths, URLs) pose a challenge. Unlike integers, they do not have a natural magnitude.
3.3.1 Length-Prefix Encoding
The baseline approach models a string s as a length followed by raw characters.
Where C_{char} is the cost per character. Assuming standard ASCII entropy, C_{char} \approx 5\text{--}6 bits (derived from empirical entropy of log text) rather than the full 8 bits.
3.3.2 Dictionary Encoding for Low Cardinality
Often, a "variable" field actually iterates through a small set of values (e.g., LogLevel is INFO, WARN, ERROR). The MDL regime should detect this.
	•	Dynamic Dictionary: The system can maintain a dictionary \mathcal{D} for a specific template slot.
	•	Cost Switch: If the cost of encoding the dictionary L(\mathcal{D}) plus the cost of indices \sum -\log P(index) is less than the cost of literal string encoding, the system switches to Dictionary Mode.
	•	Insight: This allows the system to discover "Enums" automatically. The cost function acts as a type inference engine, upgrading a <STRING> slot to an <ENUM> slot purely based on bit savings.
3.4 Calibration Factors
While pure MDL is parameter-free, practical implementations often require a calibration factor \alpha (often denoted as \lambda in regularization contexts) to manage the finite-sample behaviors.
	•	\alpha = 1: The theoretical ideal (Pure MDL).
	•	\alpha > 1: A "Conservative" regime. This increases the penalty for model complexity (adding new templates). It forces the system to generalize more aggresively, preferring fewer, broader templates. This is useful for noisy logs where overfitting is a risk.
	•	\alpha < 1: A "Precise" regime. This lowers the barrier for creating new templates. It is useful in security contexts where distinguishing User Admin from User Root is critical, even if they look structurally identical.
Table 1: Recommended Cost Models by Data Type
Data Type
Cost Model Strategy
Formula
Application Context
Small Integers
Elias Gamma
2\lfloor \log_2(x) \rfloor + 1
Error codes, Counts, HTTP Status, Line Numbers
Large Integers
Elias Delta
\approx \log x + 2\log \log x
Process IDs, Thread IDs, Memory Addresses, Offsets
Timestamps
Delta Time + Elias
L_\delta(t_i - t_{i-1})
Log Headers (ISO-8601, Unix Epoch)
Strings (General)
Length-Prefix + Entropy
$L_\delta(
s
Strings (Enum)
Dictionary Index
-\log_2 P(token)
Log Levels (INFO, WARN), Component Names, Status
Template ID
Huffman/Shannon
-\log_2 P(template)
Identifying the template in the Code Table
4. Selection Algorithms: Solving the Combinatorial Puzzle
Defining the objective function is only half the battle. The search space for templates is combinatorial; a log file can be partitioned in exponentially many ways. We need an efficient Selection Regime to find the hypothesis H that minimizes the objective.
4.1 Candidate Generation Strategies
Before we can select the best templates, we must propose candidates. Since we cannot iterate all possibilities, we rely on heuristic generators that feed the MDL selector.
	1	LCS-Based (Spell-like): This strategy computes the Longest Common Subsequence (LCS) between the incoming log line and a buffer of recent lines. The LCS (e.g., User * logged in) becomes a candidate template. This is highly effective for discovering the static "backbone" of log messages.
	2	Frequent Subsequence Mining (Slim-like): This approach identifies frequent itemsets (tokens that appear together often) and merges them into candidates. It is an "assembled" approach, building templates bottom-up from atomic words.
	3	Heuristic Clustering (Drain/LogCluster): These algorithms partition logs into buckets based on length and first token. The "centroid" or consensus string of each bucket is proposed as a candidate template. While Drain itself doesn't use MDL, its output serves as excellent candidate input for an MDL selector.
4.2 Selection as Weighted Interval Scheduling (WIS)
A profound insight in MDL pattern mining is mapping the selection problem to the Weighted Interval Scheduling (WIS) problem.
	•	The Analogy:
	•	Time/Interval: The sequence of tokens in a log line (or the sequence of lines in a file) can be viewed as a timeline.
	•	Job: A candidate template t is a "job" that covers a specific range of this timeline (a set of tokens or lines).
	•	Weight: The weight of the job is the Compression Gain \Delta L(t) provided by that template.
	•	Conflict: Two templates conflict if they claim the same data (tokens/lines). We cannot use two different templates to explain the exact same log entry.
	•	The Goal: Select a subset of non-conflicting templates that maximizes the total Compression Gain (Weight).
4.2.1 The Dynamic Programming Solution
For a linear sequence (e.g., selecting templates to cover a single complex log line, or selecting a set of non-overlapping templates for a batch of logs), the optimal solution can be found using Dynamic Programming (DP).
Let Candidates = \{c_1, c_2,..., c_n\} be sorted by their endpoint (e.g., the last log line they cover). Let w_i be the compression gain of candidate c_i. Let p(i) be the index of the last candidate c_j that is compatible (non-overlapping) with c_i.
The recurrence relation for the optimal score OPT(i) is:
	•	Option 1: Select candidate c_i. We gain its weight w_i, plus the optimal solution for the remaining data up to p(i).
	•	Option 2: Do not select candidate c_i. The score is the same as the optimal solution for the first i-1 candidates.
This algorithm guarantees the selection of the subset of templates that maximizes global compression for the covered region in O(N \log N) time.
4.3 The Greedy-Cover Strategy (Practical Implementation)
While WIS provides an optimal solution for 1D coverage, log template dependencies can be complex (e.g., Template A overlaps with Template B, which overlaps with Template C). For large-scale mining, a Greedy-Cover strategy with Post-Pruning (as used in Krimp and Slim) is the standard engineering choice.
The Algorithm:
	1	Gain Estimation: Calculate the estimated \Delta L for all candidate templates.
	2	Sorting: Sort candidates in descending order of Gain (The "Standard Candidate Order").
	3	Greedy Acceptance: Iterate through the list.
	•	Tentatively add candidate t to the Code Table.
	•	Cover the log data using the new table. (This resolves conflicts: the higher-gain template "eats" the data first).
	•	Calculate the actual global description length L(D, CT_{new}).
	•	Accept if L(D, CT_{new}) < L(D, CT_{old}). Otherwise, Reject.
	4	Pruning: After accepting a new template, existing templates in the table may lose their support (their data was "stolen" by the better template). Check all templates in reverse order. If a template no longer contributes a positive gain (i.e., its model cost > the data cost it saves), delete it from the table.
Insight: This Greedy-Update-Prune cycle approximates the global optimum efficiently. It avoids the combinatorial explosion by making locally optimal decisions that are iteratively refined.
5. Handling Partial Matches and Fuzzy Matching
In real-world logs, templates are rarely rigid. Variations occur (e.g., Disk 1 failure vs. Disk 2 failed). A strict matching regime would create two separate templates. MDL encourages merging them if the cost of describing the difference is low. This leads to the concept of Partial Matches handled via Edit Distance Costs.
5.1 The Edit Cost Model
Instead of a binary "Match/No Match," we define the cost of encoding a log line l given a template t as:
	•	L_{edit}(t \to l): The cost to transform the template string into the log line. This is typically modeled using the Levenshtein Distance (number of insertions, deletions, substitutions).
	•	Encoding Edits: Each edit operation must be encoded. For example, "Substitute character at index 5 with 'a'" costs bits.
5.2 The Distortion Penalty and Thresholding
Fuzzy matching introduces a risk: one could theoretically match any log line to any template by just listing enough edits. This would degenerate the model. MDL naturally prevents this via the Distortion Penalty.
We accept a partial match only if:
If the cost of describing the edits (L_{edit}) plus the template reference (L_{code}) exceeds the cost of just writing the log line literally (L_{literal}), the MDL objective rejects the match.
	•	Self-Calibration: This eliminates the need for an arbitrary "80% similarity" threshold. The threshold is dynamic: a partial match is valid only if it compresses the data. If the "noise" (edits) is too high, compression is lost, and the match is rejected.
5.3 Regex Induction as a Cost Problem
Partial matching leads to Regex Induction. If multiple log lines partially match a template with consistent variations, MDL favors upgrading the template to a Regular Expression.
	•	Cost of Regex: A regex User (Admin|Root) is more complex than a string User Admin. It has a higher Model Cost L(H).
	•	Benefit of Regex: It covers more data lines without needing expensive edit operations.
	•	Selection: The system selects the regex form if the reduction in Data Cost (by avoiding edit codes or literal encoding) outweighs the increase in Model Cost (for storing the complex regex syntax).
6. Residual Classification and Anomaly Detection
A distinct advantage of the MDL framework is that "failures" to match are informative. Data that cannot be compressed is, by definition, high-entropy or anomalous relative to the model.
6.1 Classification of Residuals
We classify any log line that is not covered by the Code Table (i.e., encoded literally) as a Residual.
	•	Universal Outliers: These are lines that are fundamentally incompressible (random noise) or represent a completely new, unseen behavior (concept drift).
	•	Parameter Outliers: These are lines that match a template but have variable values with anomalously high costs (e.g., an integer slot that normally holds 0-10 suddenly holding 2^64). The Elias Delta cost function naturally flags these as "expensive" events.
6.2 Anomaly Scoring via Compression Ratio
MDL provides a unified metric for anomaly detection: the Compression Ratio or Bits Per Character (BPC).
	•	Normal Log: High compression (Score \ll 1.0). The line is predictable.
	•	Anomaly: Low compression (Score \approx 1.0). The line is surprising to the model. This score provides a normalized, mathematically grounded way to detect anomalies without setting manual thresholds for every log type.
7. Termination Criteria
When should the mining process stop? Heuristic algorithms often run for a fixed number of iterations. MDL provides intrinsic stopping criteria based on information gain.
7.1 The No-Gain Condition (Global Minimum)
The theoretically optimal stopping point is when no possible candidate template yields a positive Compression Gain (\Delta L > 0).
At this point, adding any further structure to the model would cost more bits than it saves in data encoding. The model has found the minimum description length.
7.2 Diminishing Returns (Relative Threshold)
In practice, finding the exact minimum is computationally expensive for diminishing returns. A Relative Gain Threshold \epsilon is used:
If the best candidate improves the total compression by less than, say, 0.01%, the process terminates. This prevents the algorithm from overfitting to insignificant noise or spending cycles optimizing rare outliers.
7.3 Code Table Stability
For iterative refinement algorithms (like Krimp), termination can be triggered when the Code Table becomes Stable. If a full pass over the data results in no templates being added, removed, or significantly altered (e.g., no promotion of variables to constants), the algorithm has reached a local optimum (a "fixed point" in the search space).
8. Conclusion and Strategic Recommendations
The transition from heuristic log parsing to MDL-based pattern mining represents a shift from "art" to "science." By operationalizing the Minimum Description Length principle, we replace fragile parameters with a unified objective: Minimize the total information cost of the system.
Key Takeaways for Implementation:
	1	Adopt Universal Coding: Reject fixed-width integers. Implement Elias Gamma/Delta and Delta-Time encoding to accurately model the "physics" of log data. This is the single highest-impact change for scoring accuracy.
	2	Use Gain-Based Selection: Replace similarity thresholds with Compression Gain. Use Weighted Interval Scheduling (WIS) logic or Greedy-Cover algorithms to resolve conflicts between overlapping templates.
	3	Leverage Residuals: Do not discard residuals. Use the compression score as a built-in anomaly detector. High-cost logs are naturally suspicious.
	4	Calibrate via Cost, Not Rules: If the system is underfitting (too many wildcards), decrease the cost of model structure (\alpha < 1). If it is overfitting (too many specific templates), increase the cost of structure or use "fuzzy" edit-distance penalties.
By adhering to this regime, practitioners can build log analysis systems that are robust, self-tuning, and mathematically guaranteed to capture the most significant structures in their data.
9. Mathematical Appendix: The Practical Cost Function Summary
For engineering reference, the recommended global objective function for a log stream D and hypothesis H is:
Variable Cost Lookup:
	•	Integer x: L_\delta(x) \approx \log x + 2\log \log x
	•	Time \Delta t: L_\gamma(\Delta t) = 2\lfloor \log_2(\Delta t) \rfloor + 1
	•	String s: L_\delta(|s|) + |s| \cdot 5.5 (approx entropy)
	•	Outlier/Residual: |l| \cdot 8 bits (Literal cost)
Works cited
1. Drain — logparser 0.1 documentation, https://logparser.readthedocs.io/en/latest/tools/Drain.html 2. The Minimum Description Length Principle in Coding and Modeling - Information Theory, IEEE Transactions on - Yale Statistics and Data Science, http://www.stat.yale.edu/~arb4/publications_files/TheMinimumDescriptionLengthPrincipleInCodingAndModelingIEEEIT.pdf 3. A Tutorial Introduction to the Minimum Description Length Principle - CWI, https://homepages.cwi.nl/~pdg/ftp/mdlintro.pdf 4. Minimum description length - Wikipedia, https://en.wikipedia.org/wiki/Minimum_description_length 5. Lecture 13: Minimum Description Length, https://www.cs.cmu.edu/~aarti/Class/10704/lec13-MDL.pdf 6. [cs/9901014] Minimum Description Length Induction, Bayesianism, and Kolmogorov Complexity - arXiv, https://arxiv.org/abs/cs/9901014 7. Slim: Directly Mining Descriptive Patterns - Exploratory Data Analysis, https://eda.rg.cispa.io/pubs/2012/slim-smets,vreeken.pdf 8. A New MDL Measure for Robust Rule Induction (Extended Abstract) - SciSpace, https://scispace.com/pdf/a-new-mdl-measure-for-robust-rule-induction-extended-50n3uq37av.pdf 9. The minimum description length principle for pattern mining: a survey, https://d-nb.info/1266171525/34 10. Krimp | Project - Jilles Vreeken, https://vreeken.eu/prj/krimp/ 11. Choosing the Right Data Types in Big Data Systems | by Huzefa Khan - Medium, https://huzzefakhan.medium.com/choosing-the-right-data-types-in-big-data-systems-875f99c28ad5 12. On the Feasibility of Parser-based Log Compression in Large-Scale Cloud Systems | USENIX, https://www.usenix.org/system/files/fast21-wei.pdf 13. Hypothesis Selection and Testing by the MDL Principle | OUP Journals & Magazine, https://ieeexplore.ieee.org/document/8138703/ 14. Model selection by normalized maximum likelihood - ResearchGate, https://www.researchgate.net/publication/222573666_Model_selection_by_normalized_maximum_likelihood 15. Elias Coding - Instituto de Telecomunicações, https://www.lx.it.pt/~mtf/Elias.pdf 16. Universal code (data compression) - Wikipedia, https://en.wikipedia.org/wiki/Universal_code_(data_compression) 17. Elias gamma coding - Wikipedia, https://en.wikipedia.org/wiki/Elias_gamma_coding 18. Integer encoding - DidaWiki, https://didawiki.cli.di.unipi.it/lib/exe/fetch.php/magistraleinformaticanetworking/ae/ae2012/chap9.pdf 19. Minimum Description Length (MDL), https://www.seas.upenn.edu/~cis520/lectures/MDL.pdf 20. 19. Protocols — Compression Algorithm Specification - UEFI Forum, https://uefi.org/specs/UEFI/2.10_A/19_Protocols_Compression_Algorithm_Specification.html 21. Data types | Adobe Journey Optimizer - Experience League, https://experienceleague.adobe.com/en/docs/journey-optimizer/using/orchestrate-journeys/building-advanced-conditions-journeys/syntax/data-types 22. Minimum Description Length (MDL) Regularization for Online Learning, http://proceedings.mlr.press/v44/shamir15.pdf 23. Spell: Streaming Parsing of System Event Logs - Virtual Server List, https://users.cs.utah.edu/~lifeifei/papers/spell.pdf 24. Spell: Online Streaming Parsing of Large ... - Virtual Server List, https://users.cs.utah.edu/~lifeifei/papers/spell-tkde19.pdf 25. LogCluster - A data clustering and pattern mining algorithm for event logs, https://www.semanticscholar.org/paper/LogCluster-A-data-clustering-and-pattern-mining-for-Vaarandi-Pihelgas/1b09d62aed3b85fc1736412f37b95422938a97a3 26. Weighted Interval Scheduling - Dynamic Programming I - Kira Goldner, https://www.kiragoldner.com/teaching/DS320/spring24/L13.pdf 27. Main Steps An Example: Weighted Interval Scheduling - Cornell: Computer Science, https://www.cs.cornell.edu/courses/cs482/2007su/dynamic.pdf 28. 6.1 Weighted Interval Scheduling: A Recursive Procedure - Fang Song, https://fangsong.info/teaching/f19_629_alg/weighted_interval_scheduling.pdf 29. Weighted Job Scheduling - GeeksforGeeks, https://www.geeksforgeeks.org/dsa/weighted-job-scheduling/ 30. Weighted Interval Scheduling | Victor Farazdagi, https://farazdagi.com/blog/2013/weighted-interval-scheduling/ 31. Weighted Interval Scheduling Problem | by Sayali Dalvi - Medium, https://medium.com/@dalvisayali97/weighted-interval-scheduling-problem-8117eecb521a 32. Fuzzy Matching In Financial Compliance: Techniques And Challenges, https://financialcrimeacademy.org/fuzzy-matching-in-financial-compliance/ 33. Common Mistakes In Fuzzy Data Matching - WinPure, https://winpure.com/fuzzy-matching-common-mistakes/ 34. Approximating Approximate Pattern Matching - DROPS, https://drops.dagstuhl.de/storage/00lipics/lipics-vol128-cpm2019/LIPIcs.CPM.2019.15/LIPIcs.CPM.2019.15.pdf 35. Algorithm to Learn a Regular Expression Pattern - Stack Overflow, https://stackoverflow.com/questions/12117551/algorithm-to-learn-a-regular-expression-pattern 36. Regular Expression Learning from Positive Examples Based on Integer Programming, https://www.researchgate.net/publication/345993614_Regular_Expression_Learning_from_Positive_Examples_Based_on_Integer_Programming 37. Automatic Generation of Regular Expressions from Examples - Old version - Regex Inginf, http://regex.inginf.units.it/extract/ 38. Outlier Analysis in Data Mining - Scaler Topics, https://www.scaler.com/topics/data-mining-tutorial/outlier-analysis-in-data-mining/ 39. Learn Outlier Analysis in Data Mining with Methods, Types & Examples, https://learninglabb.com/outlier-analysis-in-data-mining/ 40. (PDF) Noise filtering of process execution logs based on outliers detection - ResearchGate, https://www.researchgate.net/publication/312447834_Noise_filtering_of_process_execution_logs_based_on_outliers_detection 41. Unsupervised Outlier Detection on Databricks, https://www.databricks.com/blog/2023/03/13/unsupervised-outlier-detection-databricks.html
